{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/susanemiliaw/NTHU_2025_DLIA_HW/blob/main/(clear_output)Attempt4_MobileNetV2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Li0bVCTuxc6n"
      },
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "#### Lab 3\n",
        "\n",
        "# National Tsing Hua University\n",
        "\n",
        "#### Spring 2025\n",
        "\n",
        "#### 11320IEEM 513600\n",
        "\n",
        "#### Deep Learning and Industrial Applications\n",
        "    \n",
        "## Lab 3: Anomaly Detection in Industrial Applications\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlvflhYwCu8Q"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "In today's industrial landscape, the ability to detect anomalies in manufacturing processes and products is critical for maintaining quality, efficiency, and safety. This lab focuses on leveraging deep learning techniques for anomaly detection in various industrial applications, using the MVTEC Anomaly Detection Dataset. By employing ImageNet-pretrained models available in torchvision, students will gain hands-on experience in classfying defects and irregularities across different types of industrial products.\n",
        "\n",
        "Throughout this lab, you'll be involved in the following key activities:\n",
        "- Explore and process the MVTec Anomaly Detection Dataset.\n",
        "- Apply ImageNet-pretrained models from [Torchvision](https://pytorch.org/vision/stable/models.html) to detect anomalies in industrial products.\n",
        "- Evaluate the performance of the models to understand their effectiveness in real-world industrial applications.\n",
        "\n",
        "### Objectives\n",
        "\n",
        "- Understand the principles of anomaly detection in the context of industrial applications.\n",
        "- Learn how to implement and utilize ImageNet-pretrained models for detecting anomalies.\n",
        "- Analyze and interpret the results of the anomaly detection models to assess their practicality in industrial settings.\n",
        "\n",
        "### Dataset\n",
        "\n",
        "The MVTec AD Dataset is a comprehensive collection of high-resolution images across different categories of industrial products, such as bottles, cables, and metal nuts, each with various types of defects. This dataset is pivotal for developing and benchmarking anomaly detection algorithms. You can download our lab's dataset [here](https://drive.google.com/file/d/19600hUOpx0hl78TdpdH0oyy-gGTk_F_o/view?usp=share_link). You can drop downloaded data and drop to colab, or you can put into yor google drive.\n",
        "\n",
        "### References\n",
        "- [MVTec AD Dataset](https://www.kaggle.com/datasets/ipythonx/mvtec-ad/data) for the dataset used in this lab.\n",
        "- [Torchvision Models](https://pytorch.org/vision/stable/models.html) for accessing ImageNet-pretrained models to be used in anomaly detection tasks.\n",
        "- [State-of-the-Art Anomaly Detection on MVTec AD](https://paperswithcode.com/sota/anomaly-detection-on-mvtec-ad) for insights into the latest benchmarks and methodologies in anomaly detection applied to the MVTec AD dataset.\n",
        "- [CVPR 2019: MVTec AD ‚Äî A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection] for the original paper of MVTec AD dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuiEw1L0Cu8Q"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from tqdm.auto import tqdm\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvLTTCGsCu8R"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "\n",
        "# Set the category and paths\n",
        "category = \"bottle\"\n",
        "drive_root = f\"/content/drive/MyDrive/Colab Notebooks/{category}\"\n",
        "train_dir = os.path.join(drive_root, \"train\", \"good\")\n",
        "test_dir = os.path.join(drive_root, \"test\")\n",
        "\n",
        "# Get training images (only 'good')\n",
        "train_images = glob.glob(f\"{train_dir}/*.png\")\n",
        "\n",
        "# Get test images: both 'good' and defective\n",
        "test_images_good = glob.glob(f\"{test_dir}/good/*.png\")\n",
        "test_images_defect = []\n",
        "defect_classes = []\n",
        "\n",
        "for defect_class in os.listdir(test_dir):\n",
        "    class_path = os.path.join(test_dir, defect_class)\n",
        "    if os.path.isdir(class_path) and defect_class != \"good\":\n",
        "        defect_classes.append(defect_class)\n",
        "        test_images_defect += glob.glob(f\"{class_path}/*.png\")\n",
        "\n",
        "test_images = test_images_good + test_images_defect\n",
        "\n",
        "# Count defect classes (excluding 'good')\n",
        "num_defect_classes = len(defect_classes)\n",
        "\n",
        "# Example image to get shape\n",
        "sample_img = cv2.imread(train_images[0])\n",
        "height, width, channels = sample_img.shape\n",
        "\n",
        "# Output summary\n",
        "print(\"üìä Dataset Summary for 'bottle':\")\n",
        "print(f\"‚Ä¢ Number of defect classes: {num_defect_classes}\")\n",
        "print(f\"‚Ä¢ Types of defect classes: {defect_classes}\")\n",
        "print(f\"‚Ä¢ Total images used: {len(train_images) + len(test_images)}\")\n",
        "print(f\"    - Training images (only 'good'): {len(train_images)}\")\n",
        "print(f\"    - Test images (good + defective): {len(test_images)}\")\n",
        "print(f\"‚Ä¢ Image dimensions: {width} x {height} x {channels}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXfjTWKUCu8R"
      },
      "outputs": [],
      "source": [
        "file_paths = glob.glob(\"/content/drive/MyDrive/Colab Notebooks/bottle/**/*/*.png\", recursive=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GiOZBRJCu8S"
      },
      "outputs": [],
      "source": [
        "all_data = []\n",
        "\n",
        "for img in tqdm(file_paths):\n",
        "    img = cv2.imread(img)\n",
        "    img = img[..., ::-1]\n",
        "    all_data.append(img)\n",
        "\n",
        "all_data = np.stack(all_data)\n",
        "print(all_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ii8LH8s4Cu8S"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define your test directory\n",
        "test_dir = \"/content/drive/MyDrive/Colab Notebooks/bottle/test\"\n",
        "\n",
        "# Get all class names in the test directory\n",
        "classes = sorted([d for d in os.listdir(test_dir) if os.path.isdir(os.path.join(test_dir, d))])\n",
        "print(f'Classes found: {classes}')\n",
        "\n",
        "# Prepare plot\n",
        "fig, axs = plt.subplots(len(classes), 2, figsize=(8, 4 * len(classes)))\n",
        "\n",
        "for i, class_name in enumerate(classes):\n",
        "    class_folder = os.path.join(test_dir, class_name)\n",
        "    image_paths = glob.glob(f\"{class_folder}/*.png\")\n",
        "\n",
        "    # If there are fewer than 2 images, fill with duplicates\n",
        "    if len(image_paths) < 2:\n",
        "        images = image_paths * 2\n",
        "    else:\n",
        "        images = random.sample(image_paths, 2)\n",
        "\n",
        "    for j in range(2):\n",
        "        img = cv2.imread(images[j])\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        axs[i, j].imshow(img)\n",
        "        axs[i, j].axis('off')\n",
        "        axs[i, j].set_title(f'{class_name}')\n",
        "\n",
        "# Super title and layout\n",
        "plt.suptitle(\"üîç Sample Images from Each Defect Class\", fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-1PsC--M7pT"
      },
      "source": [
        "## A. Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGFI8GMpCu8S"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# Paths\n",
        "train_dir = \"/content/drive/MyDrive/Colab Notebooks/bottle/train/good\"\n",
        "test_dir = \"/content/drive/MyDrive/Colab Notebooks/bottle/test\"\n",
        "\n",
        "# --- Load Training Data (GOOD + SELECTED DEFECT) ---\n",
        "train_x = []\n",
        "train_y = []\n",
        "\n",
        "# Good images (label = 0)\n",
        "good_train = glob.glob(os.path.join(train_dir, \"*.png\"))\n",
        "for path in good_train:\n",
        "    img = cv2.imread(path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    train_x.append(img)\n",
        "    train_y.append(0)\n",
        "\n",
        "# Select limited defect images from test set (label = 1)\n",
        "defect_classes = ['broken_large', 'broken_small', 'contamination']\n",
        "for cls in defect_classes:\n",
        "    defect_imgs = sorted(glob.glob(f\"{test_dir}/{cls}/*.png\"))[:10]  # pick 10 from each\n",
        "    for path in defect_imgs:\n",
        "        img = cv2.imread(path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        train_x.append(img)\n",
        "        train_y.append(1)\n",
        "\n",
        "# Convert training data\n",
        "x_train = np.transpose(np.array(train_x), (0, 3, 1, 2))  # (N, C, H, W)\n",
        "y_train = np.array(train_y, dtype=np.int64)\n",
        "print(f\"‚úÖ x_train: {x_train.shape}, y_train: {y_train.shape}\")\n",
        "print(\"üìä Training label counts:\", np.bincount(y_train))\n",
        "\n",
        "# --- Load Test Data (full test set: good + all defects) ---\n",
        "x_test = []\n",
        "y_test = []\n",
        "\n",
        "for cls in os.listdir(test_dir):\n",
        "    label = 0 if cls == \"good\" else 1\n",
        "    for path in glob.glob(f\"{test_dir}/{cls}/*.png\"):\n",
        "        img = cv2.imread(path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        x_test.append(img)\n",
        "        y_test.append(label)\n",
        "\n",
        "x_test = np.transpose(np.array(x_test), (0, 3, 1, 2))\n",
        "y_test = np.array(y_test, dtype=np.int64)\n",
        "print(f\"‚úÖ x_test: {x_test.shape}, y_test: {y_test.shape}\")\n",
        "print(\"üìä Test label counts:\", np.bincount(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-CnfsmbCu8T"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# ‚úÖ Shared transform for MobileNetV2\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Required input size for MobileNetV2\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet mean\n",
        "                         std=[0.229, 0.224, 0.225])   # ImageNet std\n",
        "])\n",
        "\n",
        "# ‚úÖ Dataset with labels (for both training and testing)\n",
        "class LabeledImageDataset(Dataset):\n",
        "    def __init__(self, x, y, transform=None):\n",
        "        self.x = x\n",
        "        self.y = torch.from_numpy(y).long()\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = np.transpose(self.x[idx], (1, 2, 0))  # Convert to HWC\n",
        "        img = Image.fromarray(img.astype(np.uint8))\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, self.y[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53ZVFFacCu8T"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_dataset = LabeledImageDataset(x_train, y_train, transform=image_transform)\n",
        "val_dataset   = LabeledImageDataset(x_test, y_test, transform=image_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "-6vBSdcBzMCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaLGtT28xc6s"
      },
      "source": [
        "## B. Defining Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDX8iDKJCu8U"
      },
      "outputs": [],
      "source": [
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "# Load pretrained MobileNetV2 from torchvision\n",
        "model = models.mobilenet_v2(weights='IMAGENET1K_V1')\n",
        "\n",
        "# Optional: Freeze the feature extractor if you only want to fine-tune the classifier\n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False  # Comment this out if you want to fine-tune the whole model\n",
        "\n",
        "# Replace the classifier to match 2 output classes (good vs defect)\n",
        "num_features = model.classifier[1].in_features\n",
        "model.classifier[1] = nn.Linear(num_features, 2)\n",
        "\n",
        "# Move model to device (GPU or CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Print summary\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvLTU-IfZLqn"
      },
      "source": [
        "## C. Training the Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45ol4lpVxc6t"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Loss, optimizer, scheduler\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "lr_scheduler = CosineAnnealingLR(optimizer, T_max=len(train_loader) * 50, eta_min=0)\n",
        "\n",
        "# Training parameters\n",
        "epochs = 50\n",
        "best_val_acc = -1\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "train_losses, val_losses = [], []\n",
        "train_accuracies, val_accuracies = [], []\n",
        "\n",
        "for epoch in tqdm(range(epochs), desc=\"Training MobileNetV2\"):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    train_correct = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        train_correct += (preds == labels).sum().item()\n",
        "        total_train += labels.size(0)\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    train_acc = 100. * train_correct / total_train\n",
        "\n",
        "    # --- Validation ---\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            val_correct += (preds == labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_acc = 100. * val_correct / val_total\n",
        "\n",
        "    # Scheduler step\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), \"model_mobilenetv2.pth\")\n",
        "\n",
        "    # Logging\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.2f}% | Best Val Acc: {best_val_acc:.2f}%\")\n",
        "\n",
        "    train_losses.append(avg_train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_losses.append(avg_val_loss)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train label counts:\", np.bincount(y_train))\n",
        "print(\"Test label counts:\", np.bincount(y_test))  # ‚úÖ corrected!\n"
      ],
      "metadata": {
        "id": "30a1cnzTCwVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjmYxAJnxc6t"
      },
      "source": [
        "### Visualizing model performance"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# üìà Accuracy Plot\n",
        "ax[0].plot(train_accuracies, label='Train Accuracy', marker='o')\n",
        "ax[0].plot(val_accuracies, label='Val Accuracy', marker='x')\n",
        "ax[0].set_title('üìä MobileNetV2 - Accuracy Over Epochs', fontsize=14)\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[0].set_ylabel('Accuracy (%)')\n",
        "ax[0].legend()\n",
        "ax[0].grid(True)\n",
        "ax[0].set_ylim(0, 100)\n",
        "\n",
        "# üìâ Loss Plot\n",
        "ax[1].plot(train_losses, label='Train Loss', marker='o')\n",
        "ax[1].plot(val_losses, label='Val Loss', marker='x')\n",
        "ax[1].set_title('üìâ MobileNetV2 - Loss Over Epochs', fontsize=14)\n",
        "ax[1].set_xlabel('Epochs')\n",
        "ax[1].set_ylabel('Loss')\n",
        "ax[1].legend()\n",
        "ax[1].grid(True)\n",
        "\n",
        "# Final layout\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1hY2-RLEPolJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/model_classification_mobilenetv2.pth\")\n"
      ],
      "metadata": {
        "id": "0I0dAGXaFVr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVDWBwv6Cu8V"
      },
      "source": [
        "## D. Evaluating Your Trained Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEztHBDjCu8V"
      },
      "source": [
        "### Load Trained Model and Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DA1qHXpCu8V"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "# Define the path where the model was saved\n",
        "model_path = \"/content/drive/MyDrive/Colab Notebooks/model_classification_mobilenetv2.pth\"\n",
        "\n",
        "# Check device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Check if model file exists\n",
        "if not os.path.exists(model_path):\n",
        "    raise FileNotFoundError(f\"‚ùå Model file not found at: {model_path}\")\n",
        "\n",
        "# Load the trained model and move to the correct device\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Evaluate on test/validation set\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device).long()\n",
        "\n",
        "        outputs = model(images)\n",
        "        predicted = outputs.argmax(-1)\n",
        "\n",
        "        # Uncomment for debugging if needed\n",
        "        # print(\"üîç Predicted:\", predicted.cpu().numpy())\n",
        "        # print(\"üéØ Ground Truth:\", labels.cpu().numpy())\n",
        "\n",
        "        test_correct += (predicted == labels).sum().item()\n",
        "        test_total += labels.size(0)\n",
        "\n",
        "# Final test accuracy\n",
        "test_acc = 100. * test_correct / test_total\n",
        "print(f'‚úÖ Final Test Accuracy (MobileNetV2): {test_acc:.2f}%')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
