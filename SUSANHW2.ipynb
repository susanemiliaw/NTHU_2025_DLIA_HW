{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/susanemiliaw/NTHU_2025_DLIA_HW/blob/main/SUSANHW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "609dcb62-c2f8-4c6d-9c89-63dc0148a87c",
      "metadata": {
        "id": "609dcb62-c2f8-4c6d-9c89-63dc0148a87c"
      },
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "###### Lab 2\n",
        "\n",
        "# National Tsing Hua University\n",
        "\n",
        "#### Spring 2025\n",
        "\n",
        "#### 11320IEEM 513600\n",
        "\n",
        "#### Deep Learning and Industrial Applications\n",
        "    \n",
        "## Lab 2: Predicting Heart Disease with Deep Learning\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "061c22d2-eec4-40f4-866b-ccaa2d9a2963",
      "metadata": {
        "tags": [],
        "id": "061c22d2-eec4-40f4-866b-ccaa2d9a2963"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "In the realm of healthcare, early detection and accurate prediction of diseases play a crucial role in patient care and management. Heart disease remains one of the leading causes of mortality worldwide, making the development of effective diagnostic tools essential. This lab leverages deep learning to predict the presence of heart disease in patients using a subset of 14 key attributes from the Cleveland Heart Disease Database. The objective is to explore and apply deep learning techniques to distinguish between the presence and absence of heart disease based on clinical parameters.\n",
        "\n",
        "Throughout this lab, you'll engage with the following key activities:\n",
        "- Use [Pandas](https://pandas.pydata.org) to process the CSV files.\n",
        "- Use [PyTorch](https://pytorch.org) to build an Artificial Neural Network (ANN) to fit the dataset.\n",
        "- Evaluate the performance of the trained model to understand its accuracy.\n",
        "\n",
        "### Attribute Information\n",
        "\n",
        "1. age: Age of the patient in years\n",
        "2. sex: (Male/Female)\n",
        "3. cp: Chest pain type (4 types: low, medium, high, and severe)\n",
        "4. trestbps: Resting blood pressure\n",
        "5. chol: Serum cholesterol in mg/dl\n",
        "6. fbs: Fasting blood sugar > 120 mg/dl\n",
        "7. restecg: Resting electrocardiographic results (values 0,1,2)\n",
        "8. thalach: Maximum heart rate achieved\n",
        "9. exang: Exercise induced angina\n",
        "10. oldpeak: Oldpeak = ST depression induced by exercise relative to rest\n",
        "11. slope: The slope of the peak exercise ST segment\n",
        "12. ca: Number of major vessels (0-3) colored by fluoroscopy\n",
        "13. thal: 3 = normal; 6 = fixed defect; 7 = reversible defect\n",
        "14. target: target have disease or not (1=yes, 0=no)\n",
        "\n",
        "### References\n",
        "- [UCI Heart Disease Data](https://www.kaggle.com/datasets/redwankarimsony/heart-disease-data) for the dataset we use in this lab.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad594fc8-4989-40f3-b124-4550fe7df386",
      "metadata": {
        "id": "ad594fc8-4989-40f3-b124-4550fe7df386"
      },
      "source": [
        "## A. Checking and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfITSFq7skol",
        "outputId": "ae40d32c-c272-47d1-f257-7bc754960169"
      },
      "id": "pfITSFq7skol",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42a3eafd-cbcd-4c56-82cb-83a0bfa2399e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "42a3eafd-cbcd-4c56-82cb-83a0bfa2399e",
        "outputId": "3ce123c4-1163-4912-8c43-6d8f24299eee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     age     sex      cp  trestbps   chol  fbs  restecg  thalach  exang  \\\n",
              "0     41    Male  medium     105.0  198.0    0      1.0    168.0      0   \n",
              "1     65  Female     low     120.0  177.0    0      1.0    140.0      0   \n",
              "2     44  Female  medium     130.0  219.0    0      0.0    188.0      0   \n",
              "3     54  Female    high     125.0  273.0    0      0.0    152.0      0   \n",
              "4     51  Female  severe     125.0  213.0    0      0.0    125.0      1   \n",
              "..   ...     ...     ...       ...    ...  ...      ...      ...    ...   \n",
              "268   40  Female     low     110.0  167.0    0      0.0    114.0      1   \n",
              "269   60  Female     low     117.0  230.0    1      1.0    160.0      1   \n",
              "270   64  Female    high     140.0  335.0    0      1.0    158.0      0   \n",
              "271   43  Female     low     120.0  177.0    0      0.0    120.0      1   \n",
              "272   57  Female     low     150.0  276.0    0      0.0    112.0      1   \n",
              "\n",
              "     oldpeak  slope  ca  thal  target  \n",
              "0        0.0    2.0   1   2.0     1.0  \n",
              "1        0.4    2.0   0   3.0     1.0  \n",
              "2        0.0    2.0   0   2.0     1.0  \n",
              "3        0.5    0.0   1   2.0     1.0  \n",
              "4        1.4    2.0   1   2.0     1.0  \n",
              "..       ...    ...  ..   ...     ...  \n",
              "268      2.0    1.0   0   3.0     0.0  \n",
              "269      1.4    2.0   2   3.0     0.0  \n",
              "270      0.0    2.0   0   2.0     0.0  \n",
              "271      2.5    1.0   0   3.0     0.0  \n",
              "272      0.6    1.0   1   1.0     0.0  \n",
              "\n",
              "[273 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ab366556-7e12-4dcd-b430-df479e200a3b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>41</td>\n",
              "      <td>Male</td>\n",
              "      <td>medium</td>\n",
              "      <td>105.0</td>\n",
              "      <td>198.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>65</td>\n",
              "      <td>Female</td>\n",
              "      <td>low</td>\n",
              "      <td>120.0</td>\n",
              "      <td>177.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>44</td>\n",
              "      <td>Female</td>\n",
              "      <td>medium</td>\n",
              "      <td>130.0</td>\n",
              "      <td>219.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>188.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>54</td>\n",
              "      <td>Female</td>\n",
              "      <td>high</td>\n",
              "      <td>125.0</td>\n",
              "      <td>273.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>51</td>\n",
              "      <td>Female</td>\n",
              "      <td>severe</td>\n",
              "      <td>125.0</td>\n",
              "      <td>213.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>268</th>\n",
              "      <td>40</td>\n",
              "      <td>Female</td>\n",
              "      <td>low</td>\n",
              "      <td>110.0</td>\n",
              "      <td>167.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269</th>\n",
              "      <td>60</td>\n",
              "      <td>Female</td>\n",
              "      <td>low</td>\n",
              "      <td>117.0</td>\n",
              "      <td>230.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>270</th>\n",
              "      <td>64</td>\n",
              "      <td>Female</td>\n",
              "      <td>high</td>\n",
              "      <td>140.0</td>\n",
              "      <td>335.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>158.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>271</th>\n",
              "      <td>43</td>\n",
              "      <td>Female</td>\n",
              "      <td>low</td>\n",
              "      <td>120.0</td>\n",
              "      <td>177.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>272</th>\n",
              "      <td>57</td>\n",
              "      <td>Female</td>\n",
              "      <td>low</td>\n",
              "      <td>150.0</td>\n",
              "      <td>276.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>273 rows × 14 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ab366556-7e12-4dcd-b430-df479e200a3b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ab366556-7e12-4dcd-b430-df479e200a3b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ab366556-7e12-4dcd-b430-df479e200a3b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cfb3f4dd-c6d9-488d-9f14-e713e9ccb852\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cfb3f4dd-c6d9-488d-9f14-e713e9ccb852')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cfb3f4dd-c6d9-488d-9f14-e713e9ccb852 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_356844fd-5341-47c5-9938-a33642a18623\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_356844fd-5341-47c5-9938-a33642a18623 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 273,\n  \"fields\": [\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9,\n        \"min\": 29,\n        \"max\": 77,\n        \"num_unique_values\": 41,\n        \"samples\": [\n          60,\n          62,\n          53\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sex\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Female\",\n          \"Male\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cp\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"low\",\n          \"severe\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"trestbps\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17.852394760861472,\n        \"min\": 94.0,\n        \"max\": 200.0,\n        \"num_unique_values\": 49,\n        \"samples\": [\n          108.0,\n          114.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chol\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 51.44738664891147,\n        \"min\": 126.0,\n        \"max\": 564.0,\n        \"num_unique_values\": 145,\n        \"samples\": [\n          262.0,\n          266.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fbs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"restecg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5291071887540655,\n        \"min\": 0.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thalach\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 23.20476792843544,\n        \"min\": 71.0,\n        \"max\": 202.0,\n        \"num_unique_values\": 88,\n        \"samples\": [\n          106.0,\n          168.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"exang\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"oldpeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.184296490376038,\n        \"min\": 0.0,\n        \"max\": 6.2,\n        \"num_unique_values\": 40,\n        \"samples\": [\n          0.7,\n          4.2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"slope\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6175201588975051,\n        \"min\": 0.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ca\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thal\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6220982126950155,\n        \"min\": 0.0,\n        \"max\": 3.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          3.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4986279198706136,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/heart_dataset_train_all.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34241797-60f0-4818-a44b-f5379948d621",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34241797-60f0-4818-a44b-f5379948d621",
        "outputId": "b2b2f8fa-6e0f-4eab-f5e0-c253972797e5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n",
              "       'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "026585db-a6d8-4062-85de-e3a7eaebed72",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "026585db-a6d8-4062-85de-e3a7eaebed72",
        "outputId": "1b996426-3bb4-4548-b93d-7c1b1de024f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 273 entries, 0 to 272\n",
            "Data columns (total 14 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   age       273 non-null    int64  \n",
            " 1   sex       272 non-null    object \n",
            " 2   cp        272 non-null    object \n",
            " 3   trestbps  272 non-null    float64\n",
            " 4   chol      271 non-null    float64\n",
            " 5   fbs       273 non-null    int64  \n",
            " 6   restecg   272 non-null    float64\n",
            " 7   thalach   272 non-null    float64\n",
            " 8   exang     273 non-null    int64  \n",
            " 9   oldpeak   273 non-null    float64\n",
            " 10  slope     271 non-null    float64\n",
            " 11  ca        273 non-null    int64  \n",
            " 12  thal      272 non-null    float64\n",
            " 13  target    272 non-null    float64\n",
            "dtypes: float64(8), int64(4), object(2)\n",
            "memory usage: 30.0+ KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69031e6d-0fb5-49d9-b723-a0d1fee08c3c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "69031e6d-0fb5-49d9-b723-a0d1fee08c3c",
        "outputId": "88587644-0cf8-4902-9fca-3f8f53aa5d76"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "age         0\n",
              "sex         1\n",
              "cp          1\n",
              "trestbps    1\n",
              "chol        2\n",
              "fbs         0\n",
              "restecg     1\n",
              "thalach     1\n",
              "exang       0\n",
              "oldpeak     0\n",
              "slope       2\n",
              "ca          0\n",
              "thal        1\n",
              "target      1\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>age</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sex</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cp</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>trestbps</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chol</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fbs</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>restecg</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>thalach</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>exang</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oldpeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>slope</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ca</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>thal</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>target</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# checking for null values\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb3090f8-2cfa-4f56-8aa5-cf954bb19932",
      "metadata": {
        "id": "cb3090f8-2cfa-4f56-8aa5-cf954bb19932"
      },
      "outputs": [],
      "source": [
        "df = df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38aadbee-d68f-4ae0-b842-b40800b0cac9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38aadbee-d68f-4ae0-b842-b40800b0cac9",
        "outputId": "3055ad24-c824-47a3-887f-17d7505b447f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(270, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26a69fd5-3534-4d8e-b59a-6778bf47a479",
      "metadata": {
        "id": "26a69fd5-3534-4d8e-b59a-6778bf47a479",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "5a9ebba3-14c8-4589-8c3e-2e7224757c6a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     age sex cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
              "0     41   0  1     105.0  198.0    0      1.0    168.0      0      0.0   \n",
              "1     65   1  0     120.0  177.0    0      1.0    140.0      0      0.4   \n",
              "2     44   1  1     130.0  219.0    0      0.0    188.0      0      0.0   \n",
              "3     54   1  2     125.0  273.0    0      0.0    152.0      0      0.5   \n",
              "4     51   1  3     125.0  213.0    0      0.0    125.0      1      1.4   \n",
              "..   ...  .. ..       ...    ...  ...      ...      ...    ...      ...   \n",
              "268   40   1  0     110.0  167.0    0      0.0    114.0      1      2.0   \n",
              "269   60   1  0     117.0  230.0    1      1.0    160.0      1      1.4   \n",
              "270   64   1  2     140.0  335.0    0      1.0    158.0      0      0.0   \n",
              "271   43   1  0     120.0  177.0    0      0.0    120.0      1      2.5   \n",
              "272   57   1  0     150.0  276.0    0      0.0    112.0      1      0.6   \n",
              "\n",
              "     slope  ca  thal  target  \n",
              "0      2.0   1   2.0     1.0  \n",
              "1      2.0   0   3.0     1.0  \n",
              "2      2.0   0   2.0     1.0  \n",
              "3      0.0   1   2.0     1.0  \n",
              "4      2.0   1   2.0     1.0  \n",
              "..     ...  ..   ...     ...  \n",
              "268    1.0   0   3.0     0.0  \n",
              "269    2.0   2   3.0     0.0  \n",
              "270    2.0   0   2.0     0.0  \n",
              "271    1.0   0   3.0     0.0  \n",
              "272    1.0   1   1.0     0.0  \n",
              "\n",
              "[270 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-12de98f6-47a3-4c5f-9948-8f923cb8af91\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>105.0</td>\n",
              "      <td>198.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>65</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>177.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>44</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>130.0</td>\n",
              "      <td>219.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>188.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>54</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>125.0</td>\n",
              "      <td>273.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>51</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>125.0</td>\n",
              "      <td>213.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>268</th>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>167.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269</th>\n",
              "      <td>60</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>230.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>270</th>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>140.0</td>\n",
              "      <td>335.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>158.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>271</th>\n",
              "      <td>43</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>177.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>272</th>\n",
              "      <td>57</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>276.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>270 rows × 14 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-12de98f6-47a3-4c5f-9948-8f923cb8af91')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-12de98f6-47a3-4c5f-9948-8f923cb8af91 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-12de98f6-47a3-4c5f-9948-8f923cb8af91');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1d19fa14-f6bd-47a7-9a1f-0cc41bac7578\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1d19fa14-f6bd-47a7-9a1f-0cc41bac7578')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1d19fa14-f6bd-47a7-9a1f-0cc41bac7578 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_20716024-1fc4-41d7-95e0-78147cadfa48\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_20716024-1fc4-41d7-95e0-78147cadfa48 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 270,\n  \"fields\": [\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9,\n        \"min\": 29,\n        \"max\": 77,\n        \"num_unique_values\": 41,\n        \"samples\": [\n          60,\n          62,\n          39\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sex\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cp\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"trestbps\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17.90467515098084,\n        \"min\": 94.0,\n        \"max\": 200.0,\n        \"num_unique_values\": 49,\n        \"samples\": [\n          108.0,\n          114.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chol\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 51.5294114300184,\n        \"min\": 126.0,\n        \"max\": 564.0,\n        \"num_unique_values\": 145,\n        \"samples\": [\n          262.0,\n          266.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fbs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"restecg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5293141619418645,\n        \"min\": 0.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thalach\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 23.21725300546228,\n        \"min\": 71.0,\n        \"max\": 202.0,\n        \"num_unique_values\": 88,\n        \"samples\": [\n          106.0,\n          168.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"exang\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"oldpeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.188378543758624,\n        \"min\": 0.0,\n        \"max\": 6.2,\n        \"num_unique_values\": 40,\n        \"samples\": [\n          0.7,\n          4.2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"slope\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6181877820120636,\n        \"min\": 0.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ca\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thal\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6238744511959269,\n        \"min\": 0.0,\n        \"max\": 3.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          3.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.49894560448305486,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Mapping 'sex' descriptions to numbers\n",
        "sex_description = {\n",
        "    'Male': 0,\n",
        "    'Female': 1,\n",
        "}\n",
        "df.loc[:, 'sex'] = df['sex'].map(sex_description)\n",
        "\n",
        "# Mapping 'cp' (chest pain) descriptions to numbers\n",
        "pain_description = {\n",
        "    'low': 0,\n",
        "    'medium': 1,\n",
        "    'high': 2,\n",
        "    'severe': 3\n",
        "}\n",
        "df.loc[:, 'cp'] = df['cp'].map(pain_description)\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "051108c6-7011-4187-9e36-bd2944a019ca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "051108c6-7011-4187-9e36-bd2944a019ca",
        "outputId": "ebac0c1c-14d6-44c6-f06f-b832a21dbe96"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              age    trestbps        chol         fbs     restecg     thalach  \\\n",
              "count  270.000000  270.000000  270.000000  270.000000  270.000000  270.000000   \n",
              "mean    54.385185  131.525926  245.607407    0.151852    0.522222  149.807407   \n",
              "std      9.149713   17.904675   51.529411    0.359544    0.529314   23.217253   \n",
              "min     29.000000   94.000000  126.000000    0.000000    0.000000   71.000000   \n",
              "25%     47.250000  120.000000  210.250000    0.000000    0.000000  134.500000   \n",
              "50%     56.000000  130.000000  240.500000    0.000000    1.000000  152.500000   \n",
              "75%     61.000000  140.000000  274.000000    0.000000    1.000000  167.750000   \n",
              "max     77.000000  200.000000  564.000000    1.000000    2.000000  202.000000   \n",
              "\n",
              "            exang     oldpeak       slope          ca        thal      target  \n",
              "count  270.000000  270.000000  270.000000  270.000000  270.000000  270.000000  \n",
              "mean     0.333333    1.024074    1.400000    0.744444    2.300000    0.544444  \n",
              "std      0.472280    1.188379    0.618188    1.037166    0.623874    0.498946  \n",
              "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000  \n",
              "25%      0.000000    0.000000    1.000000    0.000000    2.000000    0.000000  \n",
              "50%      0.000000    0.600000    1.000000    0.000000    2.000000    1.000000  \n",
              "75%      1.000000    1.600000    2.000000    1.000000    3.000000    1.000000  \n",
              "max      1.000000    6.200000    2.000000    4.000000    3.000000    1.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5afb0a85-a71b-40f2-9ce3-d2b22a7a65d6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>54.385185</td>\n",
              "      <td>131.525926</td>\n",
              "      <td>245.607407</td>\n",
              "      <td>0.151852</td>\n",
              "      <td>0.522222</td>\n",
              "      <td>149.807407</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>1.024074</td>\n",
              "      <td>1.400000</td>\n",
              "      <td>0.744444</td>\n",
              "      <td>2.300000</td>\n",
              "      <td>0.544444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>9.149713</td>\n",
              "      <td>17.904675</td>\n",
              "      <td>51.529411</td>\n",
              "      <td>0.359544</td>\n",
              "      <td>0.529314</td>\n",
              "      <td>23.217253</td>\n",
              "      <td>0.472280</td>\n",
              "      <td>1.188379</td>\n",
              "      <td>0.618188</td>\n",
              "      <td>1.037166</td>\n",
              "      <td>0.623874</td>\n",
              "      <td>0.498946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>29.000000</td>\n",
              "      <td>94.000000</td>\n",
              "      <td>126.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>47.250000</td>\n",
              "      <td>120.000000</td>\n",
              "      <td>210.250000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>134.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>56.000000</td>\n",
              "      <td>130.000000</td>\n",
              "      <td>240.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>152.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>61.000000</td>\n",
              "      <td>140.000000</td>\n",
              "      <td>274.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>167.750000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.600000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>77.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>564.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>202.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.200000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5afb0a85-a71b-40f2-9ce3-d2b22a7a65d6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5afb0a85-a71b-40f2-9ce3-d2b22a7a65d6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5afb0a85-a71b-40f2-9ce3-d2b22a7a65d6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9199f95e-15af-48fa-9fef-3c7f76e427aa\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9199f95e-15af-48fa-9fef-3c7f76e427aa')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9199f95e-15af-48fa-9fef-3c7f76e427aa button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 81.26807726653665,\n        \"min\": 9.149713209948986,\n        \"max\": 270.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          54.385185185185186,\n          56.0,\n          270.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"trestbps\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 73.73772068120547,\n        \"min\": 17.90467515098084,\n        \"max\": 270.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          131.52592592592592,\n          130.0,\n          270.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chol\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 149.27786697513412,\n        \"min\": 51.5294114300184,\n        \"max\": 564.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          245.6074074074074,\n          240.5,\n          270.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fbs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 95.38369701469601,\n        \"min\": 0.0,\n        \"max\": 270.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.15185185185185185,\n          1.0,\n          0.35954367027245654\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"restecg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 95.2064556868165,\n        \"min\": 0.0,\n        \"max\": 270.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          270.0,\n          0.5222222222222223,\n          2.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thalach\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 75.47345746130112,\n        \"min\": 23.21725300546228,\n        \"max\": 270.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          149.8074074074074,\n          152.5,\n          270.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"exang\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 95.31861707775643,\n        \"min\": 0.0,\n        \"max\": 270.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.3333333333333333,\n          1.0,\n          0.47227992455486234\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"oldpeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 94.94427117355892,\n        \"min\": 0.0,\n        \"max\": 270.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          270.0,\n          1.0240740740740741,\n          1.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"slope\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 95.05680862661214,\n        \"min\": 0.0,\n        \"max\": 270.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          270.0,\n          1.4,\n          2.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ca\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 95.1259484289674,\n        \"min\": 0.0,\n        \"max\": 270.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          270.0,\n          0.7444444444444445,\n          4.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thal\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 94.81255144269917,\n        \"min\": 0.0,\n        \"max\": 270.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          270.0,\n          2.3,\n          3.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 95.25610060161931,\n        \"min\": 0.0,\n        \"max\": 270.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.5444444444444444,\n          1.0,\n          0.49894560448305486\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b999df5-09a1-4ce2-b068-f1afba448ff8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "8b999df5-09a1-4ce2-b068-f1afba448ff8",
        "outputId": "e1feebde-bd4e-4403-ac11-56c6cc60b15b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               age       sex        cp  trestbps      chol       fbs  \\\n",
              "age       1.000000 -0.062222 -0.103697  0.261782  0.210520  0.109847   \n",
              "sex      -0.062222  1.000000 -0.040197 -0.055463 -0.166885  0.042384   \n",
              "cp       -0.103697 -0.040197  1.000000  0.035563 -0.063592  0.065869   \n",
              "trestbps  0.261782 -0.055463  0.035563  1.000000  0.128444  0.170606   \n",
              "chol      0.210520 -0.166885 -0.063592  0.128444  1.000000  0.003430   \n",
              "fbs       0.109847  0.042384  0.065869  0.170606  0.003430  1.000000   \n",
              "restecg  -0.124588 -0.069599  0.008389 -0.145195 -0.162687 -0.086165   \n",
              "thalach  -0.412624 -0.058626  0.300307 -0.056631 -0.023753 -0.014297   \n",
              "exang     0.111263  0.124054 -0.428233  0.067116  0.063902  0.029190   \n",
              "oldpeak   0.200243  0.089726 -0.183616  0.184896  0.084355  0.007943   \n",
              "slope    -0.165360 -0.038771  0.135174 -0.126553 -0.031929 -0.056866   \n",
              "ca        0.254462  0.140795 -0.180598  0.093545  0.068647  0.164266   \n",
              "thal      0.077368  0.198493 -0.139765  0.068690  0.121280 -0.004972   \n",
              "target   -0.244798 -0.283776  0.425574 -0.173239 -0.096773 -0.068845   \n",
              "\n",
              "           restecg   thalach     exang   oldpeak     slope        ca  \\\n",
              "age      -0.124588 -0.412624  0.111263  0.200243 -0.165360  0.254462   \n",
              "sex      -0.069599 -0.058626  0.124054  0.089726 -0.038771  0.140795   \n",
              "cp        0.008389  0.300307 -0.428233 -0.183616  0.135174 -0.180598   \n",
              "trestbps -0.145195 -0.056631  0.067116  0.184896 -0.126553  0.093545   \n",
              "chol     -0.162687 -0.023753  0.063902  0.084355 -0.031929  0.068647   \n",
              "fbs      -0.086165 -0.014297  0.029190  0.007943 -0.056866  0.164266   \n",
              "restecg   1.000000  0.025457 -0.089225 -0.047837  0.074982 -0.053946   \n",
              "thalach   0.025457  1.000000 -0.404349 -0.340564  0.370073 -0.205060   \n",
              "exang    -0.089225 -0.404349  1.000000  0.294308 -0.280124  0.106250   \n",
              "oldpeak  -0.047837 -0.340564  0.294308  1.000000 -0.585472  0.223375   \n",
              "slope     0.074982  0.370073 -0.280124 -0.585472  1.000000 -0.083491   \n",
              "ca       -0.053946 -0.205060  0.106250  0.223375 -0.083491  1.000000   \n",
              "thal     -0.003377 -0.078637  0.189253  0.200315 -0.090606  0.136160   \n",
              "target    0.101817  0.432687 -0.457502 -0.443504  0.363983 -0.391031   \n",
              "\n",
              "              thal    target  \n",
              "age       0.077368 -0.244798  \n",
              "sex       0.198493 -0.283776  \n",
              "cp       -0.139765  0.425574  \n",
              "trestbps  0.068690 -0.173239  \n",
              "chol      0.121280 -0.096773  \n",
              "fbs      -0.004972 -0.068845  \n",
              "restecg  -0.003377  0.101817  \n",
              "thalach  -0.078637  0.432687  \n",
              "exang     0.189253 -0.457502  \n",
              "oldpeak   0.200315 -0.443504  \n",
              "slope    -0.090606  0.363983  \n",
              "ca        0.136160 -0.391031  \n",
              "thal      1.000000 -0.311701  \n",
              "target   -0.311701  1.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-89f8b8c9-68fb-4b2f-97e0-1eb7fc356a6f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>age</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.062222</td>\n",
              "      <td>-0.103697</td>\n",
              "      <td>0.261782</td>\n",
              "      <td>0.210520</td>\n",
              "      <td>0.109847</td>\n",
              "      <td>-0.124588</td>\n",
              "      <td>-0.412624</td>\n",
              "      <td>0.111263</td>\n",
              "      <td>0.200243</td>\n",
              "      <td>-0.165360</td>\n",
              "      <td>0.254462</td>\n",
              "      <td>0.077368</td>\n",
              "      <td>-0.244798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sex</th>\n",
              "      <td>-0.062222</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.040197</td>\n",
              "      <td>-0.055463</td>\n",
              "      <td>-0.166885</td>\n",
              "      <td>0.042384</td>\n",
              "      <td>-0.069599</td>\n",
              "      <td>-0.058626</td>\n",
              "      <td>0.124054</td>\n",
              "      <td>0.089726</td>\n",
              "      <td>-0.038771</td>\n",
              "      <td>0.140795</td>\n",
              "      <td>0.198493</td>\n",
              "      <td>-0.283776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cp</th>\n",
              "      <td>-0.103697</td>\n",
              "      <td>-0.040197</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.035563</td>\n",
              "      <td>-0.063592</td>\n",
              "      <td>0.065869</td>\n",
              "      <td>0.008389</td>\n",
              "      <td>0.300307</td>\n",
              "      <td>-0.428233</td>\n",
              "      <td>-0.183616</td>\n",
              "      <td>0.135174</td>\n",
              "      <td>-0.180598</td>\n",
              "      <td>-0.139765</td>\n",
              "      <td>0.425574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>trestbps</th>\n",
              "      <td>0.261782</td>\n",
              "      <td>-0.055463</td>\n",
              "      <td>0.035563</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.128444</td>\n",
              "      <td>0.170606</td>\n",
              "      <td>-0.145195</td>\n",
              "      <td>-0.056631</td>\n",
              "      <td>0.067116</td>\n",
              "      <td>0.184896</td>\n",
              "      <td>-0.126553</td>\n",
              "      <td>0.093545</td>\n",
              "      <td>0.068690</td>\n",
              "      <td>-0.173239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chol</th>\n",
              "      <td>0.210520</td>\n",
              "      <td>-0.166885</td>\n",
              "      <td>-0.063592</td>\n",
              "      <td>0.128444</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.003430</td>\n",
              "      <td>-0.162687</td>\n",
              "      <td>-0.023753</td>\n",
              "      <td>0.063902</td>\n",
              "      <td>0.084355</td>\n",
              "      <td>-0.031929</td>\n",
              "      <td>0.068647</td>\n",
              "      <td>0.121280</td>\n",
              "      <td>-0.096773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fbs</th>\n",
              "      <td>0.109847</td>\n",
              "      <td>0.042384</td>\n",
              "      <td>0.065869</td>\n",
              "      <td>0.170606</td>\n",
              "      <td>0.003430</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.086165</td>\n",
              "      <td>-0.014297</td>\n",
              "      <td>0.029190</td>\n",
              "      <td>0.007943</td>\n",
              "      <td>-0.056866</td>\n",
              "      <td>0.164266</td>\n",
              "      <td>-0.004972</td>\n",
              "      <td>-0.068845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>restecg</th>\n",
              "      <td>-0.124588</td>\n",
              "      <td>-0.069599</td>\n",
              "      <td>0.008389</td>\n",
              "      <td>-0.145195</td>\n",
              "      <td>-0.162687</td>\n",
              "      <td>-0.086165</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.025457</td>\n",
              "      <td>-0.089225</td>\n",
              "      <td>-0.047837</td>\n",
              "      <td>0.074982</td>\n",
              "      <td>-0.053946</td>\n",
              "      <td>-0.003377</td>\n",
              "      <td>0.101817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>thalach</th>\n",
              "      <td>-0.412624</td>\n",
              "      <td>-0.058626</td>\n",
              "      <td>0.300307</td>\n",
              "      <td>-0.056631</td>\n",
              "      <td>-0.023753</td>\n",
              "      <td>-0.014297</td>\n",
              "      <td>0.025457</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.404349</td>\n",
              "      <td>-0.340564</td>\n",
              "      <td>0.370073</td>\n",
              "      <td>-0.205060</td>\n",
              "      <td>-0.078637</td>\n",
              "      <td>0.432687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>exang</th>\n",
              "      <td>0.111263</td>\n",
              "      <td>0.124054</td>\n",
              "      <td>-0.428233</td>\n",
              "      <td>0.067116</td>\n",
              "      <td>0.063902</td>\n",
              "      <td>0.029190</td>\n",
              "      <td>-0.089225</td>\n",
              "      <td>-0.404349</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.294308</td>\n",
              "      <td>-0.280124</td>\n",
              "      <td>0.106250</td>\n",
              "      <td>0.189253</td>\n",
              "      <td>-0.457502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oldpeak</th>\n",
              "      <td>0.200243</td>\n",
              "      <td>0.089726</td>\n",
              "      <td>-0.183616</td>\n",
              "      <td>0.184896</td>\n",
              "      <td>0.084355</td>\n",
              "      <td>0.007943</td>\n",
              "      <td>-0.047837</td>\n",
              "      <td>-0.340564</td>\n",
              "      <td>0.294308</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.585472</td>\n",
              "      <td>0.223375</td>\n",
              "      <td>0.200315</td>\n",
              "      <td>-0.443504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>slope</th>\n",
              "      <td>-0.165360</td>\n",
              "      <td>-0.038771</td>\n",
              "      <td>0.135174</td>\n",
              "      <td>-0.126553</td>\n",
              "      <td>-0.031929</td>\n",
              "      <td>-0.056866</td>\n",
              "      <td>0.074982</td>\n",
              "      <td>0.370073</td>\n",
              "      <td>-0.280124</td>\n",
              "      <td>-0.585472</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.083491</td>\n",
              "      <td>-0.090606</td>\n",
              "      <td>0.363983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ca</th>\n",
              "      <td>0.254462</td>\n",
              "      <td>0.140795</td>\n",
              "      <td>-0.180598</td>\n",
              "      <td>0.093545</td>\n",
              "      <td>0.068647</td>\n",
              "      <td>0.164266</td>\n",
              "      <td>-0.053946</td>\n",
              "      <td>-0.205060</td>\n",
              "      <td>0.106250</td>\n",
              "      <td>0.223375</td>\n",
              "      <td>-0.083491</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.136160</td>\n",
              "      <td>-0.391031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>thal</th>\n",
              "      <td>0.077368</td>\n",
              "      <td>0.198493</td>\n",
              "      <td>-0.139765</td>\n",
              "      <td>0.068690</td>\n",
              "      <td>0.121280</td>\n",
              "      <td>-0.004972</td>\n",
              "      <td>-0.003377</td>\n",
              "      <td>-0.078637</td>\n",
              "      <td>0.189253</td>\n",
              "      <td>0.200315</td>\n",
              "      <td>-0.090606</td>\n",
              "      <td>0.136160</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.311701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>target</th>\n",
              "      <td>-0.244798</td>\n",
              "      <td>-0.283776</td>\n",
              "      <td>0.425574</td>\n",
              "      <td>-0.173239</td>\n",
              "      <td>-0.096773</td>\n",
              "      <td>-0.068845</td>\n",
              "      <td>0.101817</td>\n",
              "      <td>0.432687</td>\n",
              "      <td>-0.457502</td>\n",
              "      <td>-0.443504</td>\n",
              "      <td>0.363983</td>\n",
              "      <td>-0.391031</td>\n",
              "      <td>-0.311701</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-89f8b8c9-68fb-4b2f-97e0-1eb7fc356a6f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-89f8b8c9-68fb-4b2f-97e0-1eb7fc356a6f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-89f8b8c9-68fb-4b2f-97e0-1eb7fc356a6f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3fad3fa0-792e-46d8-b745-98d4df9fc39e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3fad3fa0-792e-46d8-b745-98d4df9fc39e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3fad3fa0-792e-46d8-b745-98d4df9fc39e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 14,\n  \"fields\": [\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3336796953563416,\n        \"min\": -0.4126237683266394,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.2002432632272073,\n          0.25446220532709146,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sex\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2991228194776611,\n        \"min\": -0.28377582305309207,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.08972560084685732,\n          0.14079450235695648,\n          -0.062222038735579396\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.343237644388663,\n        \"min\": -0.42823328385023884,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          -0.18361571896503148,\n          -0.18059763067401924,\n          -0.10369651400029238\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"trestbps\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.28956654914616836,\n        \"min\": -0.1732391623681546,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.18489606509122675,\n          0.09354457752219755,\n          0.2617824169771793\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chol\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2867254997156597,\n        \"min\": -0.16688487098702884,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.08435532313293413,\n          0.06864714277304984,\n          0.21052008975387562\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fbs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.27150092525557695,\n        \"min\": -0.08616493327966852,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.00794318047234304,\n          0.16426559207767857,\n          0.10984693693665605\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"restecg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.28984501434053883,\n        \"min\": -0.16268743250369436,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          -0.04783727701622021,\n          -0.05394643396214631,\n          -0.12458764457926447\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thalach\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.38230319141316005,\n        \"min\": -0.4126237683266394,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          -0.34056397282727374,\n          -0.20506017090677786,\n          -0.4126237683266394\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"exang\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3727203587084935,\n        \"min\": -0.4575020365957928,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.2943081762485106,\n          0.10624980942135133,\n          0.11126311741056943\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"oldpeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.38472554244127527,\n        \"min\": -0.5854716356239958,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          1.0,\n          0.22337523920060878,\n          0.2002432632272073\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"slope\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3681628540625692,\n        \"min\": -0.5854716356239958,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          -0.5854716356239958,\n          -0.08349138857404798,\n          -0.16535999776800558\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ca\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3194198076353616,\n        \"min\": -0.3910311347568131,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.22337523920060878,\n          1.0,\n          0.25446220532709146\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thal\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.29843375004744976,\n        \"min\": -0.3117007343731907,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.2003145523377015,\n          0.13616037988626117,\n          0.07736766291885036\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4251943991536586,\n        \"min\": -0.4575020365957928,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          -0.4435044185350298,\n          -0.3910311347568131,\n          -0.2447981425022257\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "df.corr()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ce7a0c5-76d6-4863-ba61-0544a220962a",
      "metadata": {
        "id": "8ce7a0c5-76d6-4863-ba61-0544a220962a"
      },
      "source": [
        "#### Converting the DataFrame to a NumPy Array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5735baad-2db8-4306-aa4c-7788d2b49621",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5735baad-2db8-4306-aa4c-7788d2b49621",
        "outputId": "93c13187-a6e9-4212-b99d-943126a0e903"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(270, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "np_data = df.values\n",
        "np_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29b8e189-7f39-435a-8038-39098b147325",
      "metadata": {
        "id": "29b8e189-7f39-435a-8038-39098b147325"
      },
      "outputs": [],
      "source": [
        "split_point = int(np_data.shape[0]*0.7)\n",
        "\n",
        "np.random.shuffle(np_data)\n",
        "\n",
        "x_train = np_data[:split_point, :13]\n",
        "y_train = np_data[:split_point, 13]\n",
        "x_val = np_data[split_point:, :13]\n",
        "y_val = np_data[split_point:, 13]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fedb56d7-1665-4c90-9697-b86cab43f300",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fedb56d7-1665-4c90-9697-b86cab43f300",
        "outputId": "039ecab5-bd3a-4112-a1d5-0726d716c2b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples in train and validation are 189 and 40.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Convert to tensors\n",
        "x_train = torch.from_numpy(np.array(x_train, dtype=np.float32)).float()\n",
        "y_train = torch.from_numpy(np.array(y_train, dtype=np.int64)).long()\n",
        "x_val = torch.from_numpy(np.array(x_val, dtype=np.float32)).float()\n",
        "y_val = torch.from_numpy(np.array(y_val, dtype=np.int64)).long()\n",
        "\n",
        "# Split val into val/test\n",
        "val_split = int(x_val.shape[0] * 0.5)\n",
        "x_test = x_val[val_split:]\n",
        "y_test = y_val[val_split:]\n",
        "x_val = x_val[:val_split]\n",
        "y_val = y_val[:val_split]\n",
        "\n",
        "# Create dataloaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(TensorDataset(x_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(TensorDataset(x_val, y_val), batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(TensorDataset(x_test, y_test), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f'Number of samples in train and validation are {len(train_loader.dataset)} and {len(val_loader.dataset)}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ffc26b9-6044-41e9-93e2-7dc6250dbd27",
      "metadata": {
        "id": "8ffc26b9-6044-41e9-93e2-7dc6250dbd27"
      },
      "source": [
        "## B. Defining Neural Networks\n",
        "\n",
        "In PyTorch, we can use **class** to define our custom neural network architectures by subclassing the `nn.Module` class. This gives our neural network all the functionality it needs to work with PyTorch's other utilities and keeps our implementation organized.\n",
        "\n",
        "- Neural networks are defined by subclassing `nn.Module`.\n",
        "- The layers of the neural network are initialized in the `__init__` method.\n",
        "- The forward pass operations on input data are defined in the `forward` method.\n",
        "\n",
        "It's worth noting that while we only define the forward pass, PyTorch will automatically derive the backward pass for us, which is used during training to update the model's weights.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77975746-a7a7-4676-9527-57674cd98c0f",
      "metadata": {
        "id": "77975746-a7a7-4676-9527-57674cd98c0f"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, hidden_units):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(13, hidden_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_units, hidden_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_units, 2)\n",
        "        ).cuda()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbb8b5b0-0ec0-406c-a42e-048aa00e05aa",
      "metadata": {
        "id": "cbb8b5b0-0ec0-406c-a42e-048aa00e05aa"
      },
      "source": [
        "## C. Training the Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3602ae7d-4034-4c49-b221-0c12a5824b18",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3602ae7d-4034-4c49-b221-0c12a5824b18",
        "outputId": "45b0a496-b714-4840-d825-c526d18dd3f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Mar 21 08:36:50 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check your GPU status.\n",
        "!nvidia-smi"
      ]
    },
    {
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Hyperparameters to try\n",
        "learning_rates = [0.01, 0.001, 0.0001]\n",
        "hidden_units_list = [64, 128, 256]\n",
        "results = []\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for hidden_units in hidden_units_list:\n",
        "        print(f\"\\nTraining with LR={lr}, Hidden Units={hidden_units}\")\n",
        "\n",
        "        model = Model(hidden_units).cuda()\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "        lr_scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=0)\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        best_val_acc = -1\n",
        "\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        train_accuracies = []\n",
        "        val_accuracies = []\n",
        "\n",
        "        for epoch in range(epochs): # Fixed: Corrected the indentation of this line\n",
        "\n",
        "            # Training\n",
        "            model.train() # Fixed: Set model to training mode\n",
        "            train_loss, train_correct, train_total = 0.0, 0, 0\n",
        "            for features, labels in train_loader: # Fixed: Iterating over train_loader for training\n",
        "                features, labels = features.cuda(), labels.cuda()\n",
        "                optimizer.zero_grad() # Fixed: Reset gradients\n",
        "                outputs = model(features)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()  # Fixed: Calculate gradients\n",
        "                optimizer.step() # Fixed: Update weights\n",
        "                train_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                train_correct += predicted.eq(labels).sum().item()\n",
        "                train_total += labels.size(0)\n",
        "\n",
        "            train_acc = 100. * train_correct / train_total\n",
        "            avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "            # Validation\n",
        "            model.eval()\n",
        "            val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "            with torch.no_grad():\n",
        "                for features, labels in val_loader:\n",
        "                    features, labels = features.cuda(), labels.cuda()\n",
        "                    outputs = model(features)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    val_loss += loss.item()\n",
        "                    _, predicted = outputs.max(1)\n",
        "                    val_correct += predicted.eq(labels).sum().item()\n",
        "                    val_total += labels.size(0)\n",
        "\n",
        "            val_acc = 100. * val_correct / val_total\n",
        "            avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "            # Checkpoint\n",
        "            if avg_val_loss < best_val_loss:\n",
        "                best_val_loss = avg_val_loss\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                torch.save(model.state_dict(), f'model_LR{lr}_HU{hidden_units}.pth')\n",
        "\n",
        "            # Learning rate update\n",
        "            lr_scheduler.step()\n",
        "\n",
        "            print(f'Epoch {epoch+1}/{epochs}, '\n",
        "                  f'Train loss: {avg_train_loss:.4f}, Train acc: {train_acc:.4f}%, '\n",
        "                  f'Val loss: {avg_val_loss:.4f}, Val acc: {val_acc:.4f}%, '\n",
        "                  f'Best Val loss: {best_val_loss:.4f} Best Val acc: {best_val_acc:.2f}%')\n",
        "\n",
        "            # Store performance\n",
        "            train_losses.append(avg_train_loss)\n",
        "            train_accuracies.append(train_acc)\n",
        "            val_losses.append(avg_val_loss)\n",
        "            val_accuracies.append(val_acc)\n",
        "\n",
        "        # Save result summary after all epochs\n",
        "        results.append({\n",
        "            'Learning Rate': lr,\n",
        "            'Hidden Units': hidden_units,\n",
        "            'Best Val Loss': round(best_val_loss, 4),\n",
        "            'Best Val Acc': round(best_val_acc, 2)\n",
        "        })"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "yLpkmxNospLH",
        "outputId": "c9fdff64-7d5c-4ce6-f57a-c46e0af03707",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "yLpkmxNospLH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with LR=0.01, Hidden Units=64\n",
            "Epoch 1/100, Train loss: 2.2899, Train acc: 49.7354%, Val loss: 1.6908, Val acc: 27.5000%, Best Val loss: 1.6908 Best Val acc: 27.50%\n",
            "Epoch 2/100, Train loss: 1.2112, Train acc: 46.5608%, Val loss: 0.7554, Val acc: 60.0000%, Best Val loss: 0.7554 Best Val acc: 60.00%\n",
            "Epoch 3/100, Train loss: 1.0454, Train acc: 45.5026%, Val loss: 0.8506, Val acc: 57.5000%, Best Val loss: 0.7554 Best Val acc: 60.00%\n",
            "Epoch 4/100, Train loss: 0.8926, Train acc: 52.3810%, Val loss: 0.7670, Val acc: 62.5000%, Best Val loss: 0.7554 Best Val acc: 62.50%\n",
            "Epoch 5/100, Train loss: 0.7309, Train acc: 59.2593%, Val loss: 0.6432, Val acc: 62.5000%, Best Val loss: 0.6432 Best Val acc: 62.50%\n",
            "Epoch 6/100, Train loss: 0.6537, Train acc: 62.4339%, Val loss: 0.7639, Val acc: 57.5000%, Best Val loss: 0.6432 Best Val acc: 62.50%\n",
            "Epoch 7/100, Train loss: 0.6745, Train acc: 60.3175%, Val loss: 0.6075, Val acc: 75.0000%, Best Val loss: 0.6075 Best Val acc: 75.00%\n",
            "Epoch 8/100, Train loss: 0.6549, Train acc: 61.9048%, Val loss: 0.7156, Val acc: 62.5000%, Best Val loss: 0.6075 Best Val acc: 75.00%\n",
            "Epoch 9/100, Train loss: 0.6294, Train acc: 65.0794%, Val loss: 0.6520, Val acc: 67.5000%, Best Val loss: 0.6075 Best Val acc: 75.00%\n",
            "Epoch 10/100, Train loss: 0.6119, Train acc: 68.2540%, Val loss: 0.5940, Val acc: 77.5000%, Best Val loss: 0.5940 Best Val acc: 77.50%\n",
            "Epoch 11/100, Train loss: 0.5909, Train acc: 65.6085%, Val loss: 0.6938, Val acc: 65.0000%, Best Val loss: 0.5940 Best Val acc: 77.50%\n",
            "Epoch 12/100, Train loss: 0.5803, Train acc: 67.7249%, Val loss: 0.5992, Val acc: 72.5000%, Best Val loss: 0.5940 Best Val acc: 77.50%\n",
            "Epoch 13/100, Train loss: 0.5769, Train acc: 69.8413%, Val loss: 0.6570, Val acc: 65.0000%, Best Val loss: 0.5940 Best Val acc: 77.50%\n",
            "Epoch 14/100, Train loss: 0.5935, Train acc: 67.7249%, Val loss: 0.5876, Val acc: 75.0000%, Best Val loss: 0.5876 Best Val acc: 77.50%\n",
            "Epoch 15/100, Train loss: 0.6007, Train acc: 68.7831%, Val loss: 0.5953, Val acc: 77.5000%, Best Val loss: 0.5876 Best Val acc: 77.50%\n",
            "Epoch 16/100, Train loss: 0.5829, Train acc: 68.7831%, Val loss: 0.8546, Val acc: 52.5000%, Best Val loss: 0.5876 Best Val acc: 77.50%\n",
            "Epoch 17/100, Train loss: 0.6887, Train acc: 61.9048%, Val loss: 0.6050, Val acc: 72.5000%, Best Val loss: 0.5876 Best Val acc: 77.50%\n",
            "Epoch 18/100, Train loss: 0.6150, Train acc: 65.6085%, Val loss: 0.6013, Val acc: 75.0000%, Best Val loss: 0.5876 Best Val acc: 77.50%\n",
            "Epoch 19/100, Train loss: 0.5987, Train acc: 69.3122%, Val loss: 0.7815, Val acc: 65.0000%, Best Val loss: 0.5876 Best Val acc: 77.50%\n",
            "Epoch 20/100, Train loss: 0.5536, Train acc: 69.8413%, Val loss: 0.6136, Val acc: 77.5000%, Best Val loss: 0.5876 Best Val acc: 77.50%\n",
            "Epoch 21/100, Train loss: 0.5636, Train acc: 71.4286%, Val loss: 0.7417, Val acc: 67.5000%, Best Val loss: 0.5876 Best Val acc: 77.50%\n",
            "Epoch 22/100, Train loss: 0.5435, Train acc: 71.9577%, Val loss: 0.5841, Val acc: 72.5000%, Best Val loss: 0.5841 Best Val acc: 77.50%\n",
            "Epoch 23/100, Train loss: 0.5643, Train acc: 69.3122%, Val loss: 0.6297, Val acc: 72.5000%, Best Val loss: 0.5841 Best Val acc: 77.50%\n",
            "Epoch 24/100, Train loss: 0.5432, Train acc: 73.5450%, Val loss: 0.6075, Val acc: 77.5000%, Best Val loss: 0.5841 Best Val acc: 77.50%\n",
            "Epoch 25/100, Train loss: 0.5472, Train acc: 69.3122%, Val loss: 0.6847, Val acc: 67.5000%, Best Val loss: 0.5841 Best Val acc: 77.50%\n",
            "Epoch 26/100, Train loss: 0.5360, Train acc: 71.4286%, Val loss: 0.6223, Val acc: 77.5000%, Best Val loss: 0.5841 Best Val acc: 77.50%\n",
            "Epoch 27/100, Train loss: 0.5399, Train acc: 73.5450%, Val loss: 0.6122, Val acc: 75.0000%, Best Val loss: 0.5841 Best Val acc: 77.50%\n",
            "Epoch 28/100, Train loss: 0.5748, Train acc: 71.4286%, Val loss: 0.7292, Val acc: 70.0000%, Best Val loss: 0.5841 Best Val acc: 77.50%\n",
            "Epoch 29/100, Train loss: 0.5427, Train acc: 70.3704%, Val loss: 0.5929, Val acc: 75.0000%, Best Val loss: 0.5841 Best Val acc: 77.50%\n",
            "Epoch 30/100, Train loss: 0.5624, Train acc: 70.3704%, Val loss: 0.6181, Val acc: 77.5000%, Best Val loss: 0.5841 Best Val acc: 77.50%\n",
            "Epoch 31/100, Train loss: 0.5454, Train acc: 70.8995%, Val loss: 0.6215, Val acc: 77.5000%, Best Val loss: 0.5841 Best Val acc: 77.50%\n",
            "Epoch 32/100, Train loss: 0.5511, Train acc: 70.3704%, Val loss: 0.7449, Val acc: 70.0000%, Best Val loss: 0.5841 Best Val acc: 77.50%\n",
            "Epoch 33/100, Train loss: 0.6299, Train acc: 62.4339%, Val loss: 0.6047, Val acc: 72.5000%, Best Val loss: 0.5841 Best Val acc: 77.50%\n",
            "Epoch 34/100, Train loss: 0.5736, Train acc: 66.6667%, Val loss: 0.6501, Val acc: 75.0000%, Best Val loss: 0.5841 Best Val acc: 77.50%\n",
            "Epoch 35/100, Train loss: 0.5470, Train acc: 69.8413%, Val loss: 0.6967, Val acc: 70.0000%, Best Val loss: 0.5841 Best Val acc: 77.50%\n",
            "Epoch 36/100, Train loss: 0.5083, Train acc: 75.1323%, Val loss: 0.5960, Val acc: 75.0000%, Best Val loss: 0.5841 Best Val acc: 77.50%\n",
            "Epoch 37/100, Train loss: 0.5161, Train acc: 75.6614%, Val loss: 0.7811, Val acc: 57.5000%, Best Val loss: 0.5841 Best Val acc: 77.50%\n",
            "Epoch 38/100, Train loss: 0.5280, Train acc: 75.1323%, Val loss: 0.5796, Val acc: 77.5000%, Best Val loss: 0.5796 Best Val acc: 77.50%\n",
            "Epoch 39/100, Train loss: 0.5160, Train acc: 73.5450%, Val loss: 0.6996, Val acc: 70.0000%, Best Val loss: 0.5796 Best Val acc: 77.50%\n",
            "Epoch 40/100, Train loss: 0.5134, Train acc: 74.0741%, Val loss: 0.5894, Val acc: 77.5000%, Best Val loss: 0.5796 Best Val acc: 77.50%\n",
            "Epoch 41/100, Train loss: 0.5259, Train acc: 67.7249%, Val loss: 0.6594, Val acc: 70.0000%, Best Val loss: 0.5796 Best Val acc: 77.50%\n",
            "Epoch 42/100, Train loss: 0.5148, Train acc: 73.0159%, Val loss: 0.5924, Val acc: 77.5000%, Best Val loss: 0.5796 Best Val acc: 77.50%\n",
            "Epoch 43/100, Train loss: 0.5046, Train acc: 75.1323%, Val loss: 0.6978, Val acc: 70.0000%, Best Val loss: 0.5796 Best Val acc: 77.50%\n",
            "Epoch 44/100, Train loss: 0.4956, Train acc: 75.6614%, Val loss: 0.5822, Val acc: 77.5000%, Best Val loss: 0.5796 Best Val acc: 77.50%\n",
            "Epoch 45/100, Train loss: 0.5278, Train acc: 69.8413%, Val loss: 0.7685, Val acc: 65.0000%, Best Val loss: 0.5796 Best Val acc: 77.50%\n",
            "Epoch 46/100, Train loss: 0.5032, Train acc: 75.6614%, Val loss: 0.5791, Val acc: 77.5000%, Best Val loss: 0.5791 Best Val acc: 77.50%\n",
            "Epoch 47/100, Train loss: 0.5108, Train acc: 73.5450%, Val loss: 0.7329, Val acc: 70.0000%, Best Val loss: 0.5791 Best Val acc: 77.50%\n",
            "Epoch 48/100, Train loss: 0.4932, Train acc: 77.2487%, Val loss: 0.5796, Val acc: 80.0000%, Best Val loss: 0.5791 Best Val acc: 80.00%\n",
            "Epoch 49/100, Train loss: 0.4957, Train acc: 74.6032%, Val loss: 0.7087, Val acc: 70.0000%, Best Val loss: 0.5791 Best Val acc: 80.00%\n",
            "Epoch 50/100, Train loss: 0.5070, Train acc: 74.0741%, Val loss: 0.5818, Val acc: 77.5000%, Best Val loss: 0.5791 Best Val acc: 80.00%\n",
            "Epoch 51/100, Train loss: 0.5082, Train acc: 72.4868%, Val loss: 0.6950, Val acc: 72.5000%, Best Val loss: 0.5791 Best Val acc: 80.00%\n",
            "Epoch 52/100, Train loss: 0.4885, Train acc: 75.6614%, Val loss: 0.5962, Val acc: 80.0000%, Best Val loss: 0.5791 Best Val acc: 80.00%\n",
            "Epoch 53/100, Train loss: 0.4941, Train acc: 75.6614%, Val loss: 0.6508, Val acc: 75.0000%, Best Val loss: 0.5791 Best Val acc: 80.00%\n",
            "Epoch 54/100, Train loss: 0.4830, Train acc: 77.2487%, Val loss: 0.5939, Val acc: 80.0000%, Best Val loss: 0.5791 Best Val acc: 80.00%\n",
            "Epoch 55/100, Train loss: 0.4857, Train acc: 75.1323%, Val loss: 0.6182, Val acc: 80.0000%, Best Val loss: 0.5791 Best Val acc: 80.00%\n",
            "Epoch 56/100, Train loss: 0.4782, Train acc: 77.2487%, Val loss: 0.6168, Val acc: 80.0000%, Best Val loss: 0.5791 Best Val acc: 80.00%\n",
            "Epoch 57/100, Train loss: 0.4816, Train acc: 75.6614%, Val loss: 0.6098, Val acc: 77.5000%, Best Val loss: 0.5791 Best Val acc: 80.00%\n",
            "Epoch 58/100, Train loss: 0.4765, Train acc: 78.8360%, Val loss: 0.6455, Val acc: 75.0000%, Best Val loss: 0.5791 Best Val acc: 80.00%\n",
            "Epoch 59/100, Train loss: 0.4754, Train acc: 77.7778%, Val loss: 0.5720, Val acc: 82.5000%, Best Val loss: 0.5720 Best Val acc: 82.50%\n",
            "Epoch 60/100, Train loss: 0.4880, Train acc: 76.1905%, Val loss: 0.6162, Val acc: 80.0000%, Best Val loss: 0.5720 Best Val acc: 82.50%\n",
            "Epoch 61/100, Train loss: 0.4754, Train acc: 79.3651%, Val loss: 0.5945, Val acc: 77.5000%, Best Val loss: 0.5720 Best Val acc: 82.50%\n",
            "Epoch 62/100, Train loss: 0.4701, Train acc: 77.7778%, Val loss: 0.6401, Val acc: 75.0000%, Best Val loss: 0.5720 Best Val acc: 82.50%\n",
            "Epoch 63/100, Train loss: 0.4780, Train acc: 77.7778%, Val loss: 0.5882, Val acc: 80.0000%, Best Val loss: 0.5720 Best Val acc: 82.50%\n",
            "Epoch 64/100, Train loss: 0.4773, Train acc: 75.6614%, Val loss: 0.5769, Val acc: 82.5000%, Best Val loss: 0.5720 Best Val acc: 82.50%\n",
            "Epoch 65/100, Train loss: 0.4656, Train acc: 75.6614%, Val loss: 0.6692, Val acc: 72.5000%, Best Val loss: 0.5720 Best Val acc: 82.50%\n",
            "Epoch 66/100, Train loss: 0.4730, Train acc: 77.2487%, Val loss: 0.5791, Val acc: 82.5000%, Best Val loss: 0.5720 Best Val acc: 82.50%\n",
            "Epoch 67/100, Train loss: 0.4791, Train acc: 75.1323%, Val loss: 0.6115, Val acc: 82.5000%, Best Val loss: 0.5720 Best Val acc: 82.50%\n",
            "Epoch 68/100, Train loss: 0.4680, Train acc: 78.8360%, Val loss: 0.6452, Val acc: 75.0000%, Best Val loss: 0.5720 Best Val acc: 82.50%\n",
            "Epoch 69/100, Train loss: 0.4716, Train acc: 78.3069%, Val loss: 0.5813, Val acc: 82.5000%, Best Val loss: 0.5720 Best Val acc: 82.50%\n",
            "Epoch 70/100, Train loss: 0.4637, Train acc: 76.7196%, Val loss: 0.6112, Val acc: 82.5000%, Best Val loss: 0.5720 Best Val acc: 82.50%\n",
            "Epoch 71/100, Train loss: 0.4813, Train acc: 76.7196%, Val loss: 0.6238, Val acc: 82.5000%, Best Val loss: 0.5720 Best Val acc: 82.50%\n",
            "Epoch 72/100, Train loss: 0.4620, Train acc: 76.7196%, Val loss: 0.5603, Val acc: 80.0000%, Best Val loss: 0.5603 Best Val acc: 82.50%\n",
            "Epoch 73/100, Train loss: 0.4737, Train acc: 74.6032%, Val loss: 0.6258, Val acc: 82.5000%, Best Val loss: 0.5603 Best Val acc: 82.50%\n",
            "Epoch 74/100, Train loss: 0.4682, Train acc: 79.3651%, Val loss: 0.5999, Val acc: 80.0000%, Best Val loss: 0.5603 Best Val acc: 82.50%\n",
            "Epoch 75/100, Train loss: 0.4623, Train acc: 79.8942%, Val loss: 0.6093, Val acc: 82.5000%, Best Val loss: 0.5603 Best Val acc: 82.50%\n",
            "Epoch 76/100, Train loss: 0.4588, Train acc: 79.8942%, Val loss: 0.5910, Val acc: 77.5000%, Best Val loss: 0.5603 Best Val acc: 82.50%\n",
            "Epoch 77/100, Train loss: 0.4610, Train acc: 78.3069%, Val loss: 0.5972, Val acc: 80.0000%, Best Val loss: 0.5603 Best Val acc: 82.50%\n",
            "Epoch 78/100, Train loss: 0.4609, Train acc: 80.9524%, Val loss: 0.6185, Val acc: 85.0000%, Best Val loss: 0.5603 Best Val acc: 85.00%\n",
            "Epoch 79/100, Train loss: 0.4614, Train acc: 78.8360%, Val loss: 0.5846, Val acc: 80.0000%, Best Val loss: 0.5603 Best Val acc: 85.00%\n",
            "Epoch 80/100, Train loss: 0.4607, Train acc: 77.7778%, Val loss: 0.5932, Val acc: 77.5000%, Best Val loss: 0.5603 Best Val acc: 85.00%\n",
            "Epoch 81/100, Train loss: 0.4649, Train acc: 79.8942%, Val loss: 0.6353, Val acc: 80.0000%, Best Val loss: 0.5603 Best Val acc: 85.00%\n",
            "Epoch 82/100, Train loss: 0.4588, Train acc: 80.9524%, Val loss: 0.5920, Val acc: 77.5000%, Best Val loss: 0.5603 Best Val acc: 85.00%\n",
            "Epoch 83/100, Train loss: 0.4575, Train acc: 78.8360%, Val loss: 0.5828, Val acc: 80.0000%, Best Val loss: 0.5603 Best Val acc: 85.00%\n",
            "Epoch 84/100, Train loss: 0.4572, Train acc: 78.3069%, Val loss: 0.5919, Val acc: 80.0000%, Best Val loss: 0.5603 Best Val acc: 85.00%\n",
            "Epoch 85/100, Train loss: 0.4566, Train acc: 80.4233%, Val loss: 0.6088, Val acc: 85.0000%, Best Val loss: 0.5603 Best Val acc: 85.00%\n",
            "Epoch 86/100, Train loss: 0.4577, Train acc: 79.3651%, Val loss: 0.5963, Val acc: 82.5000%, Best Val loss: 0.5603 Best Val acc: 85.00%\n",
            "Epoch 87/100, Train loss: 0.4558, Train acc: 79.8942%, Val loss: 0.5959, Val acc: 80.0000%, Best Val loss: 0.5603 Best Val acc: 85.00%\n",
            "Epoch 88/100, Train loss: 0.4556, Train acc: 78.8360%, Val loss: 0.5966, Val acc: 82.5000%, Best Val loss: 0.5603 Best Val acc: 85.00%\n",
            "Epoch 89/100, Train loss: 0.4545, Train acc: 79.3651%, Val loss: 0.6016, Val acc: 82.5000%, Best Val loss: 0.5603 Best Val acc: 85.00%\n",
            "Epoch 90/100, Train loss: 0.4553, Train acc: 80.4233%, Val loss: 0.5980, Val acc: 82.5000%, Best Val loss: 0.5603 Best Val acc: 85.00%\n",
            "Epoch 91/100, Train loss: 0.4558, Train acc: 80.4233%, Val loss: 0.5997, Val acc: 82.5000%, Best Val loss: 0.5603 Best Val acc: 85.00%\n",
            "Epoch 92/100, Train loss: 0.4570, Train acc: 80.9524%, Val loss: 0.5965, Val acc: 82.5000%, Best Val loss: 0.5603 Best Val acc: 85.00%\n",
            "Epoch 93/100, Train loss: 0.4559, Train acc: 79.3651%, Val loss: 0.5938, Val acc: 80.0000%, Best Val loss: 0.5603 Best Val acc: 85.00%\n",
            "Epoch 94/100, Train loss: 0.4542, Train acc: 79.3651%, Val loss: 0.5942, Val acc: 80.0000%, Best Val loss: 0.5603 Best Val acc: 85.00%\n",
            "Epoch 95/100, Train loss: 0.4541, Train acc: 79.3651%, Val loss: 0.5957, Val acc: 82.5000%, Best Val loss: 0.5603 Best Val acc: 85.00%\n",
            "Epoch 96/100, Train loss: 0.4538, Train acc: 79.8942%, Val loss: 0.5956, Val acc: 82.5000%, Best Val loss: 0.5603 Best Val acc: 85.00%\n",
            "Epoch 97/100, Train loss: 0.4555, Train acc: 79.8942%, Val loss: 0.5957, Val acc: 82.5000%, Best Val loss: 0.5603 Best Val acc: 85.00%\n",
            "Epoch 98/100, Train loss: 0.4558, Train acc: 79.8942%, Val loss: 0.5958, Val acc: 82.5000%, Best Val loss: 0.5603 Best Val acc: 85.00%\n",
            "Epoch 99/100, Train loss: 0.4533, Train acc: 79.8942%, Val loss: 0.5961, Val acc: 82.5000%, Best Val loss: 0.5603 Best Val acc: 85.00%\n",
            "Epoch 100/100, Train loss: 0.4545, Train acc: 79.8942%, Val loss: 0.5961, Val acc: 82.5000%, Best Val loss: 0.5603 Best Val acc: 85.00%\n",
            "\n",
            "Training with LR=0.01, Hidden Units=128\n",
            "Epoch 1/100, Train loss: 2.6697, Train acc: 46.0317%, Val loss: 1.7698, Val acc: 70.0000%, Best Val loss: 1.7698 Best Val acc: 70.00%\n",
            "Epoch 2/100, Train loss: 1.3204, Train acc: 55.5556%, Val loss: 1.3734, Val acc: 42.5000%, Best Val loss: 1.3734 Best Val acc: 70.00%\n",
            "Epoch 3/100, Train loss: 0.9353, Train acc: 54.4974%, Val loss: 0.7096, Val acc: 57.5000%, Best Val loss: 0.7096 Best Val acc: 70.00%\n",
            "Epoch 4/100, Train loss: 0.7496, Train acc: 60.8466%, Val loss: 0.6276, Val acc: 70.0000%, Best Val loss: 0.6276 Best Val acc: 70.00%\n",
            "Epoch 5/100, Train loss: 0.6771, Train acc: 68.2540%, Val loss: 0.8511, Val acc: 60.0000%, Best Val loss: 0.6276 Best Val acc: 70.00%\n",
            "Epoch 6/100, Train loss: 0.6416, Train acc: 67.7249%, Val loss: 0.6099, Val acc: 80.0000%, Best Val loss: 0.6099 Best Val acc: 80.00%\n",
            "Epoch 7/100, Train loss: 0.6180, Train acc: 69.8413%, Val loss: 0.7037, Val acc: 60.0000%, Best Val loss: 0.6099 Best Val acc: 80.00%\n",
            "Epoch 8/100, Train loss: 0.6401, Train acc: 64.0212%, Val loss: 0.7788, Val acc: 65.0000%, Best Val loss: 0.6099 Best Val acc: 80.00%\n",
            "Epoch 9/100, Train loss: 0.6270, Train acc: 66.1376%, Val loss: 0.5780, Val acc: 72.5000%, Best Val loss: 0.5780 Best Val acc: 80.00%\n",
            "Epoch 10/100, Train loss: 0.6077, Train acc: 65.0794%, Val loss: 0.5751, Val acc: 75.0000%, Best Val loss: 0.5751 Best Val acc: 80.00%\n",
            "Epoch 11/100, Train loss: 0.6267, Train acc: 66.1376%, Val loss: 1.0858, Val acc: 40.0000%, Best Val loss: 0.5751 Best Val acc: 80.00%\n",
            "Epoch 12/100, Train loss: 0.6893, Train acc: 62.4339%, Val loss: 0.6004, Val acc: 77.5000%, Best Val loss: 0.5751 Best Val acc: 80.00%\n",
            "Epoch 13/100, Train loss: 0.7118, Train acc: 58.2011%, Val loss: 0.6692, Val acc: 70.0000%, Best Val loss: 0.5751 Best Val acc: 80.00%\n",
            "Epoch 14/100, Train loss: 0.6388, Train acc: 67.1958%, Val loss: 0.7843, Val acc: 65.0000%, Best Val loss: 0.5751 Best Val acc: 80.00%\n",
            "Epoch 15/100, Train loss: 0.5854, Train acc: 69.8413%, Val loss: 0.6625, Val acc: 70.0000%, Best Val loss: 0.5751 Best Val acc: 80.00%\n",
            "Epoch 16/100, Train loss: 0.5756, Train acc: 67.7249%, Val loss: 0.6205, Val acc: 72.5000%, Best Val loss: 0.5751 Best Val acc: 80.00%\n",
            "Epoch 17/100, Train loss: 0.5648, Train acc: 68.2540%, Val loss: 0.5739, Val acc: 77.5000%, Best Val loss: 0.5739 Best Val acc: 80.00%\n",
            "Epoch 18/100, Train loss: 0.5402, Train acc: 71.4286%, Val loss: 0.7134, Val acc: 65.0000%, Best Val loss: 0.5739 Best Val acc: 80.00%\n",
            "Epoch 19/100, Train loss: 0.5680, Train acc: 67.1958%, Val loss: 0.5670, Val acc: 80.0000%, Best Val loss: 0.5670 Best Val acc: 80.00%\n",
            "Epoch 20/100, Train loss: 0.5642, Train acc: 65.6085%, Val loss: 0.5663, Val acc: 72.5000%, Best Val loss: 0.5663 Best Val acc: 80.00%\n",
            "Epoch 21/100, Train loss: 0.6163, Train acc: 66.1376%, Val loss: 0.7013, Val acc: 67.5000%, Best Val loss: 0.5663 Best Val acc: 80.00%\n",
            "Epoch 22/100, Train loss: 0.5470, Train acc: 69.8413%, Val loss: 0.6026, Val acc: 75.0000%, Best Val loss: 0.5663 Best Val acc: 80.00%\n",
            "Epoch 23/100, Train loss: 0.5520, Train acc: 70.3704%, Val loss: 0.5712, Val acc: 70.0000%, Best Val loss: 0.5663 Best Val acc: 80.00%\n",
            "Epoch 24/100, Train loss: 0.5603, Train acc: 69.3122%, Val loss: 0.8304, Val acc: 57.5000%, Best Val loss: 0.5663 Best Val acc: 80.00%\n",
            "Epoch 25/100, Train loss: 0.5334, Train acc: 71.9577%, Val loss: 0.5477, Val acc: 77.5000%, Best Val loss: 0.5477 Best Val acc: 80.00%\n",
            "Epoch 26/100, Train loss: 0.5235, Train acc: 73.5450%, Val loss: 0.6881, Val acc: 67.5000%, Best Val loss: 0.5477 Best Val acc: 80.00%\n",
            "Epoch 27/100, Train loss: 0.6167, Train acc: 65.0794%, Val loss: 0.5477, Val acc: 80.0000%, Best Val loss: 0.5477 Best Val acc: 80.00%\n",
            "Epoch 28/100, Train loss: 0.6635, Train acc: 61.9048%, Val loss: 0.5860, Val acc: 72.5000%, Best Val loss: 0.5477 Best Val acc: 80.00%\n",
            "Epoch 29/100, Train loss: 0.5838, Train acc: 66.1376%, Val loss: 0.5570, Val acc: 77.5000%, Best Val loss: 0.5477 Best Val acc: 80.00%\n",
            "Epoch 30/100, Train loss: 0.5333, Train acc: 74.6032%, Val loss: 0.6150, Val acc: 75.0000%, Best Val loss: 0.5477 Best Val acc: 80.00%\n",
            "Epoch 31/100, Train loss: 0.5427, Train acc: 69.8413%, Val loss: 0.8261, Val acc: 50.0000%, Best Val loss: 0.5477 Best Val acc: 80.00%\n",
            "Epoch 32/100, Train loss: 0.5638, Train acc: 70.8995%, Val loss: 0.5408, Val acc: 82.5000%, Best Val loss: 0.5408 Best Val acc: 82.50%\n",
            "Epoch 33/100, Train loss: 0.5383, Train acc: 69.3122%, Val loss: 0.5422, Val acc: 80.0000%, Best Val loss: 0.5408 Best Val acc: 82.50%\n",
            "Epoch 34/100, Train loss: 0.5344, Train acc: 71.9577%, Val loss: 0.6390, Val acc: 72.5000%, Best Val loss: 0.5408 Best Val acc: 82.50%\n",
            "Epoch 35/100, Train loss: 0.5224, Train acc: 71.9577%, Val loss: 0.6362, Val acc: 75.0000%, Best Val loss: 0.5408 Best Val acc: 82.50%\n",
            "Epoch 36/100, Train loss: 0.5320, Train acc: 71.9577%, Val loss: 0.5481, Val acc: 77.5000%, Best Val loss: 0.5408 Best Val acc: 82.50%\n",
            "Epoch 37/100, Train loss: 0.5215, Train acc: 71.9577%, Val loss: 0.5413, Val acc: 77.5000%, Best Val loss: 0.5408 Best Val acc: 82.50%\n",
            "Epoch 38/100, Train loss: 0.5462, Train acc: 69.3122%, Val loss: 0.6790, Val acc: 67.5000%, Best Val loss: 0.5408 Best Val acc: 82.50%\n",
            "Epoch 39/100, Train loss: 0.5382, Train acc: 69.3122%, Val loss: 0.6593, Val acc: 70.0000%, Best Val loss: 0.5408 Best Val acc: 82.50%\n",
            "Epoch 40/100, Train loss: 0.5005, Train acc: 72.4868%, Val loss: 0.5379, Val acc: 82.5000%, Best Val loss: 0.5379 Best Val acc: 82.50%\n",
            "Epoch 41/100, Train loss: 0.5233, Train acc: 76.1905%, Val loss: 0.5462, Val acc: 82.5000%, Best Val loss: 0.5379 Best Val acc: 82.50%\n",
            "Epoch 42/100, Train loss: 0.4870, Train acc: 76.1905%, Val loss: 0.6935, Val acc: 65.0000%, Best Val loss: 0.5379 Best Val acc: 82.50%\n",
            "Epoch 43/100, Train loss: 0.5421, Train acc: 68.2540%, Val loss: 0.5500, Val acc: 77.5000%, Best Val loss: 0.5379 Best Val acc: 82.50%\n",
            "Epoch 44/100, Train loss: 0.5526, Train acc: 69.8413%, Val loss: 0.5580, Val acc: 77.5000%, Best Val loss: 0.5379 Best Val acc: 82.50%\n",
            "Epoch 45/100, Train loss: 0.5127, Train acc: 73.0159%, Val loss: 0.6367, Val acc: 72.5000%, Best Val loss: 0.5379 Best Val acc: 82.50%\n",
            "Epoch 46/100, Train loss: 0.4891, Train acc: 71.4286%, Val loss: 0.5347, Val acc: 82.5000%, Best Val loss: 0.5347 Best Val acc: 82.50%\n",
            "Epoch 47/100, Train loss: 0.4644, Train acc: 77.2487%, Val loss: 0.6628, Val acc: 67.5000%, Best Val loss: 0.5347 Best Val acc: 82.50%\n",
            "Epoch 48/100, Train loss: 0.4886, Train acc: 76.7196%, Val loss: 0.5039, Val acc: 82.5000%, Best Val loss: 0.5039 Best Val acc: 82.50%\n",
            "Epoch 49/100, Train loss: 0.4900, Train acc: 75.1323%, Val loss: 0.6278, Val acc: 75.0000%, Best Val loss: 0.5039 Best Val acc: 82.50%\n",
            "Epoch 50/100, Train loss: 0.5286, Train acc: 71.9577%, Val loss: 0.6003, Val acc: 75.0000%, Best Val loss: 0.5039 Best Val acc: 82.50%\n",
            "Epoch 51/100, Train loss: 0.5080, Train acc: 71.9577%, Val loss: 0.5094, Val acc: 80.0000%, Best Val loss: 0.5039 Best Val acc: 82.50%\n",
            "Epoch 52/100, Train loss: 0.5099, Train acc: 73.0159%, Val loss: 0.6534, Val acc: 67.5000%, Best Val loss: 0.5039 Best Val acc: 82.50%\n",
            "Epoch 53/100, Train loss: 0.5140, Train acc: 72.4868%, Val loss: 0.5307, Val acc: 82.5000%, Best Val loss: 0.5039 Best Val acc: 82.50%\n",
            "Epoch 54/100, Train loss: 0.4734, Train acc: 78.3069%, Val loss: 0.5154, Val acc: 82.5000%, Best Val loss: 0.5039 Best Val acc: 82.50%\n",
            "Epoch 55/100, Train loss: 0.4725, Train acc: 76.1905%, Val loss: 0.5878, Val acc: 75.0000%, Best Val loss: 0.5039 Best Val acc: 82.50%\n",
            "Epoch 56/100, Train loss: 0.4753, Train acc: 78.3069%, Val loss: 0.5014, Val acc: 82.5000%, Best Val loss: 0.5014 Best Val acc: 82.50%\n",
            "Epoch 57/100, Train loss: 0.4910, Train acc: 75.6614%, Val loss: 0.6149, Val acc: 77.5000%, Best Val loss: 0.5014 Best Val acc: 82.50%\n",
            "Epoch 58/100, Train loss: 0.4710, Train acc: 78.8360%, Val loss: 0.5040, Val acc: 87.5000%, Best Val loss: 0.5014 Best Val acc: 87.50%\n",
            "Epoch 59/100, Train loss: 0.4480, Train acc: 79.8942%, Val loss: 0.5263, Val acc: 82.5000%, Best Val loss: 0.5014 Best Val acc: 87.50%\n",
            "Epoch 60/100, Train loss: 0.4537, Train acc: 81.4815%, Val loss: 0.5487, Val acc: 75.0000%, Best Val loss: 0.5014 Best Val acc: 87.50%\n",
            "Epoch 61/100, Train loss: 0.4559, Train acc: 80.9524%, Val loss: 0.4940, Val acc: 85.0000%, Best Val loss: 0.4940 Best Val acc: 87.50%\n",
            "Epoch 62/100, Train loss: 0.4632, Train acc: 77.2487%, Val loss: 0.5500, Val acc: 77.5000%, Best Val loss: 0.4940 Best Val acc: 87.50%\n",
            "Epoch 63/100, Train loss: 0.4656, Train acc: 76.1905%, Val loss: 0.5206, Val acc: 85.0000%, Best Val loss: 0.4940 Best Val acc: 87.50%\n",
            "Epoch 64/100, Train loss: 0.4471, Train acc: 80.4233%, Val loss: 0.5152, Val acc: 87.5000%, Best Val loss: 0.4940 Best Val acc: 87.50%\n",
            "Epoch 65/100, Train loss: 0.4463, Train acc: 77.7778%, Val loss: 0.5162, Val acc: 85.0000%, Best Val loss: 0.4940 Best Val acc: 87.50%\n",
            "Epoch 66/100, Train loss: 0.4429, Train acc: 79.3651%, Val loss: 0.5119, Val acc: 85.0000%, Best Val loss: 0.4940 Best Val acc: 87.50%\n",
            "Epoch 67/100, Train loss: 0.4361, Train acc: 80.4233%, Val loss: 0.5164, Val acc: 82.5000%, Best Val loss: 0.4940 Best Val acc: 87.50%\n",
            "Epoch 68/100, Train loss: 0.4384, Train acc: 80.4233%, Val loss: 0.5091, Val acc: 85.0000%, Best Val loss: 0.4940 Best Val acc: 87.50%\n",
            "Epoch 69/100, Train loss: 0.4350, Train acc: 79.8942%, Val loss: 0.5181, Val acc: 85.0000%, Best Val loss: 0.4940 Best Val acc: 87.50%\n",
            "Epoch 70/100, Train loss: 0.4371, Train acc: 79.8942%, Val loss: 0.5032, Val acc: 87.5000%, Best Val loss: 0.4940 Best Val acc: 87.50%\n",
            "Epoch 71/100, Train loss: 0.4377, Train acc: 79.8942%, Val loss: 0.5338, Val acc: 82.5000%, Best Val loss: 0.4940 Best Val acc: 87.50%\n",
            "Epoch 72/100, Train loss: 0.4345, Train acc: 78.8360%, Val loss: 0.4966, Val acc: 87.5000%, Best Val loss: 0.4940 Best Val acc: 87.50%\n",
            "Epoch 73/100, Train loss: 0.4315, Train acc: 78.8360%, Val loss: 0.5219, Val acc: 82.5000%, Best Val loss: 0.4940 Best Val acc: 87.50%\n",
            "Epoch 74/100, Train loss: 0.4332, Train acc: 82.5397%, Val loss: 0.5167, Val acc: 85.0000%, Best Val loss: 0.4940 Best Val acc: 87.50%\n",
            "Epoch 75/100, Train loss: 0.4417, Train acc: 79.8942%, Val loss: 0.4959, Val acc: 87.5000%, Best Val loss: 0.4940 Best Val acc: 87.50%\n",
            "Epoch 76/100, Train loss: 0.4400, Train acc: 79.8942%, Val loss: 0.5473, Val acc: 77.5000%, Best Val loss: 0.4940 Best Val acc: 87.50%\n",
            "Epoch 77/100, Train loss: 0.4271, Train acc: 80.4233%, Val loss: 0.4936, Val acc: 87.5000%, Best Val loss: 0.4936 Best Val acc: 87.50%\n",
            "Epoch 78/100, Train loss: 0.4323, Train acc: 80.9524%, Val loss: 0.5001, Val acc: 87.5000%, Best Val loss: 0.4936 Best Val acc: 87.50%\n",
            "Epoch 79/100, Train loss: 0.4273, Train acc: 80.4233%, Val loss: 0.5171, Val acc: 85.0000%, Best Val loss: 0.4936 Best Val acc: 87.50%\n",
            "Epoch 80/100, Train loss: 0.4296, Train acc: 80.9524%, Val loss: 0.4994, Val acc: 87.5000%, Best Val loss: 0.4936 Best Val acc: 87.50%\n",
            "Epoch 81/100, Train loss: 0.4267, Train acc: 79.8942%, Val loss: 0.5025, Val acc: 85.0000%, Best Val loss: 0.4936 Best Val acc: 87.50%\n",
            "Epoch 82/100, Train loss: 0.4271, Train acc: 81.4815%, Val loss: 0.5163, Val acc: 85.0000%, Best Val loss: 0.4936 Best Val acc: 87.50%\n",
            "Epoch 83/100, Train loss: 0.4254, Train acc: 80.9524%, Val loss: 0.4969, Val acc: 87.5000%, Best Val loss: 0.4936 Best Val acc: 87.50%\n",
            "Epoch 84/100, Train loss: 0.4291, Train acc: 79.8942%, Val loss: 0.5006, Val acc: 85.0000%, Best Val loss: 0.4936 Best Val acc: 87.50%\n",
            "Epoch 85/100, Train loss: 0.4248, Train acc: 79.3651%, Val loss: 0.5055, Val acc: 85.0000%, Best Val loss: 0.4936 Best Val acc: 87.50%\n",
            "Epoch 86/100, Train loss: 0.4238, Train acc: 81.4815%, Val loss: 0.5099, Val acc: 85.0000%, Best Val loss: 0.4936 Best Val acc: 87.50%\n",
            "Epoch 87/100, Train loss: 0.4256, Train acc: 82.0106%, Val loss: 0.5134, Val acc: 85.0000%, Best Val loss: 0.4936 Best Val acc: 87.50%\n",
            "Epoch 88/100, Train loss: 0.4258, Train acc: 82.5397%, Val loss: 0.5025, Val acc: 85.0000%, Best Val loss: 0.4936 Best Val acc: 87.50%\n",
            "Epoch 89/100, Train loss: 0.4262, Train acc: 79.8942%, Val loss: 0.4977, Val acc: 87.5000%, Best Val loss: 0.4936 Best Val acc: 87.50%\n",
            "Epoch 90/100, Train loss: 0.4243, Train acc: 79.3651%, Val loss: 0.5024, Val acc: 85.0000%, Best Val loss: 0.4936 Best Val acc: 87.50%\n",
            "Epoch 91/100, Train loss: 0.4244, Train acc: 81.4815%, Val loss: 0.5056, Val acc: 85.0000%, Best Val loss: 0.4936 Best Val acc: 87.50%\n",
            "Epoch 92/100, Train loss: 0.4224, Train acc: 82.0106%, Val loss: 0.5065, Val acc: 85.0000%, Best Val loss: 0.4936 Best Val acc: 87.50%\n",
            "Epoch 93/100, Train loss: 0.4245, Train acc: 80.4233%, Val loss: 0.5020, Val acc: 85.0000%, Best Val loss: 0.4936 Best Val acc: 87.50%\n",
            "Epoch 94/100, Train loss: 0.4241, Train acc: 80.9524%, Val loss: 0.5047, Val acc: 85.0000%, Best Val loss: 0.4936 Best Val acc: 87.50%\n",
            "Epoch 95/100, Train loss: 0.4236, Train acc: 80.4233%, Val loss: 0.5025, Val acc: 85.0000%, Best Val loss: 0.4936 Best Val acc: 87.50%\n",
            "Epoch 96/100, Train loss: 0.4235, Train acc: 80.4233%, Val loss: 0.5028, Val acc: 85.0000%, Best Val loss: 0.4936 Best Val acc: 87.50%\n",
            "Epoch 97/100, Train loss: 0.4225, Train acc: 80.9524%, Val loss: 0.5027, Val acc: 85.0000%, Best Val loss: 0.4936 Best Val acc: 87.50%\n",
            "Epoch 98/100, Train loss: 0.4216, Train acc: 80.4233%, Val loss: 0.5027, Val acc: 85.0000%, Best Val loss: 0.4936 Best Val acc: 87.50%\n",
            "Epoch 99/100, Train loss: 0.4222, Train acc: 80.4233%, Val loss: 0.5026, Val acc: 85.0000%, Best Val loss: 0.4936 Best Val acc: 87.50%\n",
            "Epoch 100/100, Train loss: 0.4231, Train acc: 80.4233%, Val loss: 0.5026, Val acc: 85.0000%, Best Val loss: 0.4936 Best Val acc: 87.50%\n",
            "\n",
            "Training with LR=0.01, Hidden Units=256\n",
            "Epoch 1/100, Train loss: 5.6335, Train acc: 51.8519%, Val loss: 5.8037, Val acc: 30.0000%, Best Val loss: 5.8037 Best Val acc: 30.00%\n",
            "Epoch 2/100, Train loss: 2.3493, Train acc: 52.9101%, Val loss: 2.0198, Val acc: 47.5000%, Best Val loss: 2.0198 Best Val acc: 47.50%\n",
            "Epoch 3/100, Train loss: 1.4959, Train acc: 57.1429%, Val loss: 0.9078, Val acc: 70.0000%, Best Val loss: 0.9078 Best Val acc: 70.00%\n",
            "Epoch 4/100, Train loss: 0.8555, Train acc: 67.7249%, Val loss: 0.6617, Val acc: 67.5000%, Best Val loss: 0.6617 Best Val acc: 70.00%\n",
            "Epoch 5/100, Train loss: 0.6658, Train acc: 67.7249%, Val loss: 0.6910, Val acc: 70.0000%, Best Val loss: 0.6617 Best Val acc: 70.00%\n",
            "Epoch 6/100, Train loss: 0.7806, Train acc: 61.9048%, Val loss: 0.9355, Val acc: 55.0000%, Best Val loss: 0.6617 Best Val acc: 70.00%\n",
            "Epoch 7/100, Train loss: 0.6427, Train acc: 67.7249%, Val loss: 1.6505, Val acc: 37.5000%, Best Val loss: 0.6617 Best Val acc: 70.00%\n",
            "Epoch 8/100, Train loss: 0.7336, Train acc: 64.5503%, Val loss: 0.5896, Val acc: 67.5000%, Best Val loss: 0.5896 Best Val acc: 70.00%\n",
            "Epoch 9/100, Train loss: 0.8334, Train acc: 60.3175%, Val loss: 0.9081, Val acc: 72.5000%, Best Val loss: 0.5896 Best Val acc: 72.50%\n",
            "Epoch 10/100, Train loss: 0.8910, Train acc: 64.0212%, Val loss: 0.8016, Val acc: 75.0000%, Best Val loss: 0.5896 Best Val acc: 75.00%\n",
            "Epoch 11/100, Train loss: 0.7747, Train acc: 62.9630%, Val loss: 0.6880, Val acc: 70.0000%, Best Val loss: 0.5896 Best Val acc: 75.00%\n",
            "Epoch 12/100, Train loss: 0.7483, Train acc: 68.2540%, Val loss: 0.6164, Val acc: 77.5000%, Best Val loss: 0.5896 Best Val acc: 77.50%\n",
            "Epoch 13/100, Train loss: 0.5809, Train acc: 72.4868%, Val loss: 1.2218, Val acc: 45.0000%, Best Val loss: 0.5896 Best Val acc: 77.50%\n",
            "Epoch 14/100, Train loss: 0.6861, Train acc: 66.1376%, Val loss: 1.2032, Val acc: 40.0000%, Best Val loss: 0.5896 Best Val acc: 77.50%\n",
            "Epoch 15/100, Train loss: 0.6931, Train acc: 64.0212%, Val loss: 1.2571, Val acc: 42.5000%, Best Val loss: 0.5896 Best Val acc: 77.50%\n",
            "Epoch 16/100, Train loss: 0.7177, Train acc: 66.6667%, Val loss: 1.3471, Val acc: 42.5000%, Best Val loss: 0.5896 Best Val acc: 77.50%\n",
            "Epoch 17/100, Train loss: 0.8895, Train acc: 62.4339%, Val loss: 0.8864, Val acc: 57.5000%, Best Val loss: 0.5896 Best Val acc: 77.50%\n",
            "Epoch 18/100, Train loss: 0.7042, Train acc: 63.4921%, Val loss: 0.8398, Val acc: 60.0000%, Best Val loss: 0.5896 Best Val acc: 77.50%\n",
            "Epoch 19/100, Train loss: 0.5654, Train acc: 71.4286%, Val loss: 0.5884, Val acc: 80.0000%, Best Val loss: 0.5884 Best Val acc: 80.00%\n",
            "Epoch 20/100, Train loss: 0.5529, Train acc: 72.4868%, Val loss: 0.5817, Val acc: 77.5000%, Best Val loss: 0.5817 Best Val acc: 80.00%\n",
            "Epoch 21/100, Train loss: 0.6699, Train acc: 71.4286%, Val loss: 0.7535, Val acc: 72.5000%, Best Val loss: 0.5817 Best Val acc: 80.00%\n",
            "Epoch 22/100, Train loss: 0.7806, Train acc: 59.2593%, Val loss: 0.6619, Val acc: 75.0000%, Best Val loss: 0.5817 Best Val acc: 80.00%\n",
            "Epoch 23/100, Train loss: 0.7166, Train acc: 62.4339%, Val loss: 0.6390, Val acc: 70.0000%, Best Val loss: 0.5817 Best Val acc: 80.00%\n",
            "Epoch 24/100, Train loss: 0.8026, Train acc: 59.7884%, Val loss: 0.6189, Val acc: 72.5000%, Best Val loss: 0.5817 Best Val acc: 80.00%\n",
            "Epoch 25/100, Train loss: 0.5929, Train acc: 68.2540%, Val loss: 0.5551, Val acc: 80.0000%, Best Val loss: 0.5551 Best Val acc: 80.00%\n",
            "Epoch 26/100, Train loss: 0.5133, Train acc: 73.0159%, Val loss: 0.5534, Val acc: 77.5000%, Best Val loss: 0.5534 Best Val acc: 80.00%\n",
            "Epoch 27/100, Train loss: 0.5297, Train acc: 70.8995%, Val loss: 0.5518, Val acc: 77.5000%, Best Val loss: 0.5518 Best Val acc: 80.00%\n",
            "Epoch 28/100, Train loss: 0.5162, Train acc: 72.4868%, Val loss: 0.5268, Val acc: 82.5000%, Best Val loss: 0.5268 Best Val acc: 82.50%\n",
            "Epoch 29/100, Train loss: 0.5575, Train acc: 70.8995%, Val loss: 0.5589, Val acc: 77.5000%, Best Val loss: 0.5268 Best Val acc: 82.50%\n",
            "Epoch 30/100, Train loss: 0.5521, Train acc: 70.3704%, Val loss: 0.6618, Val acc: 72.5000%, Best Val loss: 0.5268 Best Val acc: 82.50%\n",
            "Epoch 31/100, Train loss: 0.5111, Train acc: 75.1323%, Val loss: 0.8360, Val acc: 55.0000%, Best Val loss: 0.5268 Best Val acc: 82.50%\n",
            "Epoch 32/100, Train loss: 0.5116, Train acc: 73.5450%, Val loss: 0.6469, Val acc: 70.0000%, Best Val loss: 0.5268 Best Val acc: 82.50%\n",
            "Epoch 33/100, Train loss: 0.4815, Train acc: 73.5450%, Val loss: 0.6514, Val acc: 70.0000%, Best Val loss: 0.5268 Best Val acc: 82.50%\n",
            "Epoch 34/100, Train loss: 0.4752, Train acc: 78.3069%, Val loss: 0.5264, Val acc: 82.5000%, Best Val loss: 0.5264 Best Val acc: 82.50%\n",
            "Epoch 35/100, Train loss: 0.4807, Train acc: 76.1905%, Val loss: 0.5188, Val acc: 82.5000%, Best Val loss: 0.5188 Best Val acc: 82.50%\n",
            "Epoch 36/100, Train loss: 0.4898, Train acc: 74.6032%, Val loss: 0.5178, Val acc: 82.5000%, Best Val loss: 0.5178 Best Val acc: 82.50%\n",
            "Epoch 37/100, Train loss: 0.5104, Train acc: 71.9577%, Val loss: 0.7196, Val acc: 65.0000%, Best Val loss: 0.5178 Best Val acc: 82.50%\n",
            "Epoch 38/100, Train loss: 0.4816, Train acc: 77.2487%, Val loss: 0.8634, Val acc: 47.5000%, Best Val loss: 0.5178 Best Val acc: 82.50%\n",
            "Epoch 39/100, Train loss: 0.5929, Train acc: 71.9577%, Val loss: 0.6802, Val acc: 67.5000%, Best Val loss: 0.5178 Best Val acc: 82.50%\n",
            "Epoch 40/100, Train loss: 0.5056, Train acc: 74.0741%, Val loss: 0.5266, Val acc: 87.5000%, Best Val loss: 0.5178 Best Val acc: 87.50%\n",
            "Epoch 41/100, Train loss: 0.4923, Train acc: 73.5450%, Val loss: 0.6500, Val acc: 72.5000%, Best Val loss: 0.5178 Best Val acc: 87.50%\n",
            "Epoch 42/100, Train loss: 0.5960, Train acc: 63.4921%, Val loss: 0.5227, Val acc: 82.5000%, Best Val loss: 0.5178 Best Val acc: 87.50%\n",
            "Epoch 43/100, Train loss: 0.4977, Train acc: 77.7778%, Val loss: 0.5478, Val acc: 77.5000%, Best Val loss: 0.5178 Best Val acc: 87.50%\n",
            "Epoch 44/100, Train loss: 0.4754, Train acc: 74.0741%, Val loss: 0.5164, Val acc: 82.5000%, Best Val loss: 0.5164 Best Val acc: 87.50%\n",
            "Epoch 45/100, Train loss: 0.4496, Train acc: 80.4233%, Val loss: 0.5136, Val acc: 82.5000%, Best Val loss: 0.5136 Best Val acc: 87.50%\n",
            "Epoch 46/100, Train loss: 0.4555, Train acc: 78.3069%, Val loss: 0.5282, Val acc: 82.5000%, Best Val loss: 0.5136 Best Val acc: 87.50%\n",
            "Epoch 47/100, Train loss: 0.4326, Train acc: 80.9524%, Val loss: 0.5040, Val acc: 82.5000%, Best Val loss: 0.5040 Best Val acc: 87.50%\n",
            "Epoch 48/100, Train loss: 0.4767, Train acc: 72.4868%, Val loss: 1.0193, Val acc: 40.0000%, Best Val loss: 0.5040 Best Val acc: 87.50%\n",
            "Epoch 49/100, Train loss: 0.5542, Train acc: 70.8995%, Val loss: 0.7795, Val acc: 50.0000%, Best Val loss: 0.5040 Best Val acc: 87.50%\n",
            "Epoch 50/100, Train loss: 0.6066, Train acc: 66.6667%, Val loss: 0.6244, Val acc: 75.0000%, Best Val loss: 0.5040 Best Val acc: 87.50%\n",
            "Epoch 51/100, Train loss: 0.5393, Train acc: 72.4868%, Val loss: 0.7476, Val acc: 72.5000%, Best Val loss: 0.5040 Best Val acc: 87.50%\n",
            "Epoch 52/100, Train loss: 0.6314, Train acc: 66.6667%, Val loss: 0.5947, Val acc: 75.0000%, Best Val loss: 0.5040 Best Val acc: 87.50%\n",
            "Epoch 53/100, Train loss: 0.4535, Train acc: 78.8360%, Val loss: 0.7398, Val acc: 65.0000%, Best Val loss: 0.5040 Best Val acc: 87.50%\n",
            "Epoch 54/100, Train loss: 0.4719, Train acc: 73.5450%, Val loss: 0.5172, Val acc: 87.5000%, Best Val loss: 0.5040 Best Val acc: 87.50%\n",
            "Epoch 55/100, Train loss: 0.4228, Train acc: 80.4233%, Val loss: 0.5236, Val acc: 80.0000%, Best Val loss: 0.5040 Best Val acc: 87.50%\n",
            "Epoch 56/100, Train loss: 0.4216, Train acc: 82.5397%, Val loss: 0.4904, Val acc: 82.5000%, Best Val loss: 0.4904 Best Val acc: 87.50%\n",
            "Epoch 57/100, Train loss: 0.4616, Train acc: 76.1905%, Val loss: 0.5603, Val acc: 77.5000%, Best Val loss: 0.4904 Best Val acc: 87.50%\n",
            "Epoch 58/100, Train loss: 0.4266, Train acc: 80.4233%, Val loss: 0.5821, Val acc: 77.5000%, Best Val loss: 0.4904 Best Val acc: 87.50%\n",
            "Epoch 59/100, Train loss: 0.4187, Train acc: 79.8942%, Val loss: 0.4928, Val acc: 82.5000%, Best Val loss: 0.4904 Best Val acc: 87.50%\n",
            "Epoch 60/100, Train loss: 0.4451, Train acc: 78.8360%, Val loss: 0.4884, Val acc: 87.5000%, Best Val loss: 0.4884 Best Val acc: 87.50%\n",
            "Epoch 61/100, Train loss: 0.4072, Train acc: 82.5397%, Val loss: 0.4936, Val acc: 85.0000%, Best Val loss: 0.4884 Best Val acc: 87.50%\n",
            "Epoch 62/100, Train loss: 0.4060, Train acc: 83.0688%, Val loss: 0.4860, Val acc: 87.5000%, Best Val loss: 0.4860 Best Val acc: 87.50%\n",
            "Epoch 63/100, Train loss: 0.4096, Train acc: 81.4815%, Val loss: 0.4882, Val acc: 87.5000%, Best Val loss: 0.4860 Best Val acc: 87.50%\n",
            "Epoch 64/100, Train loss: 0.4345, Train acc: 80.4233%, Val loss: 0.5103, Val acc: 80.0000%, Best Val loss: 0.4860 Best Val acc: 87.50%\n",
            "Epoch 65/100, Train loss: 0.4349, Train acc: 78.8360%, Val loss: 0.5843, Val acc: 77.5000%, Best Val loss: 0.4860 Best Val acc: 87.50%\n",
            "Epoch 66/100, Train loss: 0.4237, Train acc: 80.9524%, Val loss: 0.5094, Val acc: 80.0000%, Best Val loss: 0.4860 Best Val acc: 87.50%\n",
            "Epoch 67/100, Train loss: 0.3949, Train acc: 85.1852%, Val loss: 0.5126, Val acc: 80.0000%, Best Val loss: 0.4860 Best Val acc: 87.50%\n",
            "Epoch 68/100, Train loss: 0.4159, Train acc: 78.8360%, Val loss: 0.6118, Val acc: 77.5000%, Best Val loss: 0.4860 Best Val acc: 87.50%\n",
            "Epoch 69/100, Train loss: 0.4272, Train acc: 80.4233%, Val loss: 0.4964, Val acc: 85.0000%, Best Val loss: 0.4860 Best Val acc: 87.50%\n",
            "Epoch 70/100, Train loss: 0.3999, Train acc: 83.0688%, Val loss: 0.4853, Val acc: 87.5000%, Best Val loss: 0.4853 Best Val acc: 87.50%\n",
            "Epoch 71/100, Train loss: 0.4020, Train acc: 83.0688%, Val loss: 0.5478, Val acc: 80.0000%, Best Val loss: 0.4853 Best Val acc: 87.50%\n",
            "Epoch 72/100, Train loss: 0.3969, Train acc: 85.1852%, Val loss: 0.4967, Val acc: 85.0000%, Best Val loss: 0.4853 Best Val acc: 87.50%\n",
            "Epoch 73/100, Train loss: 0.3906, Train acc: 84.6561%, Val loss: 0.4917, Val acc: 85.0000%, Best Val loss: 0.4853 Best Val acc: 87.50%\n",
            "Epoch 74/100, Train loss: 0.3875, Train acc: 85.1852%, Val loss: 0.4956, Val acc: 85.0000%, Best Val loss: 0.4853 Best Val acc: 87.50%\n",
            "Epoch 75/100, Train loss: 0.3858, Train acc: 86.7725%, Val loss: 0.4861, Val acc: 87.5000%, Best Val loss: 0.4853 Best Val acc: 87.50%\n",
            "Epoch 76/100, Train loss: 0.3893, Train acc: 85.1852%, Val loss: 0.4966, Val acc: 82.5000%, Best Val loss: 0.4853 Best Val acc: 87.50%\n",
            "Epoch 77/100, Train loss: 0.3912, Train acc: 84.6561%, Val loss: 0.5010, Val acc: 82.5000%, Best Val loss: 0.4853 Best Val acc: 87.50%\n",
            "Epoch 78/100, Train loss: 0.3830, Train acc: 85.7143%, Val loss: 0.4978, Val acc: 82.5000%, Best Val loss: 0.4853 Best Val acc: 87.50%\n",
            "Epoch 79/100, Train loss: 0.3840, Train acc: 84.1270%, Val loss: 0.4991, Val acc: 82.5000%, Best Val loss: 0.4853 Best Val acc: 87.50%\n",
            "Epoch 80/100, Train loss: 0.3894, Train acc: 85.7143%, Val loss: 0.4897, Val acc: 85.0000%, Best Val loss: 0.4853 Best Val acc: 87.50%\n",
            "Epoch 81/100, Train loss: 0.3841, Train acc: 83.5979%, Val loss: 0.4838, Val acc: 87.5000%, Best Val loss: 0.4838 Best Val acc: 87.50%\n",
            "Epoch 82/100, Train loss: 0.3798, Train acc: 85.7143%, Val loss: 0.5088, Val acc: 82.5000%, Best Val loss: 0.4838 Best Val acc: 87.50%\n",
            "Epoch 83/100, Train loss: 0.3896, Train acc: 85.1852%, Val loss: 0.5030, Val acc: 85.0000%, Best Val loss: 0.4838 Best Val acc: 87.50%\n",
            "Epoch 84/100, Train loss: 0.3833, Train acc: 81.4815%, Val loss: 0.4745, Val acc: 87.5000%, Best Val loss: 0.4745 Best Val acc: 87.50%\n",
            "Epoch 85/100, Train loss: 0.3835, Train acc: 82.5397%, Val loss: 0.5001, Val acc: 82.5000%, Best Val loss: 0.4745 Best Val acc: 87.50%\n",
            "Epoch 86/100, Train loss: 0.3916, Train acc: 84.1270%, Val loss: 0.5312, Val acc: 82.5000%, Best Val loss: 0.4745 Best Val acc: 87.50%\n",
            "Epoch 87/100, Train loss: 0.3873, Train acc: 84.1270%, Val loss: 0.4772, Val acc: 87.5000%, Best Val loss: 0.4745 Best Val acc: 87.50%\n",
            "Epoch 88/100, Train loss: 0.3812, Train acc: 83.5979%, Val loss: 0.4849, Val acc: 85.0000%, Best Val loss: 0.4745 Best Val acc: 87.50%\n",
            "Epoch 89/100, Train loss: 0.3799, Train acc: 84.6561%, Val loss: 0.4977, Val acc: 82.5000%, Best Val loss: 0.4745 Best Val acc: 87.50%\n",
            "Epoch 90/100, Train loss: 0.3793, Train acc: 86.2434%, Val loss: 0.5012, Val acc: 85.0000%, Best Val loss: 0.4745 Best Val acc: 87.50%\n",
            "Epoch 91/100, Train loss: 0.3800, Train acc: 86.7725%, Val loss: 0.4905, Val acc: 85.0000%, Best Val loss: 0.4745 Best Val acc: 87.50%\n",
            "Epoch 92/100, Train loss: 0.3785, Train acc: 86.2434%, Val loss: 0.4938, Val acc: 82.5000%, Best Val loss: 0.4745 Best Val acc: 87.50%\n",
            "Epoch 93/100, Train loss: 0.3822, Train acc: 85.7143%, Val loss: 0.5027, Val acc: 82.5000%, Best Val loss: 0.4745 Best Val acc: 87.50%\n",
            "Epoch 94/100, Train loss: 0.3811, Train acc: 86.7725%, Val loss: 0.4936, Val acc: 82.5000%, Best Val loss: 0.4745 Best Val acc: 87.50%\n",
            "Epoch 95/100, Train loss: 0.3781, Train acc: 85.7143%, Val loss: 0.4941, Val acc: 82.5000%, Best Val loss: 0.4745 Best Val acc: 87.50%\n",
            "Epoch 96/100, Train loss: 0.3794, Train acc: 86.2434%, Val loss: 0.4905, Val acc: 85.0000%, Best Val loss: 0.4745 Best Val acc: 87.50%\n",
            "Epoch 97/100, Train loss: 0.3779, Train acc: 85.7143%, Val loss: 0.4905, Val acc: 85.0000%, Best Val loss: 0.4745 Best Val acc: 87.50%\n",
            "Epoch 98/100, Train loss: 0.3780, Train acc: 85.7143%, Val loss: 0.4906, Val acc: 85.0000%, Best Val loss: 0.4745 Best Val acc: 87.50%\n",
            "Epoch 99/100, Train loss: 0.3804, Train acc: 85.7143%, Val loss: 0.4908, Val acc: 85.0000%, Best Val loss: 0.4745 Best Val acc: 87.50%\n",
            "Epoch 100/100, Train loss: 0.3774, Train acc: 85.7143%, Val loss: 0.4909, Val acc: 85.0000%, Best Val loss: 0.4745 Best Val acc: 87.50%\n",
            "\n",
            "Training with LR=0.001, Hidden Units=64\n",
            "Epoch 1/100, Train loss: 1.2130, Train acc: 51.8519%, Val loss: 1.1419, Val acc: 30.0000%, Best Val loss: 1.1419 Best Val acc: 30.00%\n",
            "Epoch 2/100, Train loss: 0.8277, Train acc: 52.9101%, Val loss: 0.9490, Val acc: 37.5000%, Best Val loss: 0.9490 Best Val acc: 37.50%\n",
            "Epoch 3/100, Train loss: 0.6799, Train acc: 61.9048%, Val loss: 0.6259, Val acc: 72.5000%, Best Val loss: 0.6259 Best Val acc: 72.50%\n",
            "Epoch 4/100, Train loss: 0.7269, Train acc: 59.2593%, Val loss: 0.6435, Val acc: 65.0000%, Best Val loss: 0.6259 Best Val acc: 72.50%\n",
            "Epoch 5/100, Train loss: 0.6268, Train acc: 64.5503%, Val loss: 0.7199, Val acc: 62.5000%, Best Val loss: 0.6259 Best Val acc: 72.50%\n",
            "Epoch 6/100, Train loss: 0.6210, Train acc: 65.0794%, Val loss: 0.5574, Val acc: 80.0000%, Best Val loss: 0.5574 Best Val acc: 80.00%\n",
            "Epoch 7/100, Train loss: 0.6034, Train acc: 68.2540%, Val loss: 0.6157, Val acc: 70.0000%, Best Val loss: 0.5574 Best Val acc: 80.00%\n",
            "Epoch 8/100, Train loss: 0.5947, Train acc: 70.8995%, Val loss: 0.6948, Val acc: 67.5000%, Best Val loss: 0.5574 Best Val acc: 80.00%\n",
            "Epoch 9/100, Train loss: 0.5615, Train acc: 68.7831%, Val loss: 0.5571, Val acc: 77.5000%, Best Val loss: 0.5571 Best Val acc: 80.00%\n",
            "Epoch 10/100, Train loss: 0.5806, Train acc: 71.4286%, Val loss: 0.6342, Val acc: 65.0000%, Best Val loss: 0.5571 Best Val acc: 80.00%\n",
            "Epoch 11/100, Train loss: 0.5771, Train acc: 69.8413%, Val loss: 0.7700, Val acc: 65.0000%, Best Val loss: 0.5571 Best Val acc: 80.00%\n",
            "Epoch 12/100, Train loss: 0.6576, Train acc: 62.9630%, Val loss: 0.5699, Val acc: 75.0000%, Best Val loss: 0.5571 Best Val acc: 80.00%\n",
            "Epoch 13/100, Train loss: 0.6373, Train acc: 65.6085%, Val loss: 0.6598, Val acc: 75.0000%, Best Val loss: 0.5571 Best Val acc: 80.00%\n",
            "Epoch 14/100, Train loss: 0.7701, Train acc: 59.7884%, Val loss: 0.7548, Val acc: 62.5000%, Best Val loss: 0.5571 Best Val acc: 80.00%\n",
            "Epoch 15/100, Train loss: 0.6236, Train acc: 65.0794%, Val loss: 0.7470, Val acc: 62.5000%, Best Val loss: 0.5571 Best Val acc: 80.00%\n",
            "Epoch 16/100, Train loss: 0.6072, Train acc: 66.6667%, Val loss: 0.5552, Val acc: 80.0000%, Best Val loss: 0.5552 Best Val acc: 80.00%\n",
            "Epoch 17/100, Train loss: 0.5447, Train acc: 73.0159%, Val loss: 0.5892, Val acc: 72.5000%, Best Val loss: 0.5552 Best Val acc: 80.00%\n",
            "Epoch 18/100, Train loss: 0.5407, Train acc: 73.0159%, Val loss: 0.6446, Val acc: 67.5000%, Best Val loss: 0.5552 Best Val acc: 80.00%\n",
            "Epoch 19/100, Train loss: 0.5682, Train acc: 68.2540%, Val loss: 0.5461, Val acc: 80.0000%, Best Val loss: 0.5461 Best Val acc: 80.00%\n",
            "Epoch 20/100, Train loss: 0.5320, Train acc: 71.9577%, Val loss: 0.5880, Val acc: 72.5000%, Best Val loss: 0.5461 Best Val acc: 80.00%\n",
            "Epoch 21/100, Train loss: 0.5666, Train acc: 67.1958%, Val loss: 0.7974, Val acc: 60.0000%, Best Val loss: 0.5461 Best Val acc: 80.00%\n",
            "Epoch 22/100, Train loss: 0.5647, Train acc: 70.3704%, Val loss: 0.5551, Val acc: 75.0000%, Best Val loss: 0.5461 Best Val acc: 80.00%\n",
            "Epoch 23/100, Train loss: 0.5427, Train acc: 71.9577%, Val loss: 0.7275, Val acc: 65.0000%, Best Val loss: 0.5461 Best Val acc: 80.00%\n",
            "Epoch 24/100, Train loss: 0.5309, Train acc: 71.4286%, Val loss: 0.5369, Val acc: 77.5000%, Best Val loss: 0.5369 Best Val acc: 80.00%\n",
            "Epoch 25/100, Train loss: 0.5378, Train acc: 70.3704%, Val loss: 0.5604, Val acc: 75.0000%, Best Val loss: 0.5369 Best Val acc: 80.00%\n",
            "Epoch 26/100, Train loss: 0.5279, Train acc: 76.1905%, Val loss: 0.5706, Val acc: 72.5000%, Best Val loss: 0.5369 Best Val acc: 80.00%\n",
            "Epoch 27/100, Train loss: 0.5232, Train acc: 73.0159%, Val loss: 0.6096, Val acc: 70.0000%, Best Val loss: 0.5369 Best Val acc: 80.00%\n",
            "Epoch 28/100, Train loss: 0.5365, Train acc: 74.0741%, Val loss: 0.5407, Val acc: 77.5000%, Best Val loss: 0.5369 Best Val acc: 80.00%\n",
            "Epoch 29/100, Train loss: 0.6008, Train acc: 68.2540%, Val loss: 0.5693, Val acc: 72.5000%, Best Val loss: 0.5369 Best Val acc: 80.00%\n",
            "Epoch 30/100, Train loss: 0.5788, Train acc: 68.7831%, Val loss: 0.7519, Val acc: 62.5000%, Best Val loss: 0.5369 Best Val acc: 80.00%\n",
            "Epoch 31/100, Train loss: 0.5421, Train acc: 71.9577%, Val loss: 0.5286, Val acc: 85.0000%, Best Val loss: 0.5286 Best Val acc: 85.00%\n",
            "Epoch 32/100, Train loss: 0.5309, Train acc: 72.4868%, Val loss: 0.5265, Val acc: 77.5000%, Best Val loss: 0.5265 Best Val acc: 85.00%\n",
            "Epoch 33/100, Train loss: 0.5902, Train acc: 68.2540%, Val loss: 0.6840, Val acc: 67.5000%, Best Val loss: 0.5265 Best Val acc: 85.00%\n",
            "Epoch 34/100, Train loss: 0.5335, Train acc: 75.1323%, Val loss: 0.5672, Val acc: 72.5000%, Best Val loss: 0.5265 Best Val acc: 85.00%\n",
            "Epoch 35/100, Train loss: 0.5205, Train acc: 70.3704%, Val loss: 0.5197, Val acc: 77.5000%, Best Val loss: 0.5197 Best Val acc: 85.00%\n",
            "Epoch 36/100, Train loss: 0.5232, Train acc: 73.5450%, Val loss: 0.9094, Val acc: 50.0000%, Best Val loss: 0.5197 Best Val acc: 85.00%\n",
            "Epoch 37/100, Train loss: 0.6039, Train acc: 68.2540%, Val loss: 0.5326, Val acc: 77.5000%, Best Val loss: 0.5197 Best Val acc: 85.00%\n",
            "Epoch 38/100, Train loss: 0.5503, Train acc: 68.2540%, Val loss: 0.5608, Val acc: 72.5000%, Best Val loss: 0.5197 Best Val acc: 85.00%\n",
            "Epoch 39/100, Train loss: 0.4989, Train acc: 77.7778%, Val loss: 0.5612, Val acc: 75.0000%, Best Val loss: 0.5197 Best Val acc: 85.00%\n",
            "Epoch 40/100, Train loss: 0.4885, Train acc: 76.7196%, Val loss: 0.5528, Val acc: 77.5000%, Best Val loss: 0.5197 Best Val acc: 85.00%\n",
            "Epoch 41/100, Train loss: 0.4868, Train acc: 78.8360%, Val loss: 0.5124, Val acc: 85.0000%, Best Val loss: 0.5124 Best Val acc: 85.00%\n",
            "Epoch 42/100, Train loss: 0.5107, Train acc: 74.0741%, Val loss: 0.6993, Val acc: 65.0000%, Best Val loss: 0.5124 Best Val acc: 85.00%\n",
            "Epoch 43/100, Train loss: 0.4973, Train acc: 73.0159%, Val loss: 0.5111, Val acc: 85.0000%, Best Val loss: 0.5111 Best Val acc: 85.00%\n",
            "Epoch 44/100, Train loss: 0.5165, Train acc: 72.4868%, Val loss: 0.6315, Val acc: 67.5000%, Best Val loss: 0.5111 Best Val acc: 85.00%\n",
            "Epoch 45/100, Train loss: 0.4956, Train acc: 76.1905%, Val loss: 0.5127, Val acc: 87.5000%, Best Val loss: 0.5111 Best Val acc: 87.50%\n",
            "Epoch 46/100, Train loss: 0.4940, Train acc: 77.7778%, Val loss: 0.5109, Val acc: 87.5000%, Best Val loss: 0.5109 Best Val acc: 87.50%\n",
            "Epoch 47/100, Train loss: 0.4949, Train acc: 77.2487%, Val loss: 0.6463, Val acc: 67.5000%, Best Val loss: 0.5109 Best Val acc: 87.50%\n",
            "Epoch 48/100, Train loss: 0.4695, Train acc: 77.7778%, Val loss: 0.4994, Val acc: 80.0000%, Best Val loss: 0.4994 Best Val acc: 87.50%\n",
            "Epoch 49/100, Train loss: 0.4774, Train acc: 78.3069%, Val loss: 0.7565, Val acc: 60.0000%, Best Val loss: 0.4994 Best Val acc: 87.50%\n",
            "Epoch 50/100, Train loss: 0.5082, Train acc: 74.0741%, Val loss: 0.4957, Val acc: 80.0000%, Best Val loss: 0.4957 Best Val acc: 87.50%\n",
            "Epoch 51/100, Train loss: 0.4767, Train acc: 77.2487%, Val loss: 0.5797, Val acc: 70.0000%, Best Val loss: 0.4957 Best Val acc: 87.50%\n",
            "Epoch 52/100, Train loss: 0.4912, Train acc: 75.6614%, Val loss: 0.4948, Val acc: 85.0000%, Best Val loss: 0.4948 Best Val acc: 87.50%\n",
            "Epoch 53/100, Train loss: 0.4681, Train acc: 77.7778%, Val loss: 0.6318, Val acc: 70.0000%, Best Val loss: 0.4948 Best Val acc: 87.50%\n",
            "Epoch 54/100, Train loss: 0.4819, Train acc: 73.5450%, Val loss: 0.4909, Val acc: 85.0000%, Best Val loss: 0.4909 Best Val acc: 87.50%\n",
            "Epoch 55/100, Train loss: 0.4759, Train acc: 77.7778%, Val loss: 0.5723, Val acc: 72.5000%, Best Val loss: 0.4909 Best Val acc: 87.50%\n",
            "Epoch 56/100, Train loss: 0.4813, Train acc: 76.1905%, Val loss: 0.4886, Val acc: 85.0000%, Best Val loss: 0.4886 Best Val acc: 87.50%\n",
            "Epoch 57/100, Train loss: 0.4716, Train acc: 77.7778%, Val loss: 0.5786, Val acc: 70.0000%, Best Val loss: 0.4886 Best Val acc: 87.50%\n",
            "Epoch 58/100, Train loss: 0.4798, Train acc: 77.7778%, Val loss: 0.5080, Val acc: 82.5000%, Best Val loss: 0.4886 Best Val acc: 87.50%\n",
            "Epoch 59/100, Train loss: 0.4583, Train acc: 79.3651%, Val loss: 0.5134, Val acc: 82.5000%, Best Val loss: 0.4886 Best Val acc: 87.50%\n",
            "Epoch 60/100, Train loss: 0.4523, Train acc: 79.8942%, Val loss: 0.4961, Val acc: 82.5000%, Best Val loss: 0.4886 Best Val acc: 87.50%\n",
            "Epoch 61/100, Train loss: 0.4525, Train acc: 80.9524%, Val loss: 0.5171, Val acc: 82.5000%, Best Val loss: 0.4886 Best Val acc: 87.50%\n",
            "Epoch 62/100, Train loss: 0.4572, Train acc: 79.3651%, Val loss: 0.4980, Val acc: 85.0000%, Best Val loss: 0.4886 Best Val acc: 87.50%\n",
            "Epoch 63/100, Train loss: 0.4616, Train acc: 78.3069%, Val loss: 0.5444, Val acc: 80.0000%, Best Val loss: 0.4886 Best Val acc: 87.50%\n",
            "Epoch 64/100, Train loss: 0.4493, Train acc: 80.4233%, Val loss: 0.4934, Val acc: 85.0000%, Best Val loss: 0.4886 Best Val acc: 87.50%\n",
            "Epoch 65/100, Train loss: 0.4741, Train acc: 77.7778%, Val loss: 0.5168, Val acc: 80.0000%, Best Val loss: 0.4886 Best Val acc: 87.50%\n",
            "Epoch 66/100, Train loss: 0.4641, Train acc: 74.6032%, Val loss: 0.5413, Val acc: 80.0000%, Best Val loss: 0.4886 Best Val acc: 87.50%\n",
            "Epoch 67/100, Train loss: 0.4777, Train acc: 77.2487%, Val loss: 0.4850, Val acc: 85.0000%, Best Val loss: 0.4850 Best Val acc: 87.50%\n",
            "Epoch 68/100, Train loss: 0.4581, Train acc: 79.3651%, Val loss: 0.5631, Val acc: 72.5000%, Best Val loss: 0.4850 Best Val acc: 87.50%\n",
            "Epoch 69/100, Train loss: 0.4541, Train acc: 77.7778%, Val loss: 0.4748, Val acc: 87.5000%, Best Val loss: 0.4748 Best Val acc: 87.50%\n",
            "Epoch 70/100, Train loss: 0.4503, Train acc: 78.3069%, Val loss: 0.5177, Val acc: 80.0000%, Best Val loss: 0.4748 Best Val acc: 87.50%\n",
            "Epoch 71/100, Train loss: 0.4530, Train acc: 79.8942%, Val loss: 0.4982, Val acc: 82.5000%, Best Val loss: 0.4748 Best Val acc: 87.50%\n",
            "Epoch 72/100, Train loss: 0.4450, Train acc: 80.4233%, Val loss: 0.4910, Val acc: 85.0000%, Best Val loss: 0.4748 Best Val acc: 87.50%\n",
            "Epoch 73/100, Train loss: 0.4438, Train acc: 80.9524%, Val loss: 0.4846, Val acc: 85.0000%, Best Val loss: 0.4748 Best Val acc: 87.50%\n",
            "Epoch 74/100, Train loss: 0.4431, Train acc: 80.4233%, Val loss: 0.5139, Val acc: 82.5000%, Best Val loss: 0.4748 Best Val acc: 87.50%\n",
            "Epoch 75/100, Train loss: 0.4446, Train acc: 80.9524%, Val loss: 0.4900, Val acc: 85.0000%, Best Val loss: 0.4748 Best Val acc: 87.50%\n",
            "Epoch 76/100, Train loss: 0.4390, Train acc: 78.8360%, Val loss: 0.5030, Val acc: 85.0000%, Best Val loss: 0.4748 Best Val acc: 87.50%\n",
            "Epoch 77/100, Train loss: 0.4428, Train acc: 79.3651%, Val loss: 0.5021, Val acc: 85.0000%, Best Val loss: 0.4748 Best Val acc: 87.50%\n",
            "Epoch 78/100, Train loss: 0.4395, Train acc: 80.4233%, Val loss: 0.4791, Val acc: 85.0000%, Best Val loss: 0.4748 Best Val acc: 87.50%\n",
            "Epoch 79/100, Train loss: 0.4414, Train acc: 79.8942%, Val loss: 0.4955, Val acc: 82.5000%, Best Val loss: 0.4748 Best Val acc: 87.50%\n",
            "Epoch 80/100, Train loss: 0.4384, Train acc: 78.3069%, Val loss: 0.5074, Val acc: 85.0000%, Best Val loss: 0.4748 Best Val acc: 87.50%\n",
            "Epoch 81/100, Train loss: 0.4390, Train acc: 80.9524%, Val loss: 0.5076, Val acc: 85.0000%, Best Val loss: 0.4748 Best Val acc: 87.50%\n",
            "Epoch 82/100, Train loss: 0.4385, Train acc: 79.3651%, Val loss: 0.4873, Val acc: 85.0000%, Best Val loss: 0.4748 Best Val acc: 87.50%\n",
            "Epoch 83/100, Train loss: 0.4379, Train acc: 79.8942%, Val loss: 0.4922, Val acc: 82.5000%, Best Val loss: 0.4748 Best Val acc: 87.50%\n",
            "Epoch 84/100, Train loss: 0.4387, Train acc: 79.8942%, Val loss: 0.4875, Val acc: 85.0000%, Best Val loss: 0.4748 Best Val acc: 87.50%\n",
            "Epoch 85/100, Train loss: 0.4378, Train acc: 78.8360%, Val loss: 0.4996, Val acc: 85.0000%, Best Val loss: 0.4748 Best Val acc: 87.50%\n",
            "Epoch 86/100, Train loss: 0.4368, Train acc: 79.8942%, Val loss: 0.5010, Val acc: 85.0000%, Best Val loss: 0.4748 Best Val acc: 87.50%\n",
            "Epoch 87/100, Train loss: 0.4361, Train acc: 78.8360%, Val loss: 0.4914, Val acc: 82.5000%, Best Val loss: 0.4748 Best Val acc: 87.50%\n",
            "Epoch 88/100, Train loss: 0.4365, Train acc: 79.3651%, Val loss: 0.4938, Val acc: 82.5000%, Best Val loss: 0.4748 Best Val acc: 87.50%\n",
            "Epoch 89/100, Train loss: 0.4385, Train acc: 78.3069%, Val loss: 0.4871, Val acc: 85.0000%, Best Val loss: 0.4748 Best Val acc: 87.50%\n",
            "Epoch 90/100, Train loss: 0.4352, Train acc: 79.8942%, Val loss: 0.4933, Val acc: 82.5000%, Best Val loss: 0.4748 Best Val acc: 87.50%\n",
            "Epoch 91/100, Train loss: 0.4366, Train acc: 79.3651%, Val loss: 0.4967, Val acc: 85.0000%, Best Val loss: 0.4748 Best Val acc: 87.50%\n",
            "Epoch 92/100, Train loss: 0.4358, Train acc: 78.8360%, Val loss: 0.4939, Val acc: 82.5000%, Best Val loss: 0.4748 Best Val acc: 87.50%\n",
            "Epoch 93/100, Train loss: 0.4363, Train acc: 78.8360%, Val loss: 0.4935, Val acc: 82.5000%, Best Val loss: 0.4748 Best Val acc: 87.50%\n",
            "Epoch 94/100, Train loss: 0.4356, Train acc: 78.8360%, Val loss: 0.4950, Val acc: 82.5000%, Best Val loss: 0.4748 Best Val acc: 87.50%\n",
            "Epoch 95/100, Train loss: 0.4366, Train acc: 79.3651%, Val loss: 0.4950, Val acc: 82.5000%, Best Val loss: 0.4748 Best Val acc: 87.50%\n",
            "Epoch 96/100, Train loss: 0.4357, Train acc: 78.8360%, Val loss: 0.4941, Val acc: 82.5000%, Best Val loss: 0.4748 Best Val acc: 87.50%\n",
            "Epoch 97/100, Train loss: 0.4345, Train acc: 78.8360%, Val loss: 0.4935, Val acc: 82.5000%, Best Val loss: 0.4748 Best Val acc: 87.50%\n",
            "Epoch 98/100, Train loss: 0.4351, Train acc: 78.8360%, Val loss: 0.4934, Val acc: 82.5000%, Best Val loss: 0.4748 Best Val acc: 87.50%\n",
            "Epoch 99/100, Train loss: 0.4359, Train acc: 78.8360%, Val loss: 0.4934, Val acc: 82.5000%, Best Val loss: 0.4748 Best Val acc: 87.50%\n",
            "Epoch 100/100, Train loss: 0.4358, Train acc: 78.8360%, Val loss: 0.4934, Val acc: 82.5000%, Best Val loss: 0.4748 Best Val acc: 87.50%\n",
            "\n",
            "Training with LR=0.001, Hidden Units=128\n",
            "Epoch 1/100, Train loss: 2.0106, Train acc: 52.3810%, Val loss: 0.5770, Val acc: 70.0000%, Best Val loss: 0.5770 Best Val acc: 70.00%\n",
            "Epoch 2/100, Train loss: 1.0515, Train acc: 56.0847%, Val loss: 1.0904, Val acc: 50.0000%, Best Val loss: 0.5770 Best Val acc: 70.00%\n",
            "Epoch 3/100, Train loss: 0.7872, Train acc: 60.8466%, Val loss: 1.0649, Val acc: 52.5000%, Best Val loss: 0.5770 Best Val acc: 70.00%\n",
            "Epoch 4/100, Train loss: 0.8400, Train acc: 62.9630%, Val loss: 0.6042, Val acc: 70.0000%, Best Val loss: 0.5770 Best Val acc: 70.00%\n",
            "Epoch 5/100, Train loss: 0.7125, Train acc: 66.1376%, Val loss: 0.6061, Val acc: 72.5000%, Best Val loss: 0.5770 Best Val acc: 72.50%\n",
            "Epoch 6/100, Train loss: 0.6909, Train acc: 66.1376%, Val loss: 0.5876, Val acc: 75.0000%, Best Val loss: 0.5770 Best Val acc: 75.00%\n",
            "Epoch 7/100, Train loss: 0.5956, Train acc: 62.9630%, Val loss: 0.5575, Val acc: 77.5000%, Best Val loss: 0.5575 Best Val acc: 77.50%\n",
            "Epoch 8/100, Train loss: 0.5833, Train acc: 69.3122%, Val loss: 0.5501, Val acc: 77.5000%, Best Val loss: 0.5501 Best Val acc: 77.50%\n",
            "Epoch 9/100, Train loss: 0.6066, Train acc: 67.1958%, Val loss: 0.5953, Val acc: 72.5000%, Best Val loss: 0.5501 Best Val acc: 77.50%\n",
            "Epoch 10/100, Train loss: 0.5800, Train acc: 66.6667%, Val loss: 0.5962, Val acc: 72.5000%, Best Val loss: 0.5501 Best Val acc: 77.50%\n",
            "Epoch 11/100, Train loss: 0.5687, Train acc: 69.8413%, Val loss: 0.6827, Val acc: 67.5000%, Best Val loss: 0.5501 Best Val acc: 77.50%\n",
            "Epoch 12/100, Train loss: 0.5636, Train acc: 68.7831%, Val loss: 0.5584, Val acc: 82.5000%, Best Val loss: 0.5501 Best Val acc: 82.50%\n",
            "Epoch 13/100, Train loss: 0.6420, Train acc: 60.3175%, Val loss: 0.5698, Val acc: 75.0000%, Best Val loss: 0.5501 Best Val acc: 82.50%\n",
            "Epoch 14/100, Train loss: 0.6086, Train acc: 61.9048%, Val loss: 0.5434, Val acc: 82.5000%, Best Val loss: 0.5434 Best Val acc: 82.50%\n",
            "Epoch 15/100, Train loss: 0.5683, Train acc: 70.8995%, Val loss: 0.5421, Val acc: 77.5000%, Best Val loss: 0.5421 Best Val acc: 82.50%\n",
            "Epoch 16/100, Train loss: 0.5806, Train acc: 66.1376%, Val loss: 0.5485, Val acc: 82.5000%, Best Val loss: 0.5421 Best Val acc: 82.50%\n",
            "Epoch 17/100, Train loss: 0.6188, Train acc: 65.6085%, Val loss: 0.5457, Val acc: 77.5000%, Best Val loss: 0.5421 Best Val acc: 82.50%\n",
            "Epoch 18/100, Train loss: 0.5445, Train acc: 71.4286%, Val loss: 0.5898, Val acc: 70.0000%, Best Val loss: 0.5421 Best Val acc: 82.50%\n",
            "Epoch 19/100, Train loss: 0.7043, Train acc: 63.4921%, Val loss: 0.5539, Val acc: 72.5000%, Best Val loss: 0.5421 Best Val acc: 82.50%\n",
            "Epoch 20/100, Train loss: 0.6531, Train acc: 65.0794%, Val loss: 0.6784, Val acc: 72.5000%, Best Val loss: 0.5421 Best Val acc: 82.50%\n",
            "Epoch 21/100, Train loss: 0.5727, Train acc: 72.4868%, Val loss: 0.9214, Val acc: 52.5000%, Best Val loss: 0.5421 Best Val acc: 82.50%\n",
            "Epoch 22/100, Train loss: 0.7212, Train acc: 64.0212%, Val loss: 1.7810, Val acc: 32.5000%, Best Val loss: 0.5421 Best Val acc: 82.50%\n",
            "Epoch 23/100, Train loss: 1.1737, Train acc: 55.0265%, Val loss: 0.6898, Val acc: 70.0000%, Best Val loss: 0.5421 Best Val acc: 82.50%\n",
            "Epoch 24/100, Train loss: 0.8897, Train acc: 61.3757%, Val loss: 0.9248, Val acc: 75.0000%, Best Val loss: 0.5421 Best Val acc: 82.50%\n",
            "Epoch 25/100, Train loss: 1.0853, Train acc: 55.5556%, Val loss: 1.1355, Val acc: 47.5000%, Best Val loss: 0.5421 Best Val acc: 82.50%\n",
            "Epoch 26/100, Train loss: 0.7721, Train acc: 66.6667%, Val loss: 0.7636, Val acc: 70.0000%, Best Val loss: 0.5421 Best Val acc: 82.50%\n",
            "Epoch 27/100, Train loss: 0.6622, Train acc: 65.0794%, Val loss: 0.6556, Val acc: 70.0000%, Best Val loss: 0.5421 Best Val acc: 82.50%\n",
            "Epoch 28/100, Train loss: 0.6412, Train acc: 65.0794%, Val loss: 0.5806, Val acc: 77.5000%, Best Val loss: 0.5421 Best Val acc: 82.50%\n",
            "Epoch 29/100, Train loss: 0.5410, Train acc: 71.9577%, Val loss: 0.6897, Val acc: 70.0000%, Best Val loss: 0.5421 Best Val acc: 82.50%\n",
            "Epoch 30/100, Train loss: 0.5053, Train acc: 73.0159%, Val loss: 0.5907, Val acc: 75.0000%, Best Val loss: 0.5421 Best Val acc: 82.50%\n",
            "Epoch 31/100, Train loss: 0.5064, Train acc: 75.6614%, Val loss: 0.5381, Val acc: 82.5000%, Best Val loss: 0.5381 Best Val acc: 82.50%\n",
            "Epoch 32/100, Train loss: 0.5164, Train acc: 73.5450%, Val loss: 0.5631, Val acc: 75.0000%, Best Val loss: 0.5381 Best Val acc: 82.50%\n",
            "Epoch 33/100, Train loss: 0.5042, Train acc: 76.1905%, Val loss: 0.6051, Val acc: 72.5000%, Best Val loss: 0.5381 Best Val acc: 82.50%\n",
            "Epoch 34/100, Train loss: 0.5054, Train acc: 76.7196%, Val loss: 0.6119, Val acc: 72.5000%, Best Val loss: 0.5381 Best Val acc: 82.50%\n",
            "Epoch 35/100, Train loss: 0.5328, Train acc: 71.4286%, Val loss: 0.5797, Val acc: 70.0000%, Best Val loss: 0.5381 Best Val acc: 82.50%\n",
            "Epoch 36/100, Train loss: 0.5719, Train acc: 67.1958%, Val loss: 0.5194, Val acc: 82.5000%, Best Val loss: 0.5194 Best Val acc: 82.50%\n",
            "Epoch 37/100, Train loss: 0.5163, Train acc: 74.0741%, Val loss: 0.7958, Val acc: 57.5000%, Best Val loss: 0.5194 Best Val acc: 82.50%\n",
            "Epoch 38/100, Train loss: 0.5461, Train acc: 68.7831%, Val loss: 0.5318, Val acc: 82.5000%, Best Val loss: 0.5194 Best Val acc: 82.50%\n",
            "Epoch 39/100, Train loss: 0.4850, Train acc: 76.7196%, Val loss: 0.5352, Val acc: 82.5000%, Best Val loss: 0.5194 Best Val acc: 82.50%\n",
            "Epoch 40/100, Train loss: 0.4884, Train acc: 74.0741%, Val loss: 0.5322, Val acc: 82.5000%, Best Val loss: 0.5194 Best Val acc: 82.50%\n",
            "Epoch 41/100, Train loss: 0.4590, Train acc: 77.2487%, Val loss: 0.7206, Val acc: 65.0000%, Best Val loss: 0.5194 Best Val acc: 82.50%\n",
            "Epoch 42/100, Train loss: 0.4868, Train acc: 77.7778%, Val loss: 0.5509, Val acc: 80.0000%, Best Val loss: 0.5194 Best Val acc: 82.50%\n",
            "Epoch 43/100, Train loss: 0.4803, Train acc: 75.1323%, Val loss: 0.6259, Val acc: 70.0000%, Best Val loss: 0.5194 Best Val acc: 82.50%\n",
            "Epoch 44/100, Train loss: 0.4653, Train acc: 75.6614%, Val loss: 0.5221, Val acc: 82.5000%, Best Val loss: 0.5194 Best Val acc: 82.50%\n",
            "Epoch 45/100, Train loss: 0.4602, Train acc: 79.8942%, Val loss: 0.6355, Val acc: 72.5000%, Best Val loss: 0.5194 Best Val acc: 82.50%\n",
            "Epoch 46/100, Train loss: 0.4680, Train acc: 78.8360%, Val loss: 0.5094, Val acc: 82.5000%, Best Val loss: 0.5094 Best Val acc: 82.50%\n",
            "Epoch 47/100, Train loss: 0.4555, Train acc: 80.9524%, Val loss: 0.5749, Val acc: 77.5000%, Best Val loss: 0.5094 Best Val acc: 82.50%\n",
            "Epoch 48/100, Train loss: 0.4568, Train acc: 80.4233%, Val loss: 0.5039, Val acc: 82.5000%, Best Val loss: 0.5039 Best Val acc: 82.50%\n",
            "Epoch 49/100, Train loss: 0.5009, Train acc: 72.4868%, Val loss: 0.5127, Val acc: 80.0000%, Best Val loss: 0.5039 Best Val acc: 82.50%\n",
            "Epoch 50/100, Train loss: 0.6099, Train acc: 66.6667%, Val loss: 0.6660, Val acc: 72.5000%, Best Val loss: 0.5039 Best Val acc: 82.50%\n",
            "Epoch 51/100, Train loss: 0.5206, Train acc: 69.3122%, Val loss: 0.5720, Val acc: 75.0000%, Best Val loss: 0.5039 Best Val acc: 82.50%\n",
            "Epoch 52/100, Train loss: 0.4805, Train acc: 75.1323%, Val loss: 0.4967, Val acc: 82.5000%, Best Val loss: 0.4967 Best Val acc: 82.50%\n",
            "Epoch 53/100, Train loss: 0.4547, Train acc: 79.3651%, Val loss: 0.5328, Val acc: 82.5000%, Best Val loss: 0.4967 Best Val acc: 82.50%\n",
            "Epoch 54/100, Train loss: 0.4536, Train acc: 78.8360%, Val loss: 0.6075, Val acc: 72.5000%, Best Val loss: 0.4967 Best Val acc: 82.50%\n",
            "Epoch 55/100, Train loss: 0.4836, Train acc: 76.1905%, Val loss: 0.5146, Val acc: 80.0000%, Best Val loss: 0.4967 Best Val acc: 82.50%\n",
            "Epoch 56/100, Train loss: 0.5230, Train acc: 74.6032%, Val loss: 0.7709, Val acc: 55.0000%, Best Val loss: 0.4967 Best Val acc: 82.50%\n",
            "Epoch 57/100, Train loss: 0.4986, Train acc: 74.6032%, Val loss: 0.4882, Val acc: 82.5000%, Best Val loss: 0.4882 Best Val acc: 82.50%\n",
            "Epoch 58/100, Train loss: 0.4885, Train acc: 75.6614%, Val loss: 0.4952, Val acc: 87.5000%, Best Val loss: 0.4882 Best Val acc: 87.50%\n",
            "Epoch 59/100, Train loss: 0.4414, Train acc: 77.7778%, Val loss: 0.5173, Val acc: 82.5000%, Best Val loss: 0.4882 Best Val acc: 87.50%\n",
            "Epoch 60/100, Train loss: 0.4328, Train acc: 80.4233%, Val loss: 0.5140, Val acc: 82.5000%, Best Val loss: 0.4882 Best Val acc: 87.50%\n",
            "Epoch 61/100, Train loss: 0.4285, Train acc: 79.8942%, Val loss: 0.5093, Val acc: 85.0000%, Best Val loss: 0.4882 Best Val acc: 87.50%\n",
            "Epoch 62/100, Train loss: 0.4285, Train acc: 80.9524%, Val loss: 0.5237, Val acc: 82.5000%, Best Val loss: 0.4882 Best Val acc: 87.50%\n",
            "Epoch 63/100, Train loss: 0.4609, Train acc: 77.7778%, Val loss: 0.4750, Val acc: 82.5000%, Best Val loss: 0.4750 Best Val acc: 87.50%\n",
            "Epoch 64/100, Train loss: 0.4476, Train acc: 78.3069%, Val loss: 0.5415, Val acc: 82.5000%, Best Val loss: 0.4750 Best Val acc: 87.50%\n",
            "Epoch 65/100, Train loss: 0.4324, Train acc: 80.9524%, Val loss: 0.4891, Val acc: 85.0000%, Best Val loss: 0.4750 Best Val acc: 87.50%\n",
            "Epoch 66/100, Train loss: 0.4316, Train acc: 81.4815%, Val loss: 0.5255, Val acc: 82.5000%, Best Val loss: 0.4750 Best Val acc: 87.50%\n",
            "Epoch 67/100, Train loss: 0.4227, Train acc: 81.4815%, Val loss: 0.4763, Val acc: 87.5000%, Best Val loss: 0.4750 Best Val acc: 87.50%\n",
            "Epoch 68/100, Train loss: 0.4269, Train acc: 80.4233%, Val loss: 0.5231, Val acc: 82.5000%, Best Val loss: 0.4750 Best Val acc: 87.50%\n",
            "Epoch 69/100, Train loss: 0.4431, Train acc: 80.9524%, Val loss: 0.4822, Val acc: 87.5000%, Best Val loss: 0.4750 Best Val acc: 87.50%\n",
            "Epoch 70/100, Train loss: 0.4322, Train acc: 80.4233%, Val loss: 0.4920, Val acc: 85.0000%, Best Val loss: 0.4750 Best Val acc: 87.50%\n",
            "Epoch 71/100, Train loss: 0.4194, Train acc: 79.8942%, Val loss: 0.5295, Val acc: 82.5000%, Best Val loss: 0.4750 Best Val acc: 87.50%\n",
            "Epoch 72/100, Train loss: 0.4227, Train acc: 81.4815%, Val loss: 0.5089, Val acc: 82.5000%, Best Val loss: 0.4750 Best Val acc: 87.50%\n",
            "Epoch 73/100, Train loss: 0.4189, Train acc: 82.0106%, Val loss: 0.4996, Val acc: 82.5000%, Best Val loss: 0.4750 Best Val acc: 87.50%\n",
            "Epoch 74/100, Train loss: 0.4197, Train acc: 81.4815%, Val loss: 0.4928, Val acc: 85.0000%, Best Val loss: 0.4750 Best Val acc: 87.50%\n",
            "Epoch 75/100, Train loss: 0.4272, Train acc: 80.9524%, Val loss: 0.4982, Val acc: 85.0000%, Best Val loss: 0.4750 Best Val acc: 87.50%\n",
            "Epoch 76/100, Train loss: 0.4214, Train acc: 79.8942%, Val loss: 0.4986, Val acc: 82.5000%, Best Val loss: 0.4750 Best Val acc: 87.50%\n",
            "Epoch 77/100, Train loss: 0.4189, Train acc: 81.4815%, Val loss: 0.5040, Val acc: 82.5000%, Best Val loss: 0.4750 Best Val acc: 87.50%\n",
            "Epoch 78/100, Train loss: 0.4164, Train acc: 82.0106%, Val loss: 0.4996, Val acc: 82.5000%, Best Val loss: 0.4750 Best Val acc: 87.50%\n",
            "Epoch 79/100, Train loss: 0.4147, Train acc: 80.9524%, Val loss: 0.4979, Val acc: 82.5000%, Best Val loss: 0.4750 Best Val acc: 87.50%\n",
            "Epoch 80/100, Train loss: 0.4144, Train acc: 82.5397%, Val loss: 0.5156, Val acc: 82.5000%, Best Val loss: 0.4750 Best Val acc: 87.50%\n",
            "Epoch 81/100, Train loss: 0.4162, Train acc: 82.0106%, Val loss: 0.4963, Val acc: 85.0000%, Best Val loss: 0.4750 Best Val acc: 87.50%\n",
            "Epoch 82/100, Train loss: 0.4211, Train acc: 79.3651%, Val loss: 0.4944, Val acc: 85.0000%, Best Val loss: 0.4750 Best Val acc: 87.50%\n",
            "Epoch 83/100, Train loss: 0.4155, Train acc: 82.5397%, Val loss: 0.5101, Val acc: 82.5000%, Best Val loss: 0.4750 Best Val acc: 87.50%\n",
            "Epoch 84/100, Train loss: 0.4148, Train acc: 81.4815%, Val loss: 0.4978, Val acc: 82.5000%, Best Val loss: 0.4750 Best Val acc: 87.50%\n",
            "Epoch 85/100, Train loss: 0.4099, Train acc: 82.5397%, Val loss: 0.4888, Val acc: 87.5000%, Best Val loss: 0.4750 Best Val acc: 87.50%\n",
            "Epoch 86/100, Train loss: 0.4111, Train acc: 81.4815%, Val loss: 0.4912, Val acc: 85.0000%, Best Val loss: 0.4750 Best Val acc: 87.50%\n",
            "Epoch 87/100, Train loss: 0.4129, Train acc: 82.0106%, Val loss: 0.4969, Val acc: 82.5000%, Best Val loss: 0.4750 Best Val acc: 87.50%\n",
            "Epoch 88/100, Train loss: 0.4129, Train acc: 82.0106%, Val loss: 0.5101, Val acc: 85.0000%, Best Val loss: 0.4750 Best Val acc: 87.50%\n",
            "Epoch 89/100, Train loss: 0.4114, Train acc: 82.0106%, Val loss: 0.5027, Val acc: 82.5000%, Best Val loss: 0.4750 Best Val acc: 87.50%\n",
            "Epoch 90/100, Train loss: 0.4107, Train acc: 82.0106%, Val loss: 0.4943, Val acc: 85.0000%, Best Val loss: 0.4750 Best Val acc: 87.50%\n",
            "Epoch 91/100, Train loss: 0.4114, Train acc: 82.0106%, Val loss: 0.4931, Val acc: 85.0000%, Best Val loss: 0.4750 Best Val acc: 87.50%\n",
            "Epoch 92/100, Train loss: 0.4108, Train acc: 82.0106%, Val loss: 0.4941, Val acc: 82.5000%, Best Val loss: 0.4750 Best Val acc: 87.50%\n",
            "Epoch 93/100, Train loss: 0.4116, Train acc: 82.0106%, Val loss: 0.4916, Val acc: 85.0000%, Best Val loss: 0.4750 Best Val acc: 87.50%\n",
            "Epoch 94/100, Train loss: 0.4105, Train acc: 82.0106%, Val loss: 0.4956, Val acc: 82.5000%, Best Val loss: 0.4750 Best Val acc: 87.50%\n",
            "Epoch 95/100, Train loss: 0.4097, Train acc: 80.9524%, Val loss: 0.4996, Val acc: 82.5000%, Best Val loss: 0.4750 Best Val acc: 87.50%\n",
            "Epoch 96/100, Train loss: 0.4115, Train acc: 81.4815%, Val loss: 0.5012, Val acc: 82.5000%, Best Val loss: 0.4750 Best Val acc: 87.50%\n",
            "Epoch 97/100, Train loss: 0.4108, Train acc: 81.4815%, Val loss: 0.4989, Val acc: 82.5000%, Best Val loss: 0.4750 Best Val acc: 87.50%\n",
            "Epoch 98/100, Train loss: 0.4104, Train acc: 82.5397%, Val loss: 0.4983, Val acc: 82.5000%, Best Val loss: 0.4750 Best Val acc: 87.50%\n",
            "Epoch 99/100, Train loss: 0.4104, Train acc: 83.0688%, Val loss: 0.4983, Val acc: 82.5000%, Best Val loss: 0.4750 Best Val acc: 87.50%\n",
            "Epoch 100/100, Train loss: 0.4094, Train acc: 83.0688%, Val loss: 0.4982, Val acc: 82.5000%, Best Val loss: 0.4750 Best Val acc: 87.50%\n",
            "\n",
            "Training with LR=0.001, Hidden Units=256\n",
            "Epoch 1/100, Train loss: 8.3020, Train acc: 49.2063%, Val loss: 3.3001, Val acc: 70.0000%, Best Val loss: 3.3001 Best Val acc: 70.00%\n",
            "Epoch 2/100, Train loss: 2.9606, Train acc: 49.2063%, Val loss: 1.4993, Val acc: 72.5000%, Best Val loss: 1.4993 Best Val acc: 72.50%\n",
            "Epoch 3/100, Train loss: 2.4013, Train acc: 57.1429%, Val loss: 4.2549, Val acc: 30.0000%, Best Val loss: 1.4993 Best Val acc: 72.50%\n",
            "Epoch 4/100, Train loss: 1.8385, Train acc: 54.4974%, Val loss: 0.9857, Val acc: 70.0000%, Best Val loss: 0.9857 Best Val acc: 72.50%\n",
            "Epoch 5/100, Train loss: 1.2458, Train acc: 59.7884%, Val loss: 1.0138, Val acc: 70.0000%, Best Val loss: 0.9857 Best Val acc: 72.50%\n",
            "Epoch 6/100, Train loss: 1.0104, Train acc: 64.0212%, Val loss: 0.9954, Val acc: 62.5000%, Best Val loss: 0.9857 Best Val acc: 72.50%\n",
            "Epoch 7/100, Train loss: 0.8240, Train acc: 66.6667%, Val loss: 0.8499, Val acc: 72.5000%, Best Val loss: 0.8499 Best Val acc: 72.50%\n",
            "Epoch 8/100, Train loss: 0.7187, Train acc: 68.2540%, Val loss: 1.0107, Val acc: 57.5000%, Best Val loss: 0.8499 Best Val acc: 72.50%\n",
            "Epoch 9/100, Train loss: 0.7108, Train acc: 68.7831%, Val loss: 0.6898, Val acc: 77.5000%, Best Val loss: 0.6898 Best Val acc: 77.50%\n",
            "Epoch 10/100, Train loss: 0.6639, Train acc: 65.0794%, Val loss: 0.7048, Val acc: 65.0000%, Best Val loss: 0.6898 Best Val acc: 77.50%\n",
            "Epoch 11/100, Train loss: 0.6265, Train acc: 66.6667%, Val loss: 0.9106, Val acc: 57.5000%, Best Val loss: 0.6898 Best Val acc: 77.50%\n",
            "Epoch 12/100, Train loss: 0.5913, Train acc: 69.3122%, Val loss: 0.9962, Val acc: 50.0000%, Best Val loss: 0.6898 Best Val acc: 77.50%\n",
            "Epoch 13/100, Train loss: 0.6758, Train acc: 61.9048%, Val loss: 0.8589, Val acc: 52.5000%, Best Val loss: 0.6898 Best Val acc: 77.50%\n",
            "Epoch 14/100, Train loss: 0.6089, Train acc: 67.7249%, Val loss: 0.6147, Val acc: 77.5000%, Best Val loss: 0.6147 Best Val acc: 77.50%\n",
            "Epoch 15/100, Train loss: 0.5552, Train acc: 73.5450%, Val loss: 0.5770, Val acc: 75.0000%, Best Val loss: 0.5770 Best Val acc: 77.50%\n",
            "Epoch 16/100, Train loss: 0.6189, Train acc: 64.5503%, Val loss: 0.6120, Val acc: 72.5000%, Best Val loss: 0.5770 Best Val acc: 77.50%\n",
            "Epoch 17/100, Train loss: 0.6393, Train acc: 65.6085%, Val loss: 0.6089, Val acc: 72.5000%, Best Val loss: 0.5770 Best Val acc: 77.50%\n",
            "Epoch 18/100, Train loss: 0.7728, Train acc: 58.2011%, Val loss: 0.7687, Val acc: 72.5000%, Best Val loss: 0.5770 Best Val acc: 77.50%\n",
            "Epoch 19/100, Train loss: 0.7835, Train acc: 61.9048%, Val loss: 1.0448, Val acc: 72.5000%, Best Val loss: 0.5770 Best Val acc: 77.50%\n",
            "Epoch 20/100, Train loss: 0.8332, Train acc: 64.0212%, Val loss: 0.7231, Val acc: 75.0000%, Best Val loss: 0.5770 Best Val acc: 77.50%\n",
            "Epoch 21/100, Train loss: 0.6552, Train acc: 68.2540%, Val loss: 0.5945, Val acc: 75.0000%, Best Val loss: 0.5770 Best Val acc: 77.50%\n",
            "Epoch 22/100, Train loss: 0.5384, Train acc: 73.5450%, Val loss: 0.5247, Val acc: 82.5000%, Best Val loss: 0.5247 Best Val acc: 82.50%\n",
            "Epoch 23/100, Train loss: 0.6009, Train acc: 68.2540%, Val loss: 0.5519, Val acc: 80.0000%, Best Val loss: 0.5247 Best Val acc: 82.50%\n",
            "Epoch 24/100, Train loss: 0.5719, Train acc: 69.3122%, Val loss: 0.9023, Val acc: 42.5000%, Best Val loss: 0.5247 Best Val acc: 82.50%\n",
            "Epoch 25/100, Train loss: 0.6243, Train acc: 65.6085%, Val loss: 1.4979, Val acc: 35.0000%, Best Val loss: 0.5247 Best Val acc: 82.50%\n",
            "Epoch 26/100, Train loss: 0.7334, Train acc: 66.6667%, Val loss: 0.9877, Val acc: 47.5000%, Best Val loss: 0.5247 Best Val acc: 82.50%\n",
            "Epoch 27/100, Train loss: 0.5674, Train acc: 69.8413%, Val loss: 0.5762, Val acc: 80.0000%, Best Val loss: 0.5247 Best Val acc: 82.50%\n",
            "Epoch 28/100, Train loss: 0.5305, Train acc: 74.0741%, Val loss: 0.5952, Val acc: 77.5000%, Best Val loss: 0.5247 Best Val acc: 82.50%\n",
            "Epoch 29/100, Train loss: 0.5427, Train acc: 72.4868%, Val loss: 0.7349, Val acc: 65.0000%, Best Val loss: 0.5247 Best Val acc: 82.50%\n",
            "Epoch 30/100, Train loss: 0.5578, Train acc: 64.0212%, Val loss: 0.5547, Val acc: 82.5000%, Best Val loss: 0.5247 Best Val acc: 82.50%\n",
            "Epoch 31/100, Train loss: 0.5364, Train acc: 71.9577%, Val loss: 0.5707, Val acc: 80.0000%, Best Val loss: 0.5247 Best Val acc: 82.50%\n",
            "Epoch 32/100, Train loss: 0.5051, Train acc: 76.1905%, Val loss: 0.5147, Val acc: 80.0000%, Best Val loss: 0.5147 Best Val acc: 82.50%\n",
            "Epoch 33/100, Train loss: 0.4715, Train acc: 78.3069%, Val loss: 0.6692, Val acc: 67.5000%, Best Val loss: 0.5147 Best Val acc: 82.50%\n",
            "Epoch 34/100, Train loss: 0.5032, Train acc: 75.6614%, Val loss: 0.6230, Val acc: 75.0000%, Best Val loss: 0.5147 Best Val acc: 82.50%\n",
            "Epoch 35/100, Train loss: 0.5247, Train acc: 70.3704%, Val loss: 0.7421, Val acc: 50.0000%, Best Val loss: 0.5147 Best Val acc: 82.50%\n",
            "Epoch 36/100, Train loss: 0.5131, Train acc: 75.1323%, Val loss: 0.8796, Val acc: 42.5000%, Best Val loss: 0.5147 Best Val acc: 82.50%\n",
            "Epoch 37/100, Train loss: 0.5481, Train acc: 72.4868%, Val loss: 0.6502, Val acc: 67.5000%, Best Val loss: 0.5147 Best Val acc: 82.50%\n",
            "Epoch 38/100, Train loss: 0.4876, Train acc: 75.1323%, Val loss: 0.9663, Val acc: 42.5000%, Best Val loss: 0.5147 Best Val acc: 82.50%\n",
            "Epoch 39/100, Train loss: 0.6234, Train acc: 67.1958%, Val loss: 0.7697, Val acc: 50.0000%, Best Val loss: 0.5147 Best Val acc: 82.50%\n",
            "Epoch 40/100, Train loss: 0.5628, Train acc: 67.1958%, Val loss: 0.7320, Val acc: 60.0000%, Best Val loss: 0.5147 Best Val acc: 82.50%\n",
            "Epoch 41/100, Train loss: 0.5475, Train acc: 73.0159%, Val loss: 1.0883, Val acc: 42.5000%, Best Val loss: 0.5147 Best Val acc: 82.50%\n",
            "Epoch 42/100, Train loss: 0.6531, Train acc: 64.5503%, Val loss: 0.5343, Val acc: 80.0000%, Best Val loss: 0.5147 Best Val acc: 82.50%\n",
            "Epoch 43/100, Train loss: 0.5095, Train acc: 72.4868%, Val loss: 0.5338, Val acc: 77.5000%, Best Val loss: 0.5147 Best Val acc: 82.50%\n",
            "Epoch 44/100, Train loss: 0.5375, Train acc: 70.3704%, Val loss: 0.4967, Val acc: 82.5000%, Best Val loss: 0.4967 Best Val acc: 82.50%\n",
            "Epoch 45/100, Train loss: 0.5827, Train acc: 67.7249%, Val loss: 0.5008, Val acc: 80.0000%, Best Val loss: 0.4967 Best Val acc: 82.50%\n",
            "Epoch 46/100, Train loss: 0.5671, Train acc: 71.9577%, Val loss: 0.6592, Val acc: 75.0000%, Best Val loss: 0.4967 Best Val acc: 82.50%\n",
            "Epoch 47/100, Train loss: 0.6643, Train acc: 66.1376%, Val loss: 0.4586, Val acc: 82.5000%, Best Val loss: 0.4586 Best Val acc: 82.50%\n",
            "Epoch 48/100, Train loss: 0.5407, Train acc: 72.4868%, Val loss: 0.5998, Val acc: 72.5000%, Best Val loss: 0.4586 Best Val acc: 82.50%\n",
            "Epoch 49/100, Train loss: 0.5835, Train acc: 67.1958%, Val loss: 1.1222, Val acc: 42.5000%, Best Val loss: 0.4586 Best Val acc: 82.50%\n",
            "Epoch 50/100, Train loss: 0.7140, Train acc: 65.0794%, Val loss: 0.6259, Val acc: 72.5000%, Best Val loss: 0.4586 Best Val acc: 82.50%\n",
            "Epoch 51/100, Train loss: 0.5603, Train acc: 71.9577%, Val loss: 0.6137, Val acc: 75.0000%, Best Val loss: 0.4586 Best Val acc: 82.50%\n",
            "Epoch 52/100, Train loss: 0.4949, Train acc: 73.5450%, Val loss: 0.5173, Val acc: 82.5000%, Best Val loss: 0.4586 Best Val acc: 82.50%\n",
            "Epoch 53/100, Train loss: 0.4452, Train acc: 79.8942%, Val loss: 0.4975, Val acc: 87.5000%, Best Val loss: 0.4586 Best Val acc: 87.50%\n",
            "Epoch 54/100, Train loss: 0.4844, Train acc: 72.4868%, Val loss: 0.6831, Val acc: 75.0000%, Best Val loss: 0.4586 Best Val acc: 87.50%\n",
            "Epoch 55/100, Train loss: 0.6392, Train acc: 68.7831%, Val loss: 0.4436, Val acc: 87.5000%, Best Val loss: 0.4436 Best Val acc: 87.50%\n",
            "Epoch 56/100, Train loss: 0.4827, Train acc: 74.6032%, Val loss: 0.7432, Val acc: 55.0000%, Best Val loss: 0.4436 Best Val acc: 87.50%\n",
            "Epoch 57/100, Train loss: 0.4608, Train acc: 77.7778%, Val loss: 0.4374, Val acc: 82.5000%, Best Val loss: 0.4374 Best Val acc: 87.50%\n",
            "Epoch 58/100, Train loss: 0.4384, Train acc: 78.8360%, Val loss: 0.4313, Val acc: 90.0000%, Best Val loss: 0.4313 Best Val acc: 90.00%\n",
            "Epoch 59/100, Train loss: 0.4525, Train acc: 78.8360%, Val loss: 0.8217, Val acc: 47.5000%, Best Val loss: 0.4313 Best Val acc: 90.00%\n",
            "Epoch 60/100, Train loss: 0.4977, Train acc: 74.0741%, Val loss: 0.4356, Val acc: 82.5000%, Best Val loss: 0.4313 Best Val acc: 90.00%\n",
            "Epoch 61/100, Train loss: 0.4329, Train acc: 77.7778%, Val loss: 0.4364, Val acc: 87.5000%, Best Val loss: 0.4313 Best Val acc: 90.00%\n",
            "Epoch 62/100, Train loss: 0.4454, Train acc: 77.2487%, Val loss: 0.4315, Val acc: 82.5000%, Best Val loss: 0.4313 Best Val acc: 90.00%\n",
            "Epoch 63/100, Train loss: 0.4771, Train acc: 74.6032%, Val loss: 0.7865, Val acc: 47.5000%, Best Val loss: 0.4313 Best Val acc: 90.00%\n",
            "Epoch 64/100, Train loss: 0.4897, Train acc: 74.0741%, Val loss: 0.4286, Val acc: 82.5000%, Best Val loss: 0.4286 Best Val acc: 90.00%\n",
            "Epoch 65/100, Train loss: 0.4180, Train acc: 80.9524%, Val loss: 0.5062, Val acc: 80.0000%, Best Val loss: 0.4286 Best Val acc: 90.00%\n",
            "Epoch 66/100, Train loss: 0.4261, Train acc: 78.8360%, Val loss: 0.4506, Val acc: 90.0000%, Best Val loss: 0.4286 Best Val acc: 90.00%\n",
            "Epoch 67/100, Train loss: 0.4338, Train acc: 79.8942%, Val loss: 0.4260, Val acc: 87.5000%, Best Val loss: 0.4260 Best Val acc: 90.00%\n",
            "Epoch 68/100, Train loss: 0.4036, Train acc: 80.9524%, Val loss: 0.4695, Val acc: 85.0000%, Best Val loss: 0.4260 Best Val acc: 90.00%\n",
            "Epoch 69/100, Train loss: 0.4031, Train acc: 81.4815%, Val loss: 0.4235, Val acc: 87.5000%, Best Val loss: 0.4235 Best Val acc: 90.00%\n",
            "Epoch 70/100, Train loss: 0.4170, Train acc: 80.9524%, Val loss: 0.4281, Val acc: 90.0000%, Best Val loss: 0.4235 Best Val acc: 90.00%\n",
            "Epoch 71/100, Train loss: 0.4071, Train acc: 81.4815%, Val loss: 0.4809, Val acc: 87.5000%, Best Val loss: 0.4235 Best Val acc: 90.00%\n",
            "Epoch 72/100, Train loss: 0.4186, Train acc: 81.4815%, Val loss: 0.4277, Val acc: 90.0000%, Best Val loss: 0.4235 Best Val acc: 90.00%\n",
            "Epoch 73/100, Train loss: 0.4007, Train acc: 81.4815%, Val loss: 0.4903, Val acc: 85.0000%, Best Val loss: 0.4235 Best Val acc: 90.00%\n",
            "Epoch 74/100, Train loss: 0.4007, Train acc: 83.0688%, Val loss: 0.4246, Val acc: 90.0000%, Best Val loss: 0.4235 Best Val acc: 90.00%\n",
            "Epoch 75/100, Train loss: 0.3980, Train acc: 83.0688%, Val loss: 0.4534, Val acc: 87.5000%, Best Val loss: 0.4235 Best Val acc: 90.00%\n",
            "Epoch 76/100, Train loss: 0.4081, Train acc: 80.4233%, Val loss: 0.4215, Val acc: 90.0000%, Best Val loss: 0.4215 Best Val acc: 90.00%\n",
            "Epoch 77/100, Train loss: 0.4076, Train acc: 79.8942%, Val loss: 0.4602, Val acc: 85.0000%, Best Val loss: 0.4215 Best Val acc: 90.00%\n",
            "Epoch 78/100, Train loss: 0.4146, Train acc: 80.4233%, Val loss: 0.4274, Val acc: 90.0000%, Best Val loss: 0.4215 Best Val acc: 90.00%\n",
            "Epoch 79/100, Train loss: 0.4154, Train acc: 78.3069%, Val loss: 0.4311, Val acc: 90.0000%, Best Val loss: 0.4215 Best Val acc: 90.00%\n",
            "Epoch 80/100, Train loss: 0.4155, Train acc: 78.8360%, Val loss: 0.4724, Val acc: 85.0000%, Best Val loss: 0.4215 Best Val acc: 90.00%\n",
            "Epoch 81/100, Train loss: 0.4003, Train acc: 79.8942%, Val loss: 0.4182, Val acc: 90.0000%, Best Val loss: 0.4182 Best Val acc: 90.00%\n",
            "Epoch 82/100, Train loss: 0.4014, Train acc: 80.4233%, Val loss: 0.4577, Val acc: 85.0000%, Best Val loss: 0.4182 Best Val acc: 90.00%\n",
            "Epoch 83/100, Train loss: 0.3962, Train acc: 82.5397%, Val loss: 0.4279, Val acc: 90.0000%, Best Val loss: 0.4182 Best Val acc: 90.00%\n",
            "Epoch 84/100, Train loss: 0.3922, Train acc: 84.1270%, Val loss: 0.4305, Val acc: 92.5000%, Best Val loss: 0.4182 Best Val acc: 92.50%\n",
            "Epoch 85/100, Train loss: 0.3963, Train acc: 83.0688%, Val loss: 0.4391, Val acc: 90.0000%, Best Val loss: 0.4182 Best Val acc: 92.50%\n",
            "Epoch 86/100, Train loss: 0.3951, Train acc: 84.1270%, Val loss: 0.4337, Val acc: 90.0000%, Best Val loss: 0.4182 Best Val acc: 92.50%\n",
            "Epoch 87/100, Train loss: 0.3939, Train acc: 83.5979%, Val loss: 0.4413, Val acc: 90.0000%, Best Val loss: 0.4182 Best Val acc: 92.50%\n",
            "Epoch 88/100, Train loss: 0.3937, Train acc: 84.1270%, Val loss: 0.4378, Val acc: 90.0000%, Best Val loss: 0.4182 Best Val acc: 92.50%\n",
            "Epoch 89/100, Train loss: 0.3910, Train acc: 84.1270%, Val loss: 0.4356, Val acc: 90.0000%, Best Val loss: 0.4182 Best Val acc: 92.50%\n",
            "Epoch 90/100, Train loss: 0.3929, Train acc: 84.6561%, Val loss: 0.4304, Val acc: 92.5000%, Best Val loss: 0.4182 Best Val acc: 92.50%\n",
            "Epoch 91/100, Train loss: 0.3944, Train acc: 84.1270%, Val loss: 0.4316, Val acc: 92.5000%, Best Val loss: 0.4182 Best Val acc: 92.50%\n",
            "Epoch 92/100, Train loss: 0.3927, Train acc: 84.1270%, Val loss: 0.4307, Val acc: 92.5000%, Best Val loss: 0.4182 Best Val acc: 92.50%\n",
            "Epoch 93/100, Train loss: 0.3920, Train acc: 83.5979%, Val loss: 0.4386, Val acc: 90.0000%, Best Val loss: 0.4182 Best Val acc: 92.50%\n",
            "Epoch 94/100, Train loss: 0.3920, Train acc: 84.1270%, Val loss: 0.4372, Val acc: 90.0000%, Best Val loss: 0.4182 Best Val acc: 92.50%\n",
            "Epoch 95/100, Train loss: 0.3915, Train acc: 84.1270%, Val loss: 0.4360, Val acc: 90.0000%, Best Val loss: 0.4182 Best Val acc: 92.50%\n",
            "Epoch 96/100, Train loss: 0.3900, Train acc: 84.1270%, Val loss: 0.4366, Val acc: 90.0000%, Best Val loss: 0.4182 Best Val acc: 92.50%\n",
            "Epoch 97/100, Train loss: 0.3894, Train acc: 84.1270%, Val loss: 0.4369, Val acc: 90.0000%, Best Val loss: 0.4182 Best Val acc: 92.50%\n",
            "Epoch 98/100, Train loss: 0.3894, Train acc: 84.1270%, Val loss: 0.4362, Val acc: 90.0000%, Best Val loss: 0.4182 Best Val acc: 92.50%\n",
            "Epoch 99/100, Train loss: 0.3917, Train acc: 84.1270%, Val loss: 0.4360, Val acc: 90.0000%, Best Val loss: 0.4182 Best Val acc: 92.50%\n",
            "Epoch 100/100, Train loss: 0.3912, Train acc: 84.1270%, Val loss: 0.4360, Val acc: 90.0000%, Best Val loss: 0.4182 Best Val acc: 92.50%\n",
            "\n",
            "Training with LR=0.0001, Hidden Units=64\n",
            "Epoch 1/100, Train loss: 1.0425, Train acc: 56.0847%, Val loss: 0.8364, Val acc: 55.0000%, Best Val loss: 0.8364 Best Val acc: 55.00%\n",
            "Epoch 2/100, Train loss: 0.6908, Train acc: 63.4921%, Val loss: 0.7496, Val acc: 62.5000%, Best Val loss: 0.7496 Best Val acc: 62.50%\n",
            "Epoch 3/100, Train loss: 0.6474, Train acc: 65.0794%, Val loss: 0.6375, Val acc: 75.0000%, Best Val loss: 0.6375 Best Val acc: 75.00%\n",
            "Epoch 4/100, Train loss: 0.6470, Train acc: 65.0794%, Val loss: 0.6215, Val acc: 67.5000%, Best Val loss: 0.6215 Best Val acc: 75.00%\n",
            "Epoch 5/100, Train loss: 0.6321, Train acc: 66.6667%, Val loss: 0.8957, Val acc: 55.0000%, Best Val loss: 0.6215 Best Val acc: 75.00%\n",
            "Epoch 6/100, Train loss: 0.7397, Train acc: 59.7884%, Val loss: 0.8875, Val acc: 55.0000%, Best Val loss: 0.6215 Best Val acc: 75.00%\n",
            "Epoch 7/100, Train loss: 0.6787, Train acc: 59.2593%, Val loss: 0.6148, Val acc: 70.0000%, Best Val loss: 0.6148 Best Val acc: 75.00%\n",
            "Epoch 8/100, Train loss: 0.6344, Train acc: 69.8413%, Val loss: 0.6066, Val acc: 75.0000%, Best Val loss: 0.6066 Best Val acc: 75.00%\n",
            "Epoch 9/100, Train loss: 0.6386, Train acc: 66.1376%, Val loss: 0.6186, Val acc: 77.5000%, Best Val loss: 0.6066 Best Val acc: 77.50%\n",
            "Epoch 10/100, Train loss: 0.5905, Train acc: 70.8995%, Val loss: 0.5886, Val acc: 77.5000%, Best Val loss: 0.5886 Best Val acc: 77.50%\n",
            "Epoch 11/100, Train loss: 0.5789, Train acc: 67.1958%, Val loss: 0.6276, Val acc: 80.0000%, Best Val loss: 0.5886 Best Val acc: 80.00%\n",
            "Epoch 12/100, Train loss: 0.5887, Train acc: 69.8413%, Val loss: 0.6018, Val acc: 77.5000%, Best Val loss: 0.5886 Best Val acc: 80.00%\n",
            "Epoch 13/100, Train loss: 0.5813, Train acc: 69.3122%, Val loss: 0.5826, Val acc: 80.0000%, Best Val loss: 0.5826 Best Val acc: 80.00%\n",
            "Epoch 14/100, Train loss: 0.5749, Train acc: 68.2540%, Val loss: 0.7089, Val acc: 65.0000%, Best Val loss: 0.5826 Best Val acc: 80.00%\n",
            "Epoch 15/100, Train loss: 0.5769, Train acc: 67.7249%, Val loss: 0.5820, Val acc: 80.0000%, Best Val loss: 0.5820 Best Val acc: 80.00%\n",
            "Epoch 16/100, Train loss: 0.5623, Train acc: 68.7831%, Val loss: 0.6110, Val acc: 77.5000%, Best Val loss: 0.5820 Best Val acc: 80.00%\n",
            "Epoch 17/100, Train loss: 0.5963, Train acc: 71.4286%, Val loss: 0.5689, Val acc: 72.5000%, Best Val loss: 0.5689 Best Val acc: 80.00%\n",
            "Epoch 18/100, Train loss: 0.5518, Train acc: 71.9577%, Val loss: 0.6167, Val acc: 72.5000%, Best Val loss: 0.5689 Best Val acc: 80.00%\n",
            "Epoch 19/100, Train loss: 0.5719, Train acc: 69.8413%, Val loss: 0.8347, Val acc: 52.5000%, Best Val loss: 0.5689 Best Val acc: 80.00%\n",
            "Epoch 20/100, Train loss: 0.6397, Train acc: 66.1376%, Val loss: 0.6041, Val acc: 72.5000%, Best Val loss: 0.5689 Best Val acc: 80.00%\n",
            "Epoch 21/100, Train loss: 0.5838, Train acc: 64.5503%, Val loss: 0.6866, Val acc: 72.5000%, Best Val loss: 0.5689 Best Val acc: 80.00%\n",
            "Epoch 22/100, Train loss: 0.7225, Train acc: 62.4339%, Val loss: 0.6902, Val acc: 67.5000%, Best Val loss: 0.5689 Best Val acc: 80.00%\n",
            "Epoch 23/100, Train loss: 0.5703, Train acc: 71.4286%, Val loss: 0.6771, Val acc: 70.0000%, Best Val loss: 0.5689 Best Val acc: 80.00%\n",
            "Epoch 24/100, Train loss: 0.5870, Train acc: 68.2540%, Val loss: 0.5952, Val acc: 70.0000%, Best Val loss: 0.5689 Best Val acc: 80.00%\n",
            "Epoch 25/100, Train loss: 0.6139, Train acc: 65.6085%, Val loss: 0.5869, Val acc: 77.5000%, Best Val loss: 0.5689 Best Val acc: 80.00%\n",
            "Epoch 26/100, Train loss: 0.6491, Train acc: 66.6667%, Val loss: 1.0142, Val acc: 42.5000%, Best Val loss: 0.5689 Best Val acc: 80.00%\n",
            "Epoch 27/100, Train loss: 0.6208, Train acc: 65.6085%, Val loss: 0.5865, Val acc: 72.5000%, Best Val loss: 0.5689 Best Val acc: 80.00%\n",
            "Epoch 28/100, Train loss: 0.5824, Train acc: 68.2540%, Val loss: 0.5635, Val acc: 82.5000%, Best Val loss: 0.5635 Best Val acc: 82.50%\n",
            "Epoch 29/100, Train loss: 0.5628, Train acc: 68.2540%, Val loss: 0.9031, Val acc: 45.0000%, Best Val loss: 0.5635 Best Val acc: 82.50%\n",
            "Epoch 30/100, Train loss: 0.5862, Train acc: 67.7249%, Val loss: 0.5853, Val acc: 72.5000%, Best Val loss: 0.5635 Best Val acc: 82.50%\n",
            "Epoch 31/100, Train loss: 0.6175, Train acc: 70.3704%, Val loss: 0.5986, Val acc: 72.5000%, Best Val loss: 0.5635 Best Val acc: 82.50%\n",
            "Epoch 32/100, Train loss: 0.5379, Train acc: 73.5450%, Val loss: 0.5913, Val acc: 77.5000%, Best Val loss: 0.5635 Best Val acc: 82.50%\n",
            "Epoch 33/100, Train loss: 0.5556, Train acc: 73.0159%, Val loss: 0.7382, Val acc: 57.5000%, Best Val loss: 0.5635 Best Val acc: 82.50%\n",
            "Epoch 34/100, Train loss: 0.5998, Train acc: 64.5503%, Val loss: 0.5516, Val acc: 80.0000%, Best Val loss: 0.5516 Best Val acc: 82.50%\n",
            "Epoch 35/100, Train loss: 0.5377, Train acc: 73.0159%, Val loss: 0.5621, Val acc: 82.5000%, Best Val loss: 0.5516 Best Val acc: 82.50%\n",
            "Epoch 36/100, Train loss: 0.5385, Train acc: 73.5450%, Val loss: 0.6481, Val acc: 75.0000%, Best Val loss: 0.5516 Best Val acc: 82.50%\n",
            "Epoch 37/100, Train loss: 0.5254, Train acc: 70.8995%, Val loss: 0.5467, Val acc: 75.0000%, Best Val loss: 0.5467 Best Val acc: 82.50%\n",
            "Epoch 38/100, Train loss: 0.5688, Train acc: 67.7249%, Val loss: 0.5871, Val acc: 75.0000%, Best Val loss: 0.5467 Best Val acc: 82.50%\n",
            "Epoch 39/100, Train loss: 0.5223, Train acc: 72.4868%, Val loss: 0.5678, Val acc: 77.5000%, Best Val loss: 0.5467 Best Val acc: 82.50%\n",
            "Epoch 40/100, Train loss: 0.5165, Train acc: 74.0741%, Val loss: 0.6243, Val acc: 72.5000%, Best Val loss: 0.5467 Best Val acc: 82.50%\n",
            "Epoch 41/100, Train loss: 0.5198, Train acc: 75.1323%, Val loss: 0.5351, Val acc: 77.5000%, Best Val loss: 0.5351 Best Val acc: 82.50%\n",
            "Epoch 42/100, Train loss: 0.5437, Train acc: 72.4868%, Val loss: 0.6802, Val acc: 67.5000%, Best Val loss: 0.5351 Best Val acc: 82.50%\n",
            "Epoch 43/100, Train loss: 0.5568, Train acc: 70.3704%, Val loss: 0.5520, Val acc: 82.5000%, Best Val loss: 0.5351 Best Val acc: 82.50%\n",
            "Epoch 44/100, Train loss: 0.5462, Train acc: 71.4286%, Val loss: 0.5284, Val acc: 77.5000%, Best Val loss: 0.5284 Best Val acc: 82.50%\n",
            "Epoch 45/100, Train loss: 0.5296, Train acc: 75.1323%, Val loss: 0.5885, Val acc: 75.0000%, Best Val loss: 0.5284 Best Val acc: 82.50%\n",
            "Epoch 46/100, Train loss: 0.5061, Train acc: 74.6032%, Val loss: 0.5489, Val acc: 82.5000%, Best Val loss: 0.5284 Best Val acc: 82.50%\n",
            "Epoch 47/100, Train loss: 0.5035, Train acc: 75.1323%, Val loss: 0.5222, Val acc: 82.5000%, Best Val loss: 0.5222 Best Val acc: 82.50%\n",
            "Epoch 48/100, Train loss: 0.5458, Train acc: 70.3704%, Val loss: 0.6156, Val acc: 75.0000%, Best Val loss: 0.5222 Best Val acc: 82.50%\n",
            "Epoch 49/100, Train loss: 0.5074, Train acc: 75.1323%, Val loss: 0.5223, Val acc: 82.5000%, Best Val loss: 0.5222 Best Val acc: 82.50%\n",
            "Epoch 50/100, Train loss: 0.4958, Train acc: 75.1323%, Val loss: 0.5731, Val acc: 75.0000%, Best Val loss: 0.5222 Best Val acc: 82.50%\n",
            "Epoch 51/100, Train loss: 0.4982, Train acc: 76.7196%, Val loss: 0.5356, Val acc: 82.5000%, Best Val loss: 0.5222 Best Val acc: 82.50%\n",
            "Epoch 52/100, Train loss: 0.5013, Train acc: 73.5450%, Val loss: 0.5166, Val acc: 82.5000%, Best Val loss: 0.5166 Best Val acc: 82.50%\n",
            "Epoch 53/100, Train loss: 0.5132, Train acc: 74.0741%, Val loss: 0.6505, Val acc: 70.0000%, Best Val loss: 0.5166 Best Val acc: 82.50%\n",
            "Epoch 54/100, Train loss: 0.5134, Train acc: 72.4868%, Val loss: 0.5141, Val acc: 82.5000%, Best Val loss: 0.5141 Best Val acc: 82.50%\n",
            "Epoch 55/100, Train loss: 0.4883, Train acc: 76.7196%, Val loss: 0.5568, Val acc: 80.0000%, Best Val loss: 0.5141 Best Val acc: 82.50%\n",
            "Epoch 56/100, Train loss: 0.4980, Train acc: 74.0741%, Val loss: 0.5141, Val acc: 85.0000%, Best Val loss: 0.5141 Best Val acc: 85.00%\n",
            "Epoch 57/100, Train loss: 0.4935, Train acc: 76.1905%, Val loss: 0.5293, Val acc: 82.5000%, Best Val loss: 0.5141 Best Val acc: 85.00%\n",
            "Epoch 58/100, Train loss: 0.4866, Train acc: 75.6614%, Val loss: 0.5512, Val acc: 82.5000%, Best Val loss: 0.5141 Best Val acc: 85.00%\n",
            "Epoch 59/100, Train loss: 0.4923, Train acc: 76.1905%, Val loss: 0.5062, Val acc: 85.0000%, Best Val loss: 0.5062 Best Val acc: 85.00%\n",
            "Epoch 60/100, Train loss: 0.5159, Train acc: 73.5450%, Val loss: 0.5821, Val acc: 75.0000%, Best Val loss: 0.5062 Best Val acc: 85.00%\n",
            "Epoch 61/100, Train loss: 0.4832, Train acc: 77.7778%, Val loss: 0.5150, Val acc: 85.0000%, Best Val loss: 0.5062 Best Val acc: 85.00%\n",
            "Epoch 62/100, Train loss: 0.5169, Train acc: 72.4868%, Val loss: 0.5057, Val acc: 85.0000%, Best Val loss: 0.5057 Best Val acc: 85.00%\n",
            "Epoch 63/100, Train loss: 0.5174, Train acc: 75.6614%, Val loss: 0.5472, Val acc: 82.5000%, Best Val loss: 0.5057 Best Val acc: 85.00%\n",
            "Epoch 64/100, Train loss: 0.4986, Train acc: 75.1323%, Val loss: 0.5270, Val acc: 82.5000%, Best Val loss: 0.5057 Best Val acc: 85.00%\n",
            "Epoch 65/100, Train loss: 0.5043, Train acc: 76.7196%, Val loss: 0.5239, Val acc: 82.5000%, Best Val loss: 0.5057 Best Val acc: 85.00%\n",
            "Epoch 66/100, Train loss: 0.5207, Train acc: 70.3704%, Val loss: 0.5531, Val acc: 77.5000%, Best Val loss: 0.5057 Best Val acc: 85.00%\n",
            "Epoch 67/100, Train loss: 0.4980, Train acc: 74.0741%, Val loss: 0.5019, Val acc: 82.5000%, Best Val loss: 0.5019 Best Val acc: 85.00%\n",
            "Epoch 68/100, Train loss: 0.4820, Train acc: 75.1323%, Val loss: 0.6093, Val acc: 75.0000%, Best Val loss: 0.5019 Best Val acc: 85.00%\n",
            "Epoch 69/100, Train loss: 0.4970, Train acc: 74.6032%, Val loss: 0.4995, Val acc: 85.0000%, Best Val loss: 0.4995 Best Val acc: 85.00%\n",
            "Epoch 70/100, Train loss: 0.4763, Train acc: 77.7778%, Val loss: 0.5582, Val acc: 77.5000%, Best Val loss: 0.4995 Best Val acc: 85.00%\n",
            "Epoch 71/100, Train loss: 0.4772, Train acc: 77.7778%, Val loss: 0.5183, Val acc: 82.5000%, Best Val loss: 0.4995 Best Val acc: 85.00%\n",
            "Epoch 72/100, Train loss: 0.4683, Train acc: 77.2487%, Val loss: 0.5204, Val acc: 85.0000%, Best Val loss: 0.4995 Best Val acc: 85.00%\n",
            "Epoch 73/100, Train loss: 0.4723, Train acc: 77.7778%, Val loss: 0.5204, Val acc: 82.5000%, Best Val loss: 0.4995 Best Val acc: 85.00%\n",
            "Epoch 74/100, Train loss: 0.4784, Train acc: 77.2487%, Val loss: 0.5044, Val acc: 87.5000%, Best Val loss: 0.4995 Best Val acc: 87.50%\n",
            "Epoch 75/100, Train loss: 0.4699, Train acc: 77.2487%, Val loss: 0.5475, Val acc: 80.0000%, Best Val loss: 0.4995 Best Val acc: 87.50%\n",
            "Epoch 76/100, Train loss: 0.4710, Train acc: 78.8360%, Val loss: 0.5139, Val acc: 82.5000%, Best Val loss: 0.4995 Best Val acc: 87.50%\n",
            "Epoch 77/100, Train loss: 0.4681, Train acc: 78.3069%, Val loss: 0.5046, Val acc: 85.0000%, Best Val loss: 0.4995 Best Val acc: 87.50%\n",
            "Epoch 78/100, Train loss: 0.4661, Train acc: 77.2487%, Val loss: 0.5224, Val acc: 82.5000%, Best Val loss: 0.4995 Best Val acc: 87.50%\n",
            "Epoch 79/100, Train loss: 0.4659, Train acc: 77.7778%, Val loss: 0.5305, Val acc: 85.0000%, Best Val loss: 0.4995 Best Val acc: 87.50%\n",
            "Epoch 80/100, Train loss: 0.4666, Train acc: 77.7778%, Val loss: 0.5153, Val acc: 82.5000%, Best Val loss: 0.4995 Best Val acc: 87.50%\n",
            "Epoch 81/100, Train loss: 0.4640, Train acc: 76.7196%, Val loss: 0.5109, Val acc: 87.5000%, Best Val loss: 0.4995 Best Val acc: 87.50%\n",
            "Epoch 82/100, Train loss: 0.4640, Train acc: 78.3069%, Val loss: 0.5103, Val acc: 85.0000%, Best Val loss: 0.4995 Best Val acc: 87.50%\n",
            "Epoch 83/100, Train loss: 0.4639, Train acc: 76.7196%, Val loss: 0.5216, Val acc: 82.5000%, Best Val loss: 0.4995 Best Val acc: 87.50%\n",
            "Epoch 84/100, Train loss: 0.4655, Train acc: 77.7778%, Val loss: 0.5317, Val acc: 85.0000%, Best Val loss: 0.4995 Best Val acc: 87.50%\n",
            "Epoch 85/100, Train loss: 0.4628, Train acc: 78.3069%, Val loss: 0.5154, Val acc: 82.5000%, Best Val loss: 0.4995 Best Val acc: 87.50%\n",
            "Epoch 86/100, Train loss: 0.4650, Train acc: 78.3069%, Val loss: 0.5057, Val acc: 85.0000%, Best Val loss: 0.4995 Best Val acc: 87.50%\n",
            "Epoch 87/100, Train loss: 0.4661, Train acc: 77.2487%, Val loss: 0.5142, Val acc: 82.5000%, Best Val loss: 0.4995 Best Val acc: 87.50%\n",
            "Epoch 88/100, Train loss: 0.4629, Train acc: 76.1905%, Val loss: 0.5134, Val acc: 82.5000%, Best Val loss: 0.4995 Best Val acc: 87.50%\n",
            "Epoch 89/100, Train loss: 0.4624, Train acc: 77.2487%, Val loss: 0.5205, Val acc: 82.5000%, Best Val loss: 0.4995 Best Val acc: 87.50%\n",
            "Epoch 90/100, Train loss: 0.4625, Train acc: 78.8360%, Val loss: 0.5247, Val acc: 82.5000%, Best Val loss: 0.4995 Best Val acc: 87.50%\n",
            "Epoch 91/100, Train loss: 0.4622, Train acc: 78.3069%, Val loss: 0.5205, Val acc: 82.5000%, Best Val loss: 0.4995 Best Val acc: 87.50%\n",
            "Epoch 92/100, Train loss: 0.4630, Train acc: 78.3069%, Val loss: 0.5151, Val acc: 82.5000%, Best Val loss: 0.4995 Best Val acc: 87.50%\n",
            "Epoch 93/100, Train loss: 0.4617, Train acc: 77.2487%, Val loss: 0.5128, Val acc: 82.5000%, Best Val loss: 0.4995 Best Val acc: 87.50%\n",
            "Epoch 94/100, Train loss: 0.4634, Train acc: 76.7196%, Val loss: 0.5119, Val acc: 82.5000%, Best Val loss: 0.4995 Best Val acc: 87.50%\n",
            "Epoch 95/100, Train loss: 0.4626, Train acc: 77.2487%, Val loss: 0.5143, Val acc: 82.5000%, Best Val loss: 0.4995 Best Val acc: 87.50%\n",
            "Epoch 96/100, Train loss: 0.4623, Train acc: 77.7778%, Val loss: 0.5142, Val acc: 82.5000%, Best Val loss: 0.4995 Best Val acc: 87.50%\n",
            "Epoch 97/100, Train loss: 0.4627, Train acc: 77.7778%, Val loss: 0.5146, Val acc: 82.5000%, Best Val loss: 0.4995 Best Val acc: 87.50%\n",
            "Epoch 98/100, Train loss: 0.4619, Train acc: 77.2487%, Val loss: 0.5148, Val acc: 82.5000%, Best Val loss: 0.4995 Best Val acc: 87.50%\n",
            "Epoch 99/100, Train loss: 0.4610, Train acc: 77.2487%, Val loss: 0.5147, Val acc: 82.5000%, Best Val loss: 0.4995 Best Val acc: 87.50%\n",
            "Epoch 100/100, Train loss: 0.4602, Train acc: 77.2487%, Val loss: 0.5148, Val acc: 82.5000%, Best Val loss: 0.4995 Best Val acc: 87.50%\n",
            "\n",
            "Training with LR=0.0001, Hidden Units=128\n",
            "Epoch 1/100, Train loss: 3.2567, Train acc: 48.6772%, Val loss: 0.6712, Val acc: 70.0000%, Best Val loss: 0.6712 Best Val acc: 70.00%\n",
            "Epoch 2/100, Train loss: 1.5289, Train acc: 53.4392%, Val loss: 1.2525, Val acc: 57.5000%, Best Val loss: 0.6712 Best Val acc: 70.00%\n",
            "Epoch 3/100, Train loss: 1.0622, Train acc: 59.7884%, Val loss: 0.6965, Val acc: 70.0000%, Best Val loss: 0.6712 Best Val acc: 70.00%\n",
            "Epoch 4/100, Train loss: 0.8719, Train acc: 64.0212%, Val loss: 0.8578, Val acc: 62.5000%, Best Val loss: 0.6712 Best Val acc: 70.00%\n",
            "Epoch 5/100, Train loss: 0.7582, Train acc: 61.9048%, Val loss: 0.5919, Val acc: 77.5000%, Best Val loss: 0.5919 Best Val acc: 77.50%\n",
            "Epoch 6/100, Train loss: 0.6979, Train acc: 62.4339%, Val loss: 0.6041, Val acc: 77.5000%, Best Val loss: 0.5919 Best Val acc: 77.50%\n",
            "Epoch 7/100, Train loss: 0.6546, Train acc: 63.4921%, Val loss: 0.6706, Val acc: 70.0000%, Best Val loss: 0.5919 Best Val acc: 77.50%\n",
            "Epoch 8/100, Train loss: 0.6123, Train acc: 65.6085%, Val loss: 0.5624, Val acc: 77.5000%, Best Val loss: 0.5624 Best Val acc: 77.50%\n",
            "Epoch 9/100, Train loss: 0.6569, Train acc: 62.4339%, Val loss: 0.5793, Val acc: 67.5000%, Best Val loss: 0.5624 Best Val acc: 77.50%\n",
            "Epoch 10/100, Train loss: 0.7181, Train acc: 62.9630%, Val loss: 0.7442, Val acc: 57.5000%, Best Val loss: 0.5624 Best Val acc: 77.50%\n",
            "Epoch 11/100, Train loss: 0.5929, Train acc: 69.3122%, Val loss: 0.5727, Val acc: 77.5000%, Best Val loss: 0.5624 Best Val acc: 77.50%\n",
            "Epoch 12/100, Train loss: 0.5912, Train acc: 66.6667%, Val loss: 0.5721, Val acc: 75.0000%, Best Val loss: 0.5624 Best Val acc: 77.50%\n",
            "Epoch 13/100, Train loss: 0.5783, Train acc: 64.0212%, Val loss: 0.5600, Val acc: 75.0000%, Best Val loss: 0.5600 Best Val acc: 77.50%\n",
            "Epoch 14/100, Train loss: 0.6076, Train acc: 69.3122%, Val loss: 0.5741, Val acc: 70.0000%, Best Val loss: 0.5600 Best Val acc: 77.50%\n",
            "Epoch 15/100, Train loss: 0.7568, Train acc: 58.7302%, Val loss: 0.9179, Val acc: 47.5000%, Best Val loss: 0.5600 Best Val acc: 77.50%\n",
            "Epoch 16/100, Train loss: 0.7003, Train acc: 63.4921%, Val loss: 0.8060, Val acc: 57.5000%, Best Val loss: 0.5600 Best Val acc: 77.50%\n",
            "Epoch 17/100, Train loss: 0.6737, Train acc: 64.0212%, Val loss: 0.5871, Val acc: 70.0000%, Best Val loss: 0.5600 Best Val acc: 77.50%\n",
            "Epoch 18/100, Train loss: 0.6043, Train acc: 66.1376%, Val loss: 0.6010, Val acc: 77.5000%, Best Val loss: 0.5600 Best Val acc: 77.50%\n",
            "Epoch 19/100, Train loss: 0.5678, Train acc: 69.3122%, Val loss: 0.6460, Val acc: 75.0000%, Best Val loss: 0.5600 Best Val acc: 77.50%\n",
            "Epoch 20/100, Train loss: 0.5605, Train acc: 70.3704%, Val loss: 0.5740, Val acc: 77.5000%, Best Val loss: 0.5600 Best Val acc: 77.50%\n",
            "Epoch 21/100, Train loss: 0.6110, Train acc: 66.1376%, Val loss: 0.5725, Val acc: 70.0000%, Best Val loss: 0.5600 Best Val acc: 77.50%\n",
            "Epoch 22/100, Train loss: 0.6424, Train acc: 62.4339%, Val loss: 0.5676, Val acc: 70.0000%, Best Val loss: 0.5600 Best Val acc: 77.50%\n",
            "Epoch 23/100, Train loss: 0.6174, Train acc: 66.1376%, Val loss: 0.6516, Val acc: 70.0000%, Best Val loss: 0.5600 Best Val acc: 77.50%\n",
            "Epoch 24/100, Train loss: 0.5953, Train acc: 68.2540%, Val loss: 0.7065, Val acc: 60.0000%, Best Val loss: 0.5600 Best Val acc: 77.50%\n",
            "Epoch 25/100, Train loss: 0.6086, Train acc: 70.3704%, Val loss: 0.7000, Val acc: 65.0000%, Best Val loss: 0.5600 Best Val acc: 77.50%\n",
            "Epoch 26/100, Train loss: 0.6347, Train acc: 64.0212%, Val loss: 0.5639, Val acc: 72.5000%, Best Val loss: 0.5600 Best Val acc: 77.50%\n",
            "Epoch 27/100, Train loss: 0.5887, Train acc: 66.6667%, Val loss: 0.5533, Val acc: 77.5000%, Best Val loss: 0.5533 Best Val acc: 77.50%\n",
            "Epoch 28/100, Train loss: 0.5651, Train acc: 71.9577%, Val loss: 0.6171, Val acc: 75.0000%, Best Val loss: 0.5533 Best Val acc: 77.50%\n",
            "Epoch 29/100, Train loss: 0.5339, Train acc: 75.6614%, Val loss: 0.5629, Val acc: 85.0000%, Best Val loss: 0.5533 Best Val acc: 85.00%\n",
            "Epoch 30/100, Train loss: 0.5411, Train acc: 74.0741%, Val loss: 0.5737, Val acc: 77.5000%, Best Val loss: 0.5533 Best Val acc: 85.00%\n",
            "Epoch 31/100, Train loss: 0.5627, Train acc: 68.7831%, Val loss: 0.7085, Val acc: 57.5000%, Best Val loss: 0.5533 Best Val acc: 85.00%\n",
            "Epoch 32/100, Train loss: 0.5266, Train acc: 72.4868%, Val loss: 0.5400, Val acc: 77.5000%, Best Val loss: 0.5400 Best Val acc: 85.00%\n",
            "Epoch 33/100, Train loss: 0.5469, Train acc: 71.4286%, Val loss: 0.6932, Val acc: 60.0000%, Best Val loss: 0.5400 Best Val acc: 85.00%\n",
            "Epoch 34/100, Train loss: 0.5472, Train acc: 70.8995%, Val loss: 0.6105, Val acc: 75.0000%, Best Val loss: 0.5400 Best Val acc: 85.00%\n",
            "Epoch 35/100, Train loss: 0.5511, Train acc: 71.9577%, Val loss: 0.6149, Val acc: 72.5000%, Best Val loss: 0.5400 Best Val acc: 85.00%\n",
            "Epoch 36/100, Train loss: 0.5972, Train acc: 71.4286%, Val loss: 0.6230, Val acc: 75.0000%, Best Val loss: 0.5400 Best Val acc: 85.00%\n",
            "Epoch 37/100, Train loss: 0.5190, Train acc: 73.0159%, Val loss: 0.5290, Val acc: 82.5000%, Best Val loss: 0.5290 Best Val acc: 85.00%\n",
            "Epoch 38/100, Train loss: 0.5444, Train acc: 70.3704%, Val loss: 0.5381, Val acc: 77.5000%, Best Val loss: 0.5290 Best Val acc: 85.00%\n",
            "Epoch 39/100, Train loss: 0.5533, Train acc: 71.9577%, Val loss: 0.5276, Val acc: 80.0000%, Best Val loss: 0.5276 Best Val acc: 85.00%\n",
            "Epoch 40/100, Train loss: 0.5280, Train acc: 71.9577%, Val loss: 0.6970, Val acc: 57.5000%, Best Val loss: 0.5276 Best Val acc: 85.00%\n",
            "Epoch 41/100, Train loss: 0.5507, Train acc: 72.4868%, Val loss: 0.5695, Val acc: 75.0000%, Best Val loss: 0.5276 Best Val acc: 85.00%\n",
            "Epoch 42/100, Train loss: 0.5125, Train acc: 76.1905%, Val loss: 0.5405, Val acc: 80.0000%, Best Val loss: 0.5276 Best Val acc: 85.00%\n",
            "Epoch 43/100, Train loss: 0.5205, Train acc: 75.1323%, Val loss: 0.5256, Val acc: 82.5000%, Best Val loss: 0.5256 Best Val acc: 85.00%\n",
            "Epoch 44/100, Train loss: 0.5290, Train acc: 70.8995%, Val loss: 0.5814, Val acc: 75.0000%, Best Val loss: 0.5256 Best Val acc: 85.00%\n",
            "Epoch 45/100, Train loss: 0.5234, Train acc: 72.4868%, Val loss: 0.6163, Val acc: 70.0000%, Best Val loss: 0.5256 Best Val acc: 85.00%\n",
            "Epoch 46/100, Train loss: 0.5102, Train acc: 74.6032%, Val loss: 0.5130, Val acc: 80.0000%, Best Val loss: 0.5130 Best Val acc: 85.00%\n",
            "Epoch 47/100, Train loss: 0.4950, Train acc: 74.6032%, Val loss: 0.5449, Val acc: 82.5000%, Best Val loss: 0.5130 Best Val acc: 85.00%\n",
            "Epoch 48/100, Train loss: 0.5020, Train acc: 76.7196%, Val loss: 0.5128, Val acc: 82.5000%, Best Val loss: 0.5128 Best Val acc: 85.00%\n",
            "Epoch 49/100, Train loss: 0.5351, Train acc: 70.8995%, Val loss: 0.5108, Val acc: 80.0000%, Best Val loss: 0.5108 Best Val acc: 85.00%\n",
            "Epoch 50/100, Train loss: 0.5350, Train acc: 71.4286%, Val loss: 0.5871, Val acc: 75.0000%, Best Val loss: 0.5108 Best Val acc: 85.00%\n",
            "Epoch 51/100, Train loss: 0.5341, Train acc: 73.0159%, Val loss: 0.6158, Val acc: 70.0000%, Best Val loss: 0.5108 Best Val acc: 85.00%\n",
            "Epoch 52/100, Train loss: 0.5561, Train acc: 70.8995%, Val loss: 0.5202, Val acc: 77.5000%, Best Val loss: 0.5108 Best Val acc: 85.00%\n",
            "Epoch 53/100, Train loss: 0.5162, Train acc: 74.6032%, Val loss: 0.6409, Val acc: 62.5000%, Best Val loss: 0.5108 Best Val acc: 85.00%\n",
            "Epoch 54/100, Train loss: 0.4848, Train acc: 76.7196%, Val loss: 0.5014, Val acc: 87.5000%, Best Val loss: 0.5014 Best Val acc: 87.50%\n",
            "Epoch 55/100, Train loss: 0.4739, Train acc: 77.2487%, Val loss: 0.5413, Val acc: 82.5000%, Best Val loss: 0.5014 Best Val acc: 87.50%\n",
            "Epoch 56/100, Train loss: 0.4739, Train acc: 76.7196%, Val loss: 0.4988, Val acc: 85.0000%, Best Val loss: 0.4988 Best Val acc: 87.50%\n",
            "Epoch 57/100, Train loss: 0.4715, Train acc: 76.7196%, Val loss: 0.5911, Val acc: 72.5000%, Best Val loss: 0.4988 Best Val acc: 87.50%\n",
            "Epoch 58/100, Train loss: 0.4669, Train acc: 80.9524%, Val loss: 0.5008, Val acc: 85.0000%, Best Val loss: 0.4988 Best Val acc: 87.50%\n",
            "Epoch 59/100, Train loss: 0.4677, Train acc: 79.8942%, Val loss: 0.5487, Val acc: 77.5000%, Best Val loss: 0.4988 Best Val acc: 87.50%\n",
            "Epoch 60/100, Train loss: 0.4714, Train acc: 80.4233%, Val loss: 0.5314, Val acc: 82.5000%, Best Val loss: 0.4988 Best Val acc: 87.50%\n",
            "Epoch 61/100, Train loss: 0.4647, Train acc: 80.9524%, Val loss: 0.4993, Val acc: 87.5000%, Best Val loss: 0.4988 Best Val acc: 87.50%\n",
            "Epoch 62/100, Train loss: 0.4736, Train acc: 75.6614%, Val loss: 0.5549, Val acc: 75.0000%, Best Val loss: 0.4988 Best Val acc: 87.50%\n",
            "Epoch 63/100, Train loss: 0.4582, Train acc: 79.8942%, Val loss: 0.5018, Val acc: 87.5000%, Best Val loss: 0.4988 Best Val acc: 87.50%\n",
            "Epoch 64/100, Train loss: 0.4618, Train acc: 80.4233%, Val loss: 0.5402, Val acc: 80.0000%, Best Val loss: 0.4988 Best Val acc: 87.50%\n",
            "Epoch 65/100, Train loss: 0.4585, Train acc: 80.9524%, Val loss: 0.4993, Val acc: 87.5000%, Best Val loss: 0.4988 Best Val acc: 87.50%\n",
            "Epoch 66/100, Train loss: 0.4612, Train acc: 77.7778%, Val loss: 0.5359, Val acc: 80.0000%, Best Val loss: 0.4988 Best Val acc: 87.50%\n",
            "Epoch 67/100, Train loss: 0.4611, Train acc: 79.8942%, Val loss: 0.5022, Val acc: 87.5000%, Best Val loss: 0.4988 Best Val acc: 87.50%\n",
            "Epoch 68/100, Train loss: 0.4580, Train acc: 76.7196%, Val loss: 0.5168, Val acc: 82.5000%, Best Val loss: 0.4988 Best Val acc: 87.50%\n",
            "Epoch 69/100, Train loss: 0.4654, Train acc: 79.3651%, Val loss: 0.5202, Val acc: 80.0000%, Best Val loss: 0.4988 Best Val acc: 87.50%\n",
            "Epoch 70/100, Train loss: 0.4833, Train acc: 77.7778%, Val loss: 0.4872, Val acc: 87.5000%, Best Val loss: 0.4872 Best Val acc: 87.50%\n",
            "Epoch 71/100, Train loss: 0.4551, Train acc: 79.8942%, Val loss: 0.5568, Val acc: 75.0000%, Best Val loss: 0.4872 Best Val acc: 87.50%\n",
            "Epoch 72/100, Train loss: 0.4379, Train acc: 81.4815%, Val loss: 0.4836, Val acc: 87.5000%, Best Val loss: 0.4836 Best Val acc: 87.50%\n",
            "Epoch 73/100, Train loss: 0.4665, Train acc: 78.3069%, Val loss: 0.5137, Val acc: 85.0000%, Best Val loss: 0.4836 Best Val acc: 87.50%\n",
            "Epoch 74/100, Train loss: 0.4431, Train acc: 83.0688%, Val loss: 0.5203, Val acc: 82.5000%, Best Val loss: 0.4836 Best Val acc: 87.50%\n",
            "Epoch 75/100, Train loss: 0.4407, Train acc: 84.6561%, Val loss: 0.4904, Val acc: 87.5000%, Best Val loss: 0.4836 Best Val acc: 87.50%\n",
            "Epoch 76/100, Train loss: 0.4387, Train acc: 80.9524%, Val loss: 0.4917, Val acc: 87.5000%, Best Val loss: 0.4836 Best Val acc: 87.50%\n",
            "Epoch 77/100, Train loss: 0.4372, Train acc: 83.0688%, Val loss: 0.5057, Val acc: 82.5000%, Best Val loss: 0.4836 Best Val acc: 87.50%\n",
            "Epoch 78/100, Train loss: 0.4397, Train acc: 82.5397%, Val loss: 0.5099, Val acc: 85.0000%, Best Val loss: 0.4836 Best Val acc: 87.50%\n",
            "Epoch 79/100, Train loss: 0.4394, Train acc: 82.0106%, Val loss: 0.4905, Val acc: 87.5000%, Best Val loss: 0.4836 Best Val acc: 87.50%\n",
            "Epoch 80/100, Train loss: 0.4371, Train acc: 80.9524%, Val loss: 0.5006, Val acc: 82.5000%, Best Val loss: 0.4836 Best Val acc: 87.50%\n",
            "Epoch 81/100, Train loss: 0.4352, Train acc: 84.6561%, Val loss: 0.5090, Val acc: 85.0000%, Best Val loss: 0.4836 Best Val acc: 87.50%\n",
            "Epoch 82/100, Train loss: 0.4349, Train acc: 85.1852%, Val loss: 0.4966, Val acc: 82.5000%, Best Val loss: 0.4836 Best Val acc: 87.50%\n",
            "Epoch 83/100, Train loss: 0.4384, Train acc: 81.4815%, Val loss: 0.4845, Val acc: 87.5000%, Best Val loss: 0.4836 Best Val acc: 87.50%\n",
            "Epoch 84/100, Train loss: 0.4334, Train acc: 81.4815%, Val loss: 0.4980, Val acc: 82.5000%, Best Val loss: 0.4836 Best Val acc: 87.50%\n",
            "Epoch 85/100, Train loss: 0.4351, Train acc: 84.6561%, Val loss: 0.5091, Val acc: 82.5000%, Best Val loss: 0.4836 Best Val acc: 87.50%\n",
            "Epoch 86/100, Train loss: 0.4352, Train acc: 85.1852%, Val loss: 0.5011, Val acc: 85.0000%, Best Val loss: 0.4836 Best Val acc: 87.50%\n",
            "Epoch 87/100, Train loss: 0.4324, Train acc: 84.1270%, Val loss: 0.4887, Val acc: 87.5000%, Best Val loss: 0.4836 Best Val acc: 87.50%\n",
            "Epoch 88/100, Train loss: 0.4341, Train acc: 81.4815%, Val loss: 0.4875, Val acc: 87.5000%, Best Val loss: 0.4836 Best Val acc: 87.50%\n",
            "Epoch 89/100, Train loss: 0.4323, Train acc: 83.5979%, Val loss: 0.4939, Val acc: 85.0000%, Best Val loss: 0.4836 Best Val acc: 87.50%\n",
            "Epoch 90/100, Train loss: 0.4323, Train acc: 84.6561%, Val loss: 0.5018, Val acc: 85.0000%, Best Val loss: 0.4836 Best Val acc: 87.50%\n",
            "Epoch 91/100, Train loss: 0.4327, Train acc: 85.1852%, Val loss: 0.4984, Val acc: 85.0000%, Best Val loss: 0.4836 Best Val acc: 87.50%\n",
            "Epoch 92/100, Train loss: 0.4326, Train acc: 84.6561%, Val loss: 0.4964, Val acc: 82.5000%, Best Val loss: 0.4836 Best Val acc: 87.50%\n",
            "Epoch 93/100, Train loss: 0.4340, Train acc: 84.6561%, Val loss: 0.4910, Val acc: 87.5000%, Best Val loss: 0.4836 Best Val acc: 87.50%\n",
            "Epoch 94/100, Train loss: 0.4304, Train acc: 84.1270%, Val loss: 0.4917, Val acc: 87.5000%, Best Val loss: 0.4836 Best Val acc: 87.50%\n",
            "Epoch 95/100, Train loss: 0.4286, Train acc: 84.6561%, Val loss: 0.4940, Val acc: 85.0000%, Best Val loss: 0.4836 Best Val acc: 87.50%\n",
            "Epoch 96/100, Train loss: 0.4318, Train acc: 84.6561%, Val loss: 0.4933, Val acc: 85.0000%, Best Val loss: 0.4836 Best Val acc: 87.50%\n",
            "Epoch 97/100, Train loss: 0.4313, Train acc: 84.6561%, Val loss: 0.4932, Val acc: 87.5000%, Best Val loss: 0.4836 Best Val acc: 87.50%\n",
            "Epoch 98/100, Train loss: 0.4329, Train acc: 84.6561%, Val loss: 0.4938, Val acc: 85.0000%, Best Val loss: 0.4836 Best Val acc: 87.50%\n",
            "Epoch 99/100, Train loss: 0.4318, Train acc: 85.1852%, Val loss: 0.4939, Val acc: 85.0000%, Best Val loss: 0.4836 Best Val acc: 87.50%\n",
            "Epoch 100/100, Train loss: 0.4308, Train acc: 85.1852%, Val loss: 0.4939, Val acc: 85.0000%, Best Val loss: 0.4836 Best Val acc: 87.50%\n",
            "\n",
            "Training with LR=0.0001, Hidden Units=256\n",
            "Epoch 1/100, Train loss: 9.7489, Train acc: 44.4444%, Val loss: 4.5468, Val acc: 30.0000%, Best Val loss: 4.5468 Best Val acc: 30.00%\n",
            "Epoch 2/100, Train loss: 4.4924, Train acc: 52.3810%, Val loss: 2.9789, Val acc: 42.5000%, Best Val loss: 2.9789 Best Val acc: 42.50%\n",
            "Epoch 3/100, Train loss: 3.1639, Train acc: 53.4392%, Val loss: 2.3898, Val acc: 72.5000%, Best Val loss: 2.3898 Best Val acc: 72.50%\n",
            "Epoch 4/100, Train loss: 2.2418, Train acc: 59.2593%, Val loss: 4.3023, Val acc: 42.5000%, Best Val loss: 2.3898 Best Val acc: 72.50%\n",
            "Epoch 5/100, Train loss: 2.1031, Train acc: 57.1429%, Val loss: 0.9381, Val acc: 70.0000%, Best Val loss: 0.9381 Best Val acc: 72.50%\n",
            "Epoch 6/100, Train loss: 1.3593, Train acc: 60.3175%, Val loss: 1.0014, Val acc: 67.5000%, Best Val loss: 0.9381 Best Val acc: 72.50%\n",
            "Epoch 7/100, Train loss: 1.3614, Train acc: 68.7831%, Val loss: 3.1189, Val acc: 37.5000%, Best Val loss: 0.9381 Best Val acc: 72.50%\n",
            "Epoch 8/100, Train loss: 1.6650, Train acc: 52.9101%, Val loss: 0.8207, Val acc: 72.5000%, Best Val loss: 0.8207 Best Val acc: 72.50%\n",
            "Epoch 9/100, Train loss: 1.0379, Train acc: 65.6085%, Val loss: 1.1846, Val acc: 72.5000%, Best Val loss: 0.8207 Best Val acc: 72.50%\n",
            "Epoch 10/100, Train loss: 1.0627, Train acc: 65.6085%, Val loss: 0.7389, Val acc: 75.0000%, Best Val loss: 0.7389 Best Val acc: 75.00%\n",
            "Epoch 11/100, Train loss: 0.9683, Train acc: 59.2593%, Val loss: 0.6769, Val acc: 77.5000%, Best Val loss: 0.6769 Best Val acc: 77.50%\n",
            "Epoch 12/100, Train loss: 0.7643, Train acc: 63.4921%, Val loss: 1.1091, Val acc: 55.0000%, Best Val loss: 0.6769 Best Val acc: 77.50%\n",
            "Epoch 13/100, Train loss: 0.6851, Train acc: 68.7831%, Val loss: 1.4280, Val acc: 40.0000%, Best Val loss: 0.6769 Best Val acc: 77.50%\n",
            "Epoch 14/100, Train loss: 0.8145, Train acc: 61.9048%, Val loss: 0.8155, Val acc: 65.0000%, Best Val loss: 0.6769 Best Val acc: 77.50%\n",
            "Epoch 15/100, Train loss: 0.7456, Train acc: 64.5503%, Val loss: 0.6925, Val acc: 67.5000%, Best Val loss: 0.6769 Best Val acc: 77.50%\n",
            "Epoch 16/100, Train loss: 0.7063, Train acc: 64.0212%, Val loss: 0.6200, Val acc: 70.0000%, Best Val loss: 0.6200 Best Val acc: 77.50%\n",
            "Epoch 17/100, Train loss: 0.7291, Train acc: 64.0212%, Val loss: 0.6828, Val acc: 72.5000%, Best Val loss: 0.6200 Best Val acc: 77.50%\n",
            "Epoch 18/100, Train loss: 0.6340, Train acc: 65.0794%, Val loss: 0.5847, Val acc: 72.5000%, Best Val loss: 0.5847 Best Val acc: 77.50%\n",
            "Epoch 19/100, Train loss: 0.5850, Train acc: 67.1958%, Val loss: 0.7050, Val acc: 67.5000%, Best Val loss: 0.5847 Best Val acc: 77.50%\n",
            "Epoch 20/100, Train loss: 0.5870, Train acc: 66.1376%, Val loss: 0.5823, Val acc: 80.0000%, Best Val loss: 0.5823 Best Val acc: 80.00%\n",
            "Epoch 21/100, Train loss: 0.5549, Train acc: 71.4286%, Val loss: 0.7326, Val acc: 62.5000%, Best Val loss: 0.5823 Best Val acc: 80.00%\n",
            "Epoch 22/100, Train loss: 0.6033, Train acc: 66.6667%, Val loss: 0.5623, Val acc: 80.0000%, Best Val loss: 0.5623 Best Val acc: 80.00%\n",
            "Epoch 23/100, Train loss: 0.6105, Train acc: 65.0794%, Val loss: 0.6413, Val acc: 72.5000%, Best Val loss: 0.5623 Best Val acc: 80.00%\n",
            "Epoch 24/100, Train loss: 0.6501, Train acc: 67.7249%, Val loss: 0.5638, Val acc: 75.0000%, Best Val loss: 0.5623 Best Val acc: 80.00%\n",
            "Epoch 25/100, Train loss: 0.6107, Train acc: 67.1958%, Val loss: 0.5679, Val acc: 72.5000%, Best Val loss: 0.5623 Best Val acc: 80.00%\n",
            "Epoch 26/100, Train loss: 0.5903, Train acc: 68.2540%, Val loss: 0.5836, Val acc: 70.0000%, Best Val loss: 0.5623 Best Val acc: 80.00%\n",
            "Epoch 27/100, Train loss: 0.6347, Train acc: 66.1376%, Val loss: 0.5665, Val acc: 77.5000%, Best Val loss: 0.5623 Best Val acc: 80.00%\n",
            "Epoch 28/100, Train loss: 0.5697, Train acc: 67.7249%, Val loss: 0.5688, Val acc: 75.0000%, Best Val loss: 0.5623 Best Val acc: 80.00%\n",
            "Epoch 29/100, Train loss: 0.5591, Train acc: 69.3122%, Val loss: 0.5561, Val acc: 72.5000%, Best Val loss: 0.5561 Best Val acc: 80.00%\n",
            "Epoch 30/100, Train loss: 0.6293, Train acc: 66.6667%, Val loss: 0.5415, Val acc: 82.5000%, Best Val loss: 0.5415 Best Val acc: 82.50%\n",
            "Epoch 31/100, Train loss: 0.5298, Train acc: 71.4286%, Val loss: 0.5372, Val acc: 80.0000%, Best Val loss: 0.5372 Best Val acc: 82.50%\n",
            "Epoch 32/100, Train loss: 0.5178, Train acc: 73.5450%, Val loss: 0.5370, Val acc: 75.0000%, Best Val loss: 0.5370 Best Val acc: 82.50%\n",
            "Epoch 33/100, Train loss: 0.5120, Train acc: 75.1323%, Val loss: 0.5591, Val acc: 75.0000%, Best Val loss: 0.5370 Best Val acc: 82.50%\n",
            "Epoch 34/100, Train loss: 0.5393, Train acc: 70.8995%, Val loss: 0.5324, Val acc: 75.0000%, Best Val loss: 0.5324 Best Val acc: 82.50%\n",
            "Epoch 35/100, Train loss: 0.5185, Train acc: 71.9577%, Val loss: 0.5251, Val acc: 75.0000%, Best Val loss: 0.5251 Best Val acc: 82.50%\n",
            "Epoch 36/100, Train loss: 0.6438, Train acc: 66.1376%, Val loss: 0.6023, Val acc: 75.0000%, Best Val loss: 0.5251 Best Val acc: 82.50%\n",
            "Epoch 37/100, Train loss: 0.6360, Train acc: 65.0794%, Val loss: 0.6779, Val acc: 75.0000%, Best Val loss: 0.5251 Best Val acc: 82.50%\n",
            "Epoch 38/100, Train loss: 0.6519, Train acc: 67.7249%, Val loss: 0.5686, Val acc: 75.0000%, Best Val loss: 0.5251 Best Val acc: 82.50%\n",
            "Epoch 39/100, Train loss: 0.6203, Train acc: 67.1958%, Val loss: 0.5398, Val acc: 82.5000%, Best Val loss: 0.5251 Best Val acc: 82.50%\n",
            "Epoch 40/100, Train loss: 0.5141, Train acc: 72.4868%, Val loss: 0.5562, Val acc: 80.0000%, Best Val loss: 0.5251 Best Val acc: 82.50%\n",
            "Epoch 41/100, Train loss: 0.5417, Train acc: 69.8413%, Val loss: 0.5261, Val acc: 85.0000%, Best Val loss: 0.5251 Best Val acc: 85.00%\n",
            "Epoch 42/100, Train loss: 0.5381, Train acc: 71.4286%, Val loss: 0.8534, Val acc: 47.5000%, Best Val loss: 0.5251 Best Val acc: 85.00%\n",
            "Epoch 43/100, Train loss: 0.6218, Train acc: 61.9048%, Val loss: 0.5148, Val acc: 87.5000%, Best Val loss: 0.5148 Best Val acc: 87.50%\n",
            "Epoch 44/100, Train loss: 0.5673, Train acc: 71.4286%, Val loss: 0.5042, Val acc: 85.0000%, Best Val loss: 0.5042 Best Val acc: 87.50%\n",
            "Epoch 45/100, Train loss: 0.4931, Train acc: 75.6614%, Val loss: 0.5145, Val acc: 85.0000%, Best Val loss: 0.5042 Best Val acc: 87.50%\n",
            "Epoch 46/100, Train loss: 0.4714, Train acc: 78.8360%, Val loss: 0.6538, Val acc: 65.0000%, Best Val loss: 0.5042 Best Val acc: 87.50%\n",
            "Epoch 47/100, Train loss: 0.4893, Train acc: 77.2487%, Val loss: 0.5894, Val acc: 72.5000%, Best Val loss: 0.5042 Best Val acc: 87.50%\n",
            "Epoch 48/100, Train loss: 0.4806, Train acc: 75.6614%, Val loss: 0.5376, Val acc: 82.5000%, Best Val loss: 0.5042 Best Val acc: 87.50%\n",
            "Epoch 49/100, Train loss: 0.4836, Train acc: 76.7196%, Val loss: 0.4901, Val acc: 80.0000%, Best Val loss: 0.4901 Best Val acc: 87.50%\n",
            "Epoch 50/100, Train loss: 0.5240, Train acc: 71.4286%, Val loss: 0.4982, Val acc: 82.5000%, Best Val loss: 0.4901 Best Val acc: 87.50%\n",
            "Epoch 51/100, Train loss: 0.5251, Train acc: 70.8995%, Val loss: 0.6479, Val acc: 77.5000%, Best Val loss: 0.4901 Best Val acc: 87.50%\n",
            "Epoch 52/100, Train loss: 0.6871, Train acc: 65.6085%, Val loss: 0.4931, Val acc: 85.0000%, Best Val loss: 0.4901 Best Val acc: 87.50%\n",
            "Epoch 53/100, Train loss: 0.6286, Train acc: 66.6667%, Val loss: 0.9623, Val acc: 42.5000%, Best Val loss: 0.4901 Best Val acc: 87.50%\n",
            "Epoch 54/100, Train loss: 0.6052, Train acc: 70.8995%, Val loss: 0.7813, Val acc: 57.5000%, Best Val loss: 0.4901 Best Val acc: 87.50%\n",
            "Epoch 55/100, Train loss: 0.5718, Train acc: 69.3122%, Val loss: 0.5250, Val acc: 77.5000%, Best Val loss: 0.4901 Best Val acc: 87.50%\n",
            "Epoch 56/100, Train loss: 0.6261, Train acc: 66.6667%, Val loss: 0.6073, Val acc: 77.5000%, Best Val loss: 0.4901 Best Val acc: 87.50%\n",
            "Epoch 57/100, Train loss: 0.6302, Train acc: 70.8995%, Val loss: 0.8601, Val acc: 50.0000%, Best Val loss: 0.4901 Best Val acc: 87.50%\n",
            "Epoch 58/100, Train loss: 0.5509, Train acc: 69.8413%, Val loss: 0.5197, Val acc: 80.0000%, Best Val loss: 0.4901 Best Val acc: 87.50%\n",
            "Epoch 59/100, Train loss: 0.5025, Train acc: 75.1323%, Val loss: 0.4839, Val acc: 80.0000%, Best Val loss: 0.4839 Best Val acc: 87.50%\n",
            "Epoch 60/100, Train loss: 0.4785, Train acc: 77.2487%, Val loss: 0.4774, Val acc: 80.0000%, Best Val loss: 0.4774 Best Val acc: 87.50%\n",
            "Epoch 61/100, Train loss: 0.4924, Train acc: 74.6032%, Val loss: 0.6612, Val acc: 65.0000%, Best Val loss: 0.4774 Best Val acc: 87.50%\n",
            "Epoch 62/100, Train loss: 0.4917, Train acc: 74.0741%, Val loss: 0.5258, Val acc: 82.5000%, Best Val loss: 0.4774 Best Val acc: 87.50%\n",
            "Epoch 63/100, Train loss: 0.5045, Train acc: 70.3704%, Val loss: 0.5123, Val acc: 80.0000%, Best Val loss: 0.4774 Best Val acc: 87.50%\n",
            "Epoch 64/100, Train loss: 0.5094, Train acc: 71.9577%, Val loss: 0.5831, Val acc: 75.0000%, Best Val loss: 0.4774 Best Val acc: 87.50%\n",
            "Epoch 65/100, Train loss: 0.4674, Train acc: 75.6614%, Val loss: 0.5208, Val acc: 80.0000%, Best Val loss: 0.4774 Best Val acc: 87.50%\n",
            "Epoch 66/100, Train loss: 0.4599, Train acc: 78.3069%, Val loss: 0.4665, Val acc: 80.0000%, Best Val loss: 0.4665 Best Val acc: 87.50%\n",
            "Epoch 67/100, Train loss: 0.4582, Train acc: 78.8360%, Val loss: 0.5061, Val acc: 82.5000%, Best Val loss: 0.4665 Best Val acc: 87.50%\n",
            "Epoch 68/100, Train loss: 0.4346, Train acc: 80.9524%, Val loss: 0.4802, Val acc: 90.0000%, Best Val loss: 0.4665 Best Val acc: 90.00%\n",
            "Epoch 69/100, Train loss: 0.4359, Train acc: 81.4815%, Val loss: 0.4753, Val acc: 90.0000%, Best Val loss: 0.4665 Best Val acc: 90.00%\n",
            "Epoch 70/100, Train loss: 0.4479, Train acc: 79.3651%, Val loss: 0.5455, Val acc: 77.5000%, Best Val loss: 0.4665 Best Val acc: 90.00%\n",
            "Epoch 71/100, Train loss: 0.4395, Train acc: 80.4233%, Val loss: 0.4717, Val acc: 90.0000%, Best Val loss: 0.4665 Best Val acc: 90.00%\n",
            "Epoch 72/100, Train loss: 0.4391, Train acc: 79.3651%, Val loss: 0.5236, Val acc: 80.0000%, Best Val loss: 0.4665 Best Val acc: 90.00%\n",
            "Epoch 73/100, Train loss: 0.4355, Train acc: 81.4815%, Val loss: 0.4784, Val acc: 87.5000%, Best Val loss: 0.4665 Best Val acc: 90.00%\n",
            "Epoch 74/100, Train loss: 0.4322, Train acc: 80.9524%, Val loss: 0.4725, Val acc: 90.0000%, Best Val loss: 0.4665 Best Val acc: 90.00%\n",
            "Epoch 75/100, Train loss: 0.4289, Train acc: 84.1270%, Val loss: 0.4704, Val acc: 90.0000%, Best Val loss: 0.4665 Best Val acc: 90.00%\n",
            "Epoch 76/100, Train loss: 0.4280, Train acc: 83.0688%, Val loss: 0.4778, Val acc: 87.5000%, Best Val loss: 0.4665 Best Val acc: 90.00%\n",
            "Epoch 77/100, Train loss: 0.4302, Train acc: 83.0688%, Val loss: 0.4712, Val acc: 90.0000%, Best Val loss: 0.4665 Best Val acc: 90.00%\n",
            "Epoch 78/100, Train loss: 0.4261, Train acc: 82.0106%, Val loss: 0.4822, Val acc: 85.0000%, Best Val loss: 0.4665 Best Val acc: 90.00%\n",
            "Epoch 79/100, Train loss: 0.4323, Train acc: 81.4815%, Val loss: 0.4737, Val acc: 87.5000%, Best Val loss: 0.4665 Best Val acc: 90.00%\n",
            "Epoch 80/100, Train loss: 0.4273, Train acc: 82.0106%, Val loss: 0.4671, Val acc: 90.0000%, Best Val loss: 0.4665 Best Val acc: 90.00%\n",
            "Epoch 81/100, Train loss: 0.4232, Train acc: 83.0688%, Val loss: 0.4925, Val acc: 85.0000%, Best Val loss: 0.4665 Best Val acc: 90.00%\n",
            "Epoch 82/100, Train loss: 0.4260, Train acc: 81.4815%, Val loss: 0.4854, Val acc: 85.0000%, Best Val loss: 0.4665 Best Val acc: 90.00%\n",
            "Epoch 83/100, Train loss: 0.4210, Train acc: 83.0688%, Val loss: 0.4637, Val acc: 90.0000%, Best Val loss: 0.4637 Best Val acc: 90.00%\n",
            "Epoch 84/100, Train loss: 0.4239, Train acc: 81.4815%, Val loss: 0.4683, Val acc: 90.0000%, Best Val loss: 0.4637 Best Val acc: 90.00%\n",
            "Epoch 85/100, Train loss: 0.4222, Train acc: 83.5979%, Val loss: 0.4897, Val acc: 85.0000%, Best Val loss: 0.4637 Best Val acc: 90.00%\n",
            "Epoch 86/100, Train loss: 0.4218, Train acc: 83.0688%, Val loss: 0.4775, Val acc: 87.5000%, Best Val loss: 0.4637 Best Val acc: 90.00%\n",
            "Epoch 87/100, Train loss: 0.4199, Train acc: 83.5979%, Val loss: 0.4671, Val acc: 90.0000%, Best Val loss: 0.4637 Best Val acc: 90.00%\n",
            "Epoch 88/100, Train loss: 0.4216, Train acc: 83.5979%, Val loss: 0.4810, Val acc: 85.0000%, Best Val loss: 0.4637 Best Val acc: 90.00%\n",
            "Epoch 89/100, Train loss: 0.4214, Train acc: 83.5979%, Val loss: 0.4695, Val acc: 90.0000%, Best Val loss: 0.4637 Best Val acc: 90.00%\n",
            "Epoch 90/100, Train loss: 0.4207, Train acc: 83.5979%, Val loss: 0.4764, Val acc: 87.5000%, Best Val loss: 0.4637 Best Val acc: 90.00%\n",
            "Epoch 91/100, Train loss: 0.4202, Train acc: 83.5979%, Val loss: 0.4783, Val acc: 85.0000%, Best Val loss: 0.4637 Best Val acc: 90.00%\n",
            "Epoch 92/100, Train loss: 0.4215, Train acc: 83.5979%, Val loss: 0.4786, Val acc: 85.0000%, Best Val loss: 0.4637 Best Val acc: 90.00%\n",
            "Epoch 93/100, Train loss: 0.4213, Train acc: 83.5979%, Val loss: 0.4742, Val acc: 87.5000%, Best Val loss: 0.4637 Best Val acc: 90.00%\n",
            "Epoch 94/100, Train loss: 0.4230, Train acc: 83.5979%, Val loss: 0.4667, Val acc: 90.0000%, Best Val loss: 0.4637 Best Val acc: 90.00%\n",
            "Epoch 95/100, Train loss: 0.4203, Train acc: 83.5979%, Val loss: 0.4693, Val acc: 90.0000%, Best Val loss: 0.4637 Best Val acc: 90.00%\n",
            "Epoch 96/100, Train loss: 0.4206, Train acc: 83.5979%, Val loss: 0.4731, Val acc: 87.5000%, Best Val loss: 0.4637 Best Val acc: 90.00%\n",
            "Epoch 97/100, Train loss: 0.4191, Train acc: 83.5979%, Val loss: 0.4753, Val acc: 87.5000%, Best Val loss: 0.4637 Best Val acc: 90.00%\n",
            "Epoch 98/100, Train loss: 0.4197, Train acc: 83.5979%, Val loss: 0.4761, Val acc: 87.5000%, Best Val loss: 0.4637 Best Val acc: 90.00%\n",
            "Epoch 99/100, Train loss: 0.4198, Train acc: 83.5979%, Val loss: 0.4762, Val acc: 87.5000%, Best Val loss: 0.4637 Best Val acc: 90.00%\n",
            "Epoch 100/100, Train loss: 0.4189, Train acc: 83.5979%, Val loss: 0.4762, Val acc: 87.5000%, Best Val loss: 0.4637 Best Val acc: 90.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7984c6e-6652-4160-b572-07d48bc93a3f",
      "metadata": {
        "id": "a7984c6e-6652-4160-b572-07d48bc93a3f"
      },
      "source": [
        "#### Visualizing the model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5559d850-1fb5-4b04-b6ca-60c5b309f34e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "5559d850-1fb5-4b04-b6ca-60c5b309f34e",
        "outputId": "a17df95d-9c05-4338-86cd-8844b858f863"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "all_train_accuracies = []\n",
        "all_val_accuracies = []\n",
        "all_train_losses = []\n",
        "all_val_losses = []\n",
        "\n",
        "# Inside the loop after training each model\n",
        "all_train_accuracies.append(train_accuracies)\n",
        "all_val_accuracies.append(val_accuracies)\n",
        "all_train_losses.append(train_losses)\n",
        "all_val_losses.append(val_losses)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot all accuracy curves\n",
        "for acc in all_train_accuracies:\n",
        "    ax[0].plot(acc, linestyle='--')\n",
        "for acc in all_val_accuracies:\n",
        "    ax[0].plot(acc)\n",
        "ax[0].set_title('Model Accuracy Across Experiments')\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[0].set_ylabel('Accuracy')\n",
        "ax[0].legend(['Train', 'Val'] * len(all_train_accuracies))\n",
        "\n",
        "# Plot all loss curves\n",
        "for loss in all_train_losses:\n",
        "    ax[1].plot(loss, linestyle='--')\n",
        "for loss in all_val_losses:\n",
        "    ax[1].plot(loss)\n",
        "ax[1].set_title('Model Loss Across Experiments')\n",
        "ax[1].set_xlabel('Epochs')\n",
        "ax[1].set_ylabel('Loss')\n",
        "ax[1].legend(['Train', 'Val'] * len(all_train_losses))\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89c7e51b-8ab6-4aa2-877d-39b6daf55c20",
      "metadata": {
        "id": "89c7e51b-8ab6-4aa2-877d-39b6daf55c20"
      },
      "source": [
        "## D. Evaluating Your Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f49735d7-466f-4037-8078-172f03dffd8d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "f49735d7-466f-4037-8078-172f03dffd8d",
        "outputId": "95c6fad4-7b3b-4392-8aff-a2ce92835dc0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
              "0   50    0   2       120   219    0        1      158      0      1.6      1   \n",
              "1   58    0   2       120   340    0        1      172      0      0.0      2   \n",
              "2   66    0   3       150   226    0        1      114      0      2.6      0   \n",
              "3   43    1   0       150   247    0        1      171      0      1.5      2   \n",
              "4   69    0   3       140   239    0        1      151      0      1.8      2   \n",
              "\n",
              "   ca  thal  target  \n",
              "0   0     2       1  \n",
              "1   0     2       1  \n",
              "2   0     2       1  \n",
              "3   0     2       1  \n",
              "4   2     2       1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-70563528-03f9-4cc5-9f43-6fb7564110ca\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>50</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>120</td>\n",
              "      <td>219</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>158</td>\n",
              "      <td>0</td>\n",
              "      <td>1.6</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>58</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>120</td>\n",
              "      <td>340</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>172</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>66</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>150</td>\n",
              "      <td>226</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>114</td>\n",
              "      <td>0</td>\n",
              "      <td>2.6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>43</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>247</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>171</td>\n",
              "      <td>0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>69</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>140</td>\n",
              "      <td>239</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>151</td>\n",
              "      <td>0</td>\n",
              "      <td>1.8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-70563528-03f9-4cc5-9f43-6fb7564110ca')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-70563528-03f9-4cc5-9f43-6fb7564110ca button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-70563528-03f9-4cc5-9f43-6fb7564110ca');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e97f8035-e419-4164-bb94-c9fc41885995\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e97f8035-e419-4164-bb94-c9fc41885995')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e97f8035-e419-4164-bb94-c9fc41885995 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "test_data",
              "summary": "{\n  \"name\": \"test_data\",\n  \"rows\": 31,\n  \"fields\": [\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 40,\n        \"max\": 71,\n        \"num_unique_values\": 18,\n        \"samples\": [\n          50,\n          58,\n          61\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sex\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          3,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"trestbps\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14,\n        \"min\": 110,\n        \"max\": 170,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          145,\n          124\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chol\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 57,\n        \"min\": 172,\n        \"max\": 417,\n        \"num_unique_values\": 28,\n        \"samples\": [\n          302,\n          282\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fbs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"restecg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thalach\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 20,\n        \"min\": 109,\n        \"max\": 179,\n        \"num_unique_values\": 26,\n        \"samples\": [\n          137,\n          128\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"exang\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"oldpeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.9768723406323231,\n        \"min\": 0.0,\n        \"max\": 3.4,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          1.6,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"slope\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ca\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thal\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 2,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          3,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# read test file\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/heart_dataset_test.csv')\n",
        "test_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21ae9d85-0dc2-4db0-a7c7-807c6b6c514f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "21ae9d85-0dc2-4db0-a7c7-807c6b6c514f",
        "outputId": "bba59368-ccdc-4186-b00e-2076d224b9ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "age         0\n",
              "sex         0\n",
              "cp          0\n",
              "trestbps    0\n",
              "chol        0\n",
              "fbs         0\n",
              "restecg     0\n",
              "thalach     0\n",
              "exang       0\n",
              "oldpeak     0\n",
              "slope       0\n",
              "ca          0\n",
              "thal        0\n",
              "target      0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>age</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sex</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cp</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>trestbps</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chol</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fbs</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>restecg</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>thalach</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>exang</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oldpeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>slope</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ca</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>thal</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>target</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "test_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ff2812b-a5a5-4ea9-86be-ae2143cb2ba7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ff2812b-a5a5-4ea9-86be-ae2143cb2ba7",
        "outputId": "5f5ddf8f-da3e-4d34-afa9-d37822ad1fa5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "test_data = test_data.values\n",
        "test_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14d4be20-f64f-421d-8971-e1e47873aef8",
      "metadata": {
        "id": "14d4be20-f64f-421d-8971-e1e47873aef8"
      },
      "outputs": [],
      "source": [
        "# Convert to PyTorch tensors\n",
        "x_test = torch.from_numpy(test_data[:, :13]).float()\n",
        "y_test = torch.from_numpy(test_data[:, 13]).long()\n",
        "\n",
        "# Create datasets\n",
        "test_dataset = TensorDataset(x_test, y_test)\n",
        "\n",
        "# Create dataloaders\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bcf8580-42ee-4ee7-ad15-9f080cc57a33",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bcf8580-42ee-4ee7-ad15-9f080cc57a33",
        "outputId": "265a42ef-da70-4967-c89a-62271dac59d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Learning Rate  Hidden Units  Test Accuracy (%)  Test Loss\n",
            "0         0.0100            64              67.74     0.5479\n",
            "1         0.0100           128              70.97     0.5494\n",
            "2         0.0100           256              70.97     0.5420\n",
            "3         0.0010            64              70.97     0.5818\n",
            "4         0.0010           128              64.52     0.5566\n",
            "5         0.0010           256              80.65     0.4945\n",
            "6         0.0001            64              67.74     0.5693\n",
            "7         0.0001           128              74.19     0.6009\n",
            "8         0.0001           256              74.19     0.5387\n"
          ]
        }
      ],
      "source": [
        "test_results = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for hidden_units in hidden_units_list:\n",
        "        model = Model(hidden_units).cuda()\n",
        "        model.load_state_dict(torch.load(f'model_LR{lr}_HU{hidden_units}.pth'))\n",
        "        model.eval()\n",
        "\n",
        "        test_correct = 0\n",
        "        test_total = 0\n",
        "        total_test_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for features, labels in test_loader:\n",
        "                features = features.cuda()\n",
        "                labels = labels.cuda()\n",
        "\n",
        "                outputs = model(features)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                predicted = outputs.argmax(-1)\n",
        "                test_correct += (predicted == labels).sum().item()\n",
        "                test_total += labels.size(0)\n",
        "                total_test_loss += loss.item()\n",
        "\n",
        "        test_acc = 100. * test_correct / test_total\n",
        "        avg_test_loss = total_test_loss / len(test_loader)\n",
        "\n",
        "        test_results.append({\n",
        "            'Learning Rate': lr,\n",
        "            'Hidden Units': hidden_units,\n",
        "            'Test Accuracy (%)': round(test_acc, 2),\n",
        "            'Test Loss': round(avg_test_loss, 4)\n",
        "        })\n",
        "\n",
        "import pandas as pd\n",
        "df_test_results = pd.DataFrame(test_results)\n",
        "print(df_test_results)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}